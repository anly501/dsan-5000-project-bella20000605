{"title":"Naive Bayes","markdown":{"yaml":{"title":"Naive Bayes"},"headingText":"HW-3.2.0: Introduction to Naive Bayes","containsRefs":false,"markdown":"\n\n\n\nNaive Bayes classification is a probabilistic machine learning model that’s based on Bayes' Theorem. It’s called \"naive\" because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets.\n\n#### <b>Bayes' Theorem Foundation</b><br>\n\nAt its core, Naive Bayes relies on Bayes' Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For classification, Bayes' Theorem is stated as:\n\n\n\n\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\n\nHere,P(B|A) is the probability of hypothesis A given data B. P(B|A) is the probability of the data under the hypothesis, P(A) is the probability of the hypothesis before seeing the data, and P(B) is the probability of the data under any hypothesis.\n\n#### <b>Objectives of Naive Bayes Classification</b><br>\n\nThe objective in Naive Bayes is to determine the probability of a label given a set of features, and then predict the label with the highest probability. In other words, we calculate the posterior probability of each class given the input features and predict the class for which the posterior probability is the highest.\n\n#### <b>What Naive Bayes Aims to achieve</b><br>\n\nNaive Bayes aims to model the conditional probability of the class labels given the features in order to make predictions. It’s particularly advantageous in situations where the dimensionality of the input space is high, as it avoids the curse of dimensionality to a certain extent due to its feature independence assumption\n\n#### <b>Variants of Naive Bayes</b><br>\n\nThere are several variants of the Naive Bayes classifier, each appropriate for a different type of dataset:\n\n1. Gaussian Naive Bayes:\n\nUsed when features have a continuous distribution and an assumption of normal distribution (Gaussian) is reasonable.\nAppropriate for many real-world problems, like predicting whether a given email is spam or not.\n\n2. Multinomial Naive Bayes:\n\nUsed for discrete data where features represent counts or frequency counts of events.\nOften used in text classification, where the features are related to word counts or frequencies within the documents.\n\n3. Bernoulli Naive Bayes:\n\nUsed when features are binary (i.e., they take only two values like true and false).\nSuitable for making predictions from binary features, like if a word occurs in a document or not, which is a common scenario in text classification problems.\n\n\n##  HW-3.2.1: Prepare your Data for Naïve Bayes\n\n1.Handle missing values\n\n2.Encode the 'Income group' and 'Continent' categorical variables into a numerical format suitable for machine learning models.\n\n3.Split the data into training and test sets\n\nCheck missing values in the dataset\n\n##### 1.Handle missing values\na.Dropping columns with more than 1000 null values\n\n(We don't Remove Rows with Missing Values because although this is the simplest approach, but it can lead to a significant reduction in data size, which might not be ideal if the missing data is extensive.)\n\nso after this step, we dropped the column not_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\nb.Impute Missing Values:\n\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is a common strategy for numerical data.\n\nForward or Backward Fill: For time-series data, you can fill missing values with the next or previous values.\n\nPredictive Imputation: Use a machine learning algorithm to predict the missing values based on other data in the dataset.\n\nMean/Median Imputation: If the data is normally distributed, you might use the mean. If the distribution is skewed,the median might be more appropriate.\n\n so here we impute missing values using the mean for GDP(2022) and the median for average_learning_Adjusted_of_school (assuming the distributions are appropriate for these measures):\n\nc. Drop Rows ('Income group') with Missing Values:\n\nThe 'Income group' and 'Continent' columns still have missing values. Before we proceed with the Naive Bayes classification, we need to address these missing values. Since 'Income group' will be our target variable for classification, we cannot impute it as we did with the continuous variables. Instead, we should remove the rows with missing 'Income group' labels.\n\nAs for the 'Continent' column, it might not be necessary for our classification model if we're using economic and health indicators as features.(because we can just dont choose the feature in the feature selection). However, if we decide that continent information might be useful, we could either impute the missing values based on the 'Economy' column or drop the column if it's not required.\n\nThe dataset is now clean and ready for classification. We can proceed with the next step.   \n\n3.Encode the 'Income group' and 'Continent' categorical variables into a numerical format suitable for machine learning models and Split the data into training and test sets\n\n## HW-3.2.2: Feature selection\nDecide which features are relevant for our classification task(Naive Bayes model) and drop the rest.\n\nThe 'Income group' is a categorical variable that could be used as a label for classification, so we predict the 'Income group' based on health and economic indicators.\n\nmethod:\n 1. EDA to find the correlation between the features and the target variable (corralation matrix)\n\n\nStrong Correlation: Values above 0.7 or below -0.7 suggest a strong correlation between the variables.\nModerate Correlation: Values between 0.3 and 0.7 (or -0.3 and -0.7) suggest a moderate correlation.\nWeak Correlation: Values between 0 and 0.3 (or 0 and -0.3) suggest a weak correlation.\n\n2. Pearson Correlation\n\nThe Pearson correlation coefficients you've provided show the linear relationship between each feature and the 'Income group' target variable. A coefficient close to 1 or -1 indicates a strong positive or negative linear relationship, respectively. A coefficient close to 0 suggests no linear relationship.\n\n\nEating disorders (%): With a correlation coefficient of approximately -0.52, this feature has the strongest negative linear relationship with the 'Income group' and is likely to be the most informative for predicting income group.\n\nAnxiety disorders (%), Schizophrenia (%), average_learning_Adjusted_of_school, Bipolar disorder (%), and Depression (%): All of these features have moderate negative correlations with the 'Income group', meaning as these percentages increase, the likelihood of being in a higher income group decreases.\n\nGDP(2022): This feature has a very weak negative correlation with the 'Income group', suggesting it might not be very useful for predicting the income group on its own.\n\nYear: The correlation is effectively zero, indicating no linear relationship with the 'Income group'.\n\n3.Chisquare test\n\nIn this example, features excluding 'Year' and 'GDP(2022)' are used for classification. The 'Year' column is dropped because it's not relevant for classification, and 'GDP(2022)' is dropped because it has a very weak correlation with the 'Income group' and is unlikely to be useful for classification.\n\n### HW-3.2.3: Naïve Bayes (NB) with Labeled Record Data\n\n## HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data\nLabel the data, either manually or through automated methods, based on your classification goals.\nPreprocess the text data by cleaning and tokenizing the text, normalizing it, and removing stop words.\nExtract features using methods like TF-IDF or count vectorization.\nTrain your Naive Bayes model, typically a Multinomial Naive Bayes for text data.\nEvaluate and refine your model, testing its accuracy and other metrics on a separate test set.\n\nload the data\n\nThe dataset is structured with two columns: 'Symptoms' and 'Label'.\nPreprocess the text data in the 'Symptoms' column.\nEncode the 'Label' column to numerical values, as Naive Bayes requires numerical input.\nSplit the data into training and testing sets.\nTrain a Naive Bayes classifier on the training set.\nEvaluate the model on the testing set.\n\nThe accuracy of the Naive Bayes model on the mental health symptoms dataset is not high: 0.56. This is likely due to the following factors:\nOverlap of Symptoms: Many mental health conditions have overlapping symptoms, which can confuse the model. For example, symptoms like \"anxiety\" or \"sleep disturbances\" can occur in multiple disorders.\n\nPreprocess the \"Symptoms\" text data with vectorization.\n\nEncode the \"Label\" column into numerical values.\n\nSplit the data into a training set and a test set.\n\nTrain a Naive Bayes model.\nEvaluate the model on the test data.\n","srcMarkdownNoYaml":"\n\n\n## HW-3.2.0: Introduction to Naive Bayes\n\nNaive Bayes classification is a probabilistic machine learning model that’s based on Bayes' Theorem. It’s called \"naive\" because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets.\n\n#### <b>Bayes' Theorem Foundation</b><br>\n\nAt its core, Naive Bayes relies on Bayes' Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For classification, Bayes' Theorem is stated as:\n\n\n\n\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\n\nHere,P(B|A) is the probability of hypothesis A given data B. P(B|A) is the probability of the data under the hypothesis, P(A) is the probability of the hypothesis before seeing the data, and P(B) is the probability of the data under any hypothesis.\n\n#### <b>Objectives of Naive Bayes Classification</b><br>\n\nThe objective in Naive Bayes is to determine the probability of a label given a set of features, and then predict the label with the highest probability. In other words, we calculate the posterior probability of each class given the input features and predict the class for which the posterior probability is the highest.\n\n#### <b>What Naive Bayes Aims to achieve</b><br>\n\nNaive Bayes aims to model the conditional probability of the class labels given the features in order to make predictions. It’s particularly advantageous in situations where the dimensionality of the input space is high, as it avoids the curse of dimensionality to a certain extent due to its feature independence assumption\n\n#### <b>Variants of Naive Bayes</b><br>\n\nThere are several variants of the Naive Bayes classifier, each appropriate for a different type of dataset:\n\n1. Gaussian Naive Bayes:\n\nUsed when features have a continuous distribution and an assumption of normal distribution (Gaussian) is reasonable.\nAppropriate for many real-world problems, like predicting whether a given email is spam or not.\n\n2. Multinomial Naive Bayes:\n\nUsed for discrete data where features represent counts or frequency counts of events.\nOften used in text classification, where the features are related to word counts or frequencies within the documents.\n\n3. Bernoulli Naive Bayes:\n\nUsed when features are binary (i.e., they take only two values like true and false).\nSuitable for making predictions from binary features, like if a word occurs in a document or not, which is a common scenario in text classification problems.\n\n\n##  HW-3.2.1: Prepare your Data for Naïve Bayes\n\n1.Handle missing values\n\n2.Encode the 'Income group' and 'Continent' categorical variables into a numerical format suitable for machine learning models.\n\n3.Split the data into training and test sets\n\nCheck missing values in the dataset\n\n##### 1.Handle missing values\na.Dropping columns with more than 1000 null values\n\n(We don't Remove Rows with Missing Values because although this is the simplest approach, but it can lead to a significant reduction in data size, which might not be ideal if the missing data is extensive.)\n\nso after this step, we dropped the column not_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\nb.Impute Missing Values:\n\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is a common strategy for numerical data.\n\nForward or Backward Fill: For time-series data, you can fill missing values with the next or previous values.\n\nPredictive Imputation: Use a machine learning algorithm to predict the missing values based on other data in the dataset.\n\nMean/Median Imputation: If the data is normally distributed, you might use the mean. If the distribution is skewed,the median might be more appropriate.\n\n so here we impute missing values using the mean for GDP(2022) and the median for average_learning_Adjusted_of_school (assuming the distributions are appropriate for these measures):\n\nc. Drop Rows ('Income group') with Missing Values:\n\nThe 'Income group' and 'Continent' columns still have missing values. Before we proceed with the Naive Bayes classification, we need to address these missing values. Since 'Income group' will be our target variable for classification, we cannot impute it as we did with the continuous variables. Instead, we should remove the rows with missing 'Income group' labels.\n\nAs for the 'Continent' column, it might not be necessary for our classification model if we're using economic and health indicators as features.(because we can just dont choose the feature in the feature selection). However, if we decide that continent information might be useful, we could either impute the missing values based on the 'Economy' column or drop the column if it's not required.\n\nThe dataset is now clean and ready for classification. We can proceed with the next step.   \n\n3.Encode the 'Income group' and 'Continent' categorical variables into a numerical format suitable for machine learning models and Split the data into training and test sets\n\n## HW-3.2.2: Feature selection\nDecide which features are relevant for our classification task(Naive Bayes model) and drop the rest.\n\nThe 'Income group' is a categorical variable that could be used as a label for classification, so we predict the 'Income group' based on health and economic indicators.\n\nmethod:\n 1. EDA to find the correlation between the features and the target variable (corralation matrix)\n\n\nStrong Correlation: Values above 0.7 or below -0.7 suggest a strong correlation between the variables.\nModerate Correlation: Values between 0.3 and 0.7 (or -0.3 and -0.7) suggest a moderate correlation.\nWeak Correlation: Values between 0 and 0.3 (or 0 and -0.3) suggest a weak correlation.\n\n2. Pearson Correlation\n\nThe Pearson correlation coefficients you've provided show the linear relationship between each feature and the 'Income group' target variable. A coefficient close to 1 or -1 indicates a strong positive or negative linear relationship, respectively. A coefficient close to 0 suggests no linear relationship.\n\n\nEating disorders (%): With a correlation coefficient of approximately -0.52, this feature has the strongest negative linear relationship with the 'Income group' and is likely to be the most informative for predicting income group.\n\nAnxiety disorders (%), Schizophrenia (%), average_learning_Adjusted_of_school, Bipolar disorder (%), and Depression (%): All of these features have moderate negative correlations with the 'Income group', meaning as these percentages increase, the likelihood of being in a higher income group decreases.\n\nGDP(2022): This feature has a very weak negative correlation with the 'Income group', suggesting it might not be very useful for predicting the income group on its own.\n\nYear: The correlation is effectively zero, indicating no linear relationship with the 'Income group'.\n\n3.Chisquare test\n\nIn this example, features excluding 'Year' and 'GDP(2022)' are used for classification. The 'Year' column is dropped because it's not relevant for classification, and 'GDP(2022)' is dropped because it has a very weak correlation with the 'Income group' and is unlikely to be useful for classification.\n\n### HW-3.2.3: Naïve Bayes (NB) with Labeled Record Data\n\n## HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data\nLabel the data, either manually or through automated methods, based on your classification goals.\nPreprocess the text data by cleaning and tokenizing the text, normalizing it, and removing stop words.\nExtract features using methods like TF-IDF or count vectorization.\nTrain your Naive Bayes model, typically a Multinomial Naive Bayes for text data.\nEvaluate and refine your model, testing its accuracy and other metrics on a separate test set.\n\nload the data\n\nThe dataset is structured with two columns: 'Symptoms' and 'Label'.\nPreprocess the text data in the 'Symptoms' column.\nEncode the 'Label' column to numerical values, as Naive Bayes requires numerical input.\nSplit the data into training and testing sets.\nTrain a Naive Bayes classifier on the training set.\nEvaluate the model on the testing set.\n\nThe accuracy of the Naive Bayes model on the mental health symptoms dataset is not high: 0.56. This is likely due to the following factors:\nOverlap of Symptoms: Many mental health conditions have overlapping symptoms, which can confuse the model. For example, symptoms like \"anxiety\" or \"sleep disturbances\" can occur in multiple disorders.\n\nPreprocess the \"Symptoms\" text data with vectorization.\n\nEncode the \"Label\" column into numerical values.\n\nSplit the data into a training set and a test set.\n\nTrain a Naive Bayes model.\nEvaluate the model on the test data.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"Naive Bayes.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Naive Bayes"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}