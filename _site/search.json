[
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Dimentional reduction happens before clusering My feature data includes the Schizophrenia (%), Bipolar disorder (%),Eating disorders (%),Anxiety disorders (%),Depression (%), average_learning_Adjusted_of_school,Continent,and the goal of the project is to cluster the economies into different groups (like Income group) based on the feature data(mental health, geolocation, education factors)"
  },
  {
    "objectID": "Clustering.html#introduction",
    "href": "Clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Dimentional reduction happens before clusering My feature data includes the Schizophrenia (%), Bipolar disorder (%),Eating disorders (%),Anxiety disorders (%),Depression (%), average_learning_Adjusted_of_school,Continent,and the goal of the project is to cluster the economies into different groups (like Income group) based on the feature data(mental health, geolocation, education factors)"
  },
  {
    "objectID": "Clustering.html#theory",
    "href": "Clustering.html#theory",
    "title": "Clustering",
    "section": "Theory",
    "text": "Theory\n\nKMEANS\nIt is a popular unsupervised machine learning algorithm, which is used to divide a given unlabeled dataset into a certain number of clusters, denoted as k. One example of this is imagine you are at a farmer’s market and you have a variety of fruits and you want to organize these fruits. And k-means clustering is you sorting these fruits into different bags according to their charactersitics, like color, size, shape, etc. So the first step is you need to determine the number of bags you want to sort these fruits(the number means K in K-means,which stands for the number of clusters). Then you randomly place these bags among the fruits and you put every friut to its cloest bag. After all the fruits are put into bags, you find the average characteristics of each bag and move the bag to the average position of the fruits in the bag. Then you repeat the process until the bags stop moving(The fruits no longer switch bags) And the result is you have a certain number of bags and each bag contains a certain number of fruits with similar characteristics. The goal is to have bags(clusters) where the fruits inside are more like each other than they are like fruits in other bags. And we use Elbow method, Silhouette method, Gap statistic to determine the number of bags we want to sort the fruits into.\n\nElbow method we increase the number of bags and get relavent scores of how close fruits are within bags, and in the plot we will look for the point where adding another bags doesn’t significantly improve the closeness - this point is the ‘elbow’.\nSilhouette method It is like each fruit can whisper tou you and tell you that how happy it is in its bag by give score. The Silhouette method is like listening to all those whispers to figure out if you’ve sorted the fruits into bags well by taking all these scores and averaging them to give an overall score. The score close to +1 means that the fruit is very happy because it’s surrounded by similar fruits, and the socre about 0 means the fruit is cool and don’t care about whcih bag it is in, and the score close to -1 means the fruit is very sad because it’s surrounded by very different fruits. if many fruits are sad or cool, the he overall score will be low, you may need to reorganize the fruits into different bags by adjusting the number of bags or considering different fruit characteristics.\n\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nIt is like organizing a group of children playing in a park into clusters based on how close they are to each other. we don’t need to decide how many groups you want beforehand, you set rules for what makes a group. Like a group needs at least five children, and all members of the group need to be within arm’s reach of someone else in the group. Children who are close enough to a group join it, and groups might merge if they get close enough to each other. Some children who prefer playing alone and don’t reach out to others are not part of any group(It is like the noise in the DBSCAN). DBSCAN works similarly. It groups points that are closely packed together, and points in low-density regions are marked as ‘noise’. This method is gvery ood when you don’t know how many clusters to expect, or when your data might have outliers. The silhouette score helps to evaluate how well the points are clustered, with higher scores indicating better-defined clusters.\n\n\nHierarchical Clustering\nIt is like building a family tree for a set of ancient artifacts found at an archaeological site. Instead of sorting them into separate bages, you try to understand how these artifacts are related to each other, from the oldest to the newest. Hierarchical clustering starts by assuming every artifact is its own family (cluster). Then, it gradually links artifacts into families by their similarities, like materials or designs, until all artifacts are united into one big family (cluster). This method creates a tree diagram called a dendrogram, which shows the ‘family’ relationships. It allows us to choose the level of similarity at which we want to stop combining artifacts into families, which in turn determines the number of clusters. We can use the dendrogram to visually identify where to ‘cut’ the tree to get a sensible number of families,with the elbow method or silhouette score serving as tools to help decide on the best ‘cut’, ensuring that artifacts in the same family are as similar as possible.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n#1.load data\nimport pandas as pd\ndf_reduction=pd.read_csv('./Data_Cleaned/mental_health_DR.csv')\n\n# # #2. drop columns that are not needed after dimentional redction (PCA) using 7 components\ndf_reduction = df_reduction.drop(['Code','Year','GDP_per_capita','GDP(2022)'], axis=1)\n# df_reduction = df_reduction.drop(['Code','Year'], axis=1)\n\n# 3.lable encoding: encode the categorical data into numeric \nlabel_encoder = LabelEncoder()\ndf_reduction['Economy'] = label_encoder.fit_transform(df_reduction['Economy'])\ndf_reduction['Continent'] = label_encoder.fit_transform(df_reduction['Continent'])\ndf_reduction['Income group'] = label_encoder.fit_transform(df_reduction['Income group'])\n# 4.Normalizing the Data (The data is normalized to ensure that all features contribute equally to the analysis.)\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_reduction)\nscaled_data\n\n# 5.Display the first few rows of the processed data\nscaled_df = pd.DataFrame(scaled_data, columns=df_reduction.columns)"
  },
  {
    "objectID": "Clustering.html#clustering",
    "href": "Clustering.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\n\n1.K-Means Clustering\n\nHyper-parameter tuning\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n\nscaled_df_1 = scaled_df.drop(['Income group','Economy'], axis=1)\n# scaled_df_1 = scaled_df.drop(['Economy'], axis=1)\n# Determining the optimal number of clusters for K-Means using Elbow Method and Silhouette Score\ninertia =  [] # inertia values for different clusters of K-Means\n#Inertia is the sum of squared distances of samples to their closest cluster center.\nsilhouette_scores = []  # silhouette scores for different clusters\nK_range = range(2, 11) # defining the range of clusters to try(2-10)\n\nfor K in K_range:\n    kmeans = KMeans(n_clusters=K, random_state=0)\n    kmeans.fit(scaled_df)\n    inertia.append( kmeans.inertia_) #iertia is the sum of squared distances of samples to their closest cluster center.\n    silhouette_scores.append(silhouette_score(scaled_df_1, kmeans.labels_)) #The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters.\n\n# Plotting the Elbow Method results\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(K_range, inertia, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method For Optimal K')\n\n# Plotting the Silhouette Scores\nplt.subplot(1, 2, 2)\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Scores For K-Means')\n\nplt.tight_layout()\nplt.show()\nprint(inertia)\nprint(silhouette_scores)\n\n\n# Finding the 'elbow' point in the inertia plot\n# This is typically done by visual inspection, but you can try an algorithmic approach\nelbow_point = K_range[inertia.index(min(inertia, key=lambda x: abs(x - inertia[0]/2)))]\n\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n[30224.42426701206, 24776.262278645874, 21425.974925155522, 18979.360836351985, 17537.826306181196, 16391.350489403358, 15552.844070334944, 14554.485295258723, 13752.362163928627]\n[0.36746635256470545, 0.2904971333710016, 0.28967372291716875, 0.24487465182983167, 0.13845810521541255, 0.14578798288274575, 0.1533430283320015, 0.15307904517690488, 0.18616601602319632]\n\n\n\n\nCode\n# Finding the highest silhouette score\nhighest_silhouette_score = max(silhouette_scores)\noptimal_clusters_silhouette = silhouette_scores.index(highest_silhouette_score) + 2  # Adding 2 because range starts from 2\nelbow_point, highest_silhouette_score, optimal_clusters_silhouette\n\n\n(8, 0.36746635256470545, 2)\n\n\nElbow Method: The inertia plot again shows the sum of squared distances to the closest cluster center. The ‘elbow’ in this plot suggests the optimal number of clusters. inertia decreases with increasing k, however, after which the rate of decrease slows down and the point is called the elbow, and in this case, it is 8.\nSilhouette Score: The silhouette score plot measures the quality of the clusters formed. Higher scores indicate more clearly defined clusters. The highest silhouette score is obtained with 2 clusters.\nThese two methods suggest different optimal numbers of clusters. This is not uncommon, as different methods can highlight different aspects of the data’s structure.\n\n\n\n2.DBSCAN Clustering\n\nHyper-parameter tuning\n\n\nCode\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift, Birch, DBSCAN\nimport seaborn as sns\n##eps_values will be tested as the eps parameter of the DBSCAN\n#algorithm to determine the optimal distance within which points are considered neighbors.\neps_values = np.arange(1.0, 5.0, 0.1) # eps_values is the maximum distance between two samples for one to be considered as in the neighborhood of the other,\n# Initialize the results list\nresults = []\n# Iterate over all the combinations of eps and min_samples\nfor eps in eps_values:\n    db = DBSCAN(eps=eps, min_samples=scaled_df_1.shape[1]+1) # min_samples must be be higher than the number of features at the very least(nd min_samples is the number of samples in a neighborhood for a point to be considered as a core point.)\n    clusters = db.fit_predict(scaled_df_1)\n    # Only calculate the silhouette score if there is more than one cluster\n    # DBSCAN can sometimes mark all or nearly all points as noise, especially if eps is to\n    if len(np.unique(clusters)) &gt; 1:\n        score = silhouette_score(scaled_df_1, clusters)\n        results.append((eps, scaled_df_1.shape[1]+1, len(np.unique(clusters)), score))\n# Convert the results list to a Pandas DataFrame\ndf = pd.DataFrame(results, columns=['eps', 'min_samples', 'number of clusters', 'silhouett score'])\nprint(df)\ndf.columns = df.columns.str.strip()\nprint(df.columns)\n# Plot the silhouette score vs the number of clusters\nsns.barplot(data=df, x = 'number of clusters', y = 'silhouett score')\nplt.ylabel('Silhouett score')\nplt.xlabel('Clusters')\nplt.title('Number of Clusters vs Silhouette Score')\nplt.show()\n\n\n    eps  min_samples  number of clusters  silhouett score\n0   1.0            8                  27         0.236474\n1   1.1            8                  25         0.252701\n2   1.2            8                  23         0.206666\n3   1.3            8                  22         0.209911\n4   1.4            8                  20         0.183295\n5   1.5            8                  16         0.172240\n6   1.6            8                  13         0.202367\n7   1.7            8                   9         0.172900\n8   1.8            8                   7         0.043934\n9   1.9            8                   5         0.068565\n10  2.0            8                   5         0.068565\n11  2.1            8                   4         0.113958\n12  2.2            8                   3         0.265151\n13  2.3            8                   3         0.265151\n14  2.4            8                   2         0.512050\n15  2.5            8                   2         0.512050\nIndex(['eps', 'min_samples', 'number of clusters', 'silhouett score'], dtype='object')\n\n\n\n\n\n\n\n\n3.Hierarchical Clustering\n\nHyper-parameter tuning\n\n\nCode\n# Perform Agglomerative Clustering\nAgg = AgglomerativeClustering\nsi_scores = []\nfor n in range(2,11):\n    agglomerative = Agg(n_clusters = n)\n    agglomerative.fit(scaled_df_1)\n    si_scores.append((n, silhouette_score(scaled_df_1, agglomerative.labels_)))\n    # The silhouette score is a measure of how similar an object is to its own cluster com\ndf = pd.DataFrame(si_scores, columns=[\"Clusters\",\"Silhouette score\"])\nprint(df)\nsns.barplot(data=df, x = 'Clusters', y = 'Silhouette score')\nplt.ylabel('Silhouette Score')\nplt.xlabel('Clusters')\nplt.title('Number of Clusters vs Silhouette Score')\nplt.show()\n\n\n   Clusters  Silhouette score\n0         2          0.408056\n1         3          0.298673\n2         4          0.303361\n3         5          0.324795\n4         6          0.333638\n5         7          0.362040\n6         8          0.369374\n7         9          0.375112\n8        10          0.382288\n\n\n\n\n\n\n\nCode\n### Plot the clusters for Agglomerative Clustering\n  # create linkage for agglomerative clustering, and the dendrogram for the linkage.\n  #  Suggest the optimal number of clusters based on the dendrogram.\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nlinkage_matrix = linkage(scaled_df_1, 'ward')\ndendrogram(linkage_matrix)\nplt.title('Dendrogram')\nplt.xlabel('Income group')\nplt.ylabel('Euclidean Distance')\nplt.show()"
  },
  {
    "objectID": "Clustering.html#final-results",
    "href": "Clustering.html#final-results",
    "title": "Clustering",
    "section": "Final results",
    "text": "Final results\nRe-do the analysis with the optimal parameter 2 using K-means, DBSCAN, and Hierarchical clustering.\n\n\nCode\n# Applying K-Means clustering with 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=10)\nkmeans.fit(scaled_df)  ## This line fits the KMeans model to your scaled dataset \n\n# Cluster labels\ncluster_labels = kmeans.labels_ ## This line extracts the cluster labels from the KMeans model\n\n# Adding the cluster labels to the original DataFrame\ndf_clustered = df_reduction.copy()\ndf_clustered['Cluster'] = cluster_labels\n\n# Analyzing the distribution of data points among the clusters\ncluster_distribution = df_clustered['Cluster'].value_counts() ## This line counts the number of data points in each cluster\n\n# Displaying the cluster centroids and distribution\ncentroids = kmeans.cluster_centers_ # This line retrieves the centroids of the clusters\nprint(df_clustered.head())\nprint(df_clustered.columns)\nprint(centroids)\nprint(cluster_distribution)\n\n\n/Users/bella/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n   Economy  Schizophrenia (%)  Bipolar disorder (%)  Eating disorders (%)  \\\n0        0           0.160560              0.697779              0.101855   \n1        0           0.160312              0.697961              0.099313   \n2        0           0.160135              0.698107              0.096692   \n3        0           0.160037              0.698257              0.094336   \n4        0           0.160022              0.698469              0.092439   \n\n   Anxiety disorders (%)  Depression (%)  Income group  \\\n0               4.828830        4.071831             1   \n1               4.829740        4.079531             1   \n2               4.831108        4.088358             1   \n3               4.830864        4.096190             1   \n4               4.829423        4.099582             1   \n\n   average_learning_Adjusted_of_school  Continent  Cluster  \n0                             4.957542          1        0  \n1                             4.957542          1        0  \n2                             4.957542          1        0  \n3                             4.957542          1        0  \n4                             4.957542          1        0  \nIndex(['Economy', 'Schizophrenia (%)', 'Bipolar disorder (%)',\n       'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)',\n       'Income group', 'average_learning_Adjusted_of_school', 'Continent',\n       'Cluster'],\n      dtype='object')\n[[ 0.0799854  -0.33779751 -0.36217225 -0.46548508 -0.38863328 -0.14531467\n   0.34315228 -0.30732618 -0.17622068]\n [-0.253082    1.06882596  1.14595014  1.47284255  1.22967554  0.45979052\n  -1.08576902  0.97241154  0.5575803 ]]\n0    3702\n1    1170\nName: Cluster, dtype: int64\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n# Assuming df_clustered is your DataFrame that includes PCA components and cluster assignments\n# Replace 'PC1' and 'PC2' with the names of your principal component columns\n# Replace 'Cluster' with the name of your cluster label column\n# Replace 'country' with the name of the column you want to show on hover\n\nfig = px.scatter(df_clustered, x='Schizophrenia (%)', y='Eating disorders (%)', color='Cluster', hover_data=['Economy'], template='simple_white')\n## Remove the colorbar\nfig.update(layout_coloraxis_showscale=False)\n# Make the points larger\nfig.update_traces(marker=dict(size=12),selector=dict(mode='markers'))\n# Update layout if necessary\nfig.update_layout(\n    title='K-Means Clustering with PCA Components',\n    xaxis_title='PC1',\n    yaxis_title='PC2',\n    legend_title='Cluster'\n)\n\nfig.show()\n\n\n\n                                                \n\n\nALl clustering analysis methods. They all sugested that the data can be clustered into 2 groups is the best after the parameter tuning, except for the elbow method in K-means clustering, which suggested that the data can be clustered into 8 groups is the best.\nUsing unsupervised learning methods(clustering), we can see the underlying information is discovered from the mental health data, whch is the unexpected groupings (cluster is 2)\nWhen clustering algorithms with K-Means, DBSCAN, or Hierarchical Clustering consistently yield a certain number of clusters that doesn’t match the expected number based on known labels (like my four income groups), it suggests a few possibilities: my data naturally falls into two distinct group, maybe there is a inherent data distribution, These groups might not align with predefined income categories, suggesting that the selected features (mental health factors, educational Factors and geographic factors) do not differentiate the income groups as clearly as expected."
  },
  {
    "objectID": "Clustering.html#conclusions",
    "href": "Clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nIn my investigation into the patterns and structures within my dataset, I applied various clustering techniques to uncover hidden groupings that may reflect real-world distinctions among the subjects. The essence of my findings reveals that the complexities of mental health, geolocation indicators and educational outcomes do not necessarily align neatly with economic categories such as income levels.\nUnexpectedly, rather than observing four distinct groupings corresponding to high, upper-middle, lower-middle, and low income levels, my analysis consistently highlighted two predominant clusters. This suggests that the features we examined—ranging from schizophrenia to depression rates, and the average learning adjustments in schools—might interplay in ways that transcend traditional economic divisions.\nThis has profound implications for policymakers, educators, and healthcare providers. It suggests that strategies aimed at addressing mental health and education should not be overly tailored to economic status alone, as these issues are not confined to any one financial demographic. Instead, more universal approaches that cross these boundaries may be necessary to effectively address the needs highlighted by my findings.\nMoreover, my analysis serves as a reminder of the complex human stories behind the data. Numbers and categories can only tell us so much; the lived experiences of individuals often defy simple categorization. This study underlines the importance of considering the full tapestry of human experience when crafting policies and interventions aimed at improving mental health and educational outcomes.\nIn conclusion, my exploration into the dataset has illuminated the intricate relationship between mental health, education, geolocation and income, underscoring the need for broad-spectrum solutions that are sensitive to the nuanced ways these domains interact. These insights can guide us toward more inclusive and effective approaches to supporting well-being and educational achievement across all economic strata."
  },
  {
    "objectID": "DT.html",
    "href": "DT.html",
    "title": "Decision Trees",
    "section": "",
    "text": "In this section, I am going to predict whether a country has a high or low prevalence of eating disorders, based on the following features (educational, economial and geological factors) using a decision tree and random forest algorithm.\nIn general, a a decision Trees (DT) are a type of model used in machine learning that mimic human decision-making processes. Imagine you’re trying to decide what to wear. You might look at the weather (a feature) and choose accordingly. A decision tree does something similar with data: it makes a series of choices based on the features to reach a conclusion, like deciding if an email is spam. It’s like a flowchart – starting with a question about the data, it branches out until it reaches an answer.\nRandom Forests (RF) build on decision trees. If a decision tree is like a single advisor making a decision, a random forest is like a committee. It creates multiple decision trees, each with slightly different perspectives, and then combines their votes to make a final decision. This helps in getting a more balanced and less biased result, as it avoids over-relying on a single perspective.\n\n\n\nThe data I am using here is preprocessed (cleaned,feature selected and converted to categorical data) in the section Naive Bayes' and saved asmental_health_binarized.csv`in the Data_cleand folder.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n\ndf_mental_health=pd.read_csv('./Data_cleaned/mental_health_binarized.csv')\n# Compute the distribution of class labels whicih is eating disorder\ndf_mental_health.head()\nclass_distribution = df_mental_health['Eating disorders (%)'].value_counts(normalize=True)\n\n# Output the distribution\nprint(class_distribution)\n\n# Visualize the distribution\nclass_distribution.plot(kind='bar')\nplt.title('Class Distribution')\nplt.show()\n\nLow     0.5\nHigh    0.5\nName: Eating disorders (%), dtype: float64\n\n\n\n\n\n\n\n\nTo understand the accuracy of the model, I created a baseline model to compare it to. A baseline model in machine learning and data science serves as a simple, often basic reference point for comparing the performance of more complex models.\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Prepare the data\nX = df_mental_health.drop('Eating disorders (%)', axis=1)\ny = df_mental_health['Eating disorders (%)']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit a random classifier\ndummy = DummyClassifier(strategy='stratified')\ndummy.fit(X_train, y_train)\n\n# Evaluate the classifier\ny_pred = dummy.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n        High       0.49      0.50      0.50       474\n         Low       0.52      0.51      0.52       501\n\n    accuracy                           0.51       975\n   macro avg       0.51      0.51      0.51       975\nweighted avg       0.51      0.51      0.51       975\n\n\n\n\n\n\nDecision Trees:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# Load and preprocess your data\ndf_mental_health.columns\nX = df_mental_health.drop(['Eating disorders (%)'],axis= 1) \ny = df_mental_health['Eating disorders (%)']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest_results = []\ntrain_results = []\n\nfor num_layer in range(1, 50):\n    # Include TfidfVectorizer in the pipeline\n    model = Pipeline([\n        ('dt', DecisionTreeClassifier(max_depth=num_layer))\n    ])\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    test_results.append([num_layer, accuracy_score(y_test, y_test_pred)])\n    train_results.append([num_layer, accuracy_score(y_train, y_train_pred)])\n\n# Create DataFrames for visualization\ntuning_train_df = pd.DataFrame(train_results, columns=[\"Tree_depth\", \"train_accuracy_score\"])\ntuning_test_df = pd.DataFrame(test_results, columns=[\"Tree_depth\", \"test_accuracy_score\"])\n# print(tuning_train_df)\n# print(tuning_test_df)\n#merge two dataframes\ntuning_df = pd.merge(tuning_train_df, tuning_test_df, on='Tree_depth')\nprint(tuning_df)\n# Plotting the results\nplt.figure(figsize=(12, 6))\nplt.plot(tuning_train_df['Tree_depth'], tuning_train_df['train_accuracy_score'], label='Train Accuracy')\nplt.plot(tuning_test_df['Tree_depth'], tuning_test_df['test_accuracy_score'], label='Test Accuracy')\nplt.xlabel('Tree Depth')\nplt.ylabel('Accuracy Score')\nplt.title('Model Tuning: Decision Tree Depth vs Accuracy')\nplt.legend()\nplt.show()\n\n    Tree_depth  train_accuracy_score  test_accuracy_score\n0            1              0.841930             0.827692\n1            2              0.867847             0.856410\n2            3              0.917372             0.902564\n3            4              0.928406             0.916923\n4            5              0.944573             0.937436\n5            6              0.957147             0.949744\n6            7              0.965615             0.953846\n7            8              0.976905             0.962051\n8            9              0.983577             0.969231\n9           10              0.988709             0.966154\n10          11              0.991789             0.970256\n11          12              0.993841             0.971282\n12          13              0.995894             0.972308\n13          14              0.996407             0.971282\n14          15              0.996664             0.972308\n15          16              0.996664             0.970256\n16          17              0.996664             0.972308\n17          18              0.996664             0.972308\n18          19              0.996664             0.973333\n19          20              0.996664             0.972308\n20          21              0.996664             0.973333\n21          22              0.996664             0.971282\n22          23              0.996664             0.970256\n23          24              0.996664             0.971282\n24          25              0.996664             0.970256\n25          26              0.996664             0.970256\n26          27              0.996664             0.970256\n27          28              0.996664             0.973333\n28          29              0.996664             0.971282\n29          30              0.996664             0.972308\n30          31              0.996664             0.970256\n31          32              0.996664             0.970256\n32          33              0.996664             0.971282\n33          34              0.996664             0.970256\n34          35              0.996664             0.971282\n35          36              0.996664             0.970256\n36          37              0.996664             0.972308\n37          38              0.996664             0.972308\n38          39              0.996664             0.971282\n39          40              0.996664             0.973333\n40          41              0.996664             0.970256\n41          42              0.996664             0.973333\n42          43              0.996664             0.970256\n43          44              0.996664             0.970256\n44          45              0.996664             0.973333\n45          46              0.996664             0.973333\n46          47              0.996664             0.971282\n47          48              0.996664             0.970256\n48          49              0.996664             0.971282\n\n\n\n\n\nRandom Forests:\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = df_mental_health.drop(['Eating disorders (%)'],axis= 1) \ny = df_mental_health['Eating disorders (%)']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n# Set the parameters by cross-validation\ntuned_parameters = {\n    'rf__n_estimators': [10, 20, 30],\n    'rf__max_depth': [None, 10, 20, 30],\n    'rf__min_samples_split': [2, 5, 10],\n    'rf__min_samples_leaf': [1, 2, 4],\n    'rf__max_features': ['sqrt', 'log2']\n}\n\n# Create a new pipeline with a RandomForestClassifier\nmodel_rf = Pipeline([\n    ('rf', RandomForestClassifier(random_state=42))\n])\n\n# Use GridSearchCV to tune the model\nclf = GridSearchCV(model_rf, tuned_parameters, cv=5, scoring='accuracy')\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint(clf.best_params_)\n\nBest parameters set found on development set:\n{'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 20}\n\n\n\n# Set the best parameters in the RandomForestClassifier\nbest_params_rf = {\n    'n_estimators': 20, \n    'max_depth': None,\n    'min_samples_leaf': 1, \n    'min_samples_split': 2, \n    'max_features': 'sqrt'\n}\n\n# Create the pipeline with TfidfVectorizer and RandomForestClassifier with the best parameters\nmodel_rf = Pipeline([\n    ('rf', RandomForestClassifier(**best_params_rf, random_state=42))\n])\n\n# Train the model with the best parameters on the training data\nmodel_rf.fit(X_train, y_train)\n\n# Predict on the training and test sets\ny_train_pred_rf = model_rf.predict(X_train)\ny_test_pred_rf = model_rf.predict(X_test)\n\n# Calculate the accuracy scores for the training and test sets\ntrain_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\ntest_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n\n# Print the accuracy scores\nprint(f\"Random Forest training accuracy with best parameters: {train_accuracy_rf * 100:.2f}%\")\nprint(f\"Random Forest testing accuracy with best parameters: {test_accuracy_rf * 100:.2f}%\")\n# Plot\nmodel_accuracies = pd.DataFrame({'Set':['Training set','Test set'], 'Accuracy (%)': [accuracy_score(y_train, y_train_pred) * 100, accuracy_score(y_test, y_test_pred)*100]})\nsns.barplot(data=model_accuracies, x=\"Set\", y=\"Accuracy (%)\").set(title = 'Accuracy of model for training vs test sets' )\n\nRandom Forest training accuracy with best parameters: 99.67%\nRandom Forest testing accuracy with best parameters: 97.85%\n\n\n[Text(0.5, 1.0, 'Accuracy of model for training vs test sets')]\n\n\n\n\n\n\n# print(model.named_steps['dt'].classes_)\n# # The first element is what '0' represents, and the second element is what '1' represents.\n\n\n\n\nDecision Trees: The decesion tree has an accurcay of 0.996664 on the traning dataset and an accurcy of 0.973333 on the test dataset when the num_layer is set to 18.  Initially, as the tree depth increases, there’s an improvement in both training and testing accuracy. However, beyond a certain point, the increase in training accuracy does not translate to an improvement in testing accuracy. This suggests that the optimal tree depth for this model, where it balances learning from the training data without losing generalizability.\nRandom Forests: The random forest has an accurcay of 99.67% on the traning dataset and an accurcy of 97.85% on the test dataset.\nAnd also while a high training accuracy score may seem positive at first glance, it can be indicative of overfitting when it is not accompanied by a corresponding high test accuracy.\n\n\n\n\n\n\n\n\nTraning dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_train, model.predict(X_train))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29c3ff650&gt;\n\n\n\n\n\n\nTest dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_test, model.predict(X_test))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29c52c310&gt;\n\n\n\n\n\n\n\n\n\n\n#decesion tree visualization\nfrom sklearn.tree import plot_tree\n\n# Fit the model with desired tree depth(3)\ndesired_depth = 18\nmodel = Pipeline([\n    ('dt', DecisionTreeClassifier(max_depth=desired_depth))\n])\nmodel.fit(X_train, y_train)\n\n# Visualize the tree\nplt.figure(figsize=(20,10))\nplot_tree(model.named_steps['dt'], filled=True, class_names=True)\nplt.show()\n\n\n\n\n\n\n\nTraning dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_train, model_rf.predict(X_train))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29bf3b190&gt;\n\n\n\n\n\n\nTest dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_test, model_rf.predict(X_test))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2996d1bd0&gt;"
  },
  {
    "objectID": "DT.html#methods",
    "href": "DT.html#methods",
    "title": "Decision Trees",
    "section": "",
    "text": "In this section, I am going to predict whether a country has a high or low prevalence of eating disorders, based on the following features (educational, economial and geological factors) using a decision tree and random forest algorithm.\nIn general, a a decision Trees (DT) are a type of model used in machine learning that mimic human decision-making processes. Imagine you’re trying to decide what to wear. You might look at the weather (a feature) and choose accordingly. A decision tree does something similar with data: it makes a series of choices based on the features to reach a conclusion, like deciding if an email is spam. It’s like a flowchart – starting with a question about the data, it branches out until it reaches an answer.\nRandom Forests (RF) build on decision trees. If a decision tree is like a single advisor making a decision, a random forest is like a committee. It creates multiple decision trees, each with slightly different perspectives, and then combines their votes to make a final decision. This helps in getting a more balanced and less biased result, as it avoids over-relying on a single perspective."
  },
  {
    "objectID": "DT.html#data",
    "href": "DT.html#data",
    "title": "Decision Trees",
    "section": "",
    "text": "The data I am using here is preprocessed (cleaned,feature selected and converted to categorical data) in the section Naive Bayes' and saved asmental_health_binarized.csv`in the Data_cleand folder.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n\ndf_mental_health=pd.read_csv('./Data_cleaned/mental_health_binarized.csv')\n# Compute the distribution of class labels whicih is eating disorder\ndf_mental_health.head()\nclass_distribution = df_mental_health['Eating disorders (%)'].value_counts(normalize=True)\n\n# Output the distribution\nprint(class_distribution)\n\n# Visualize the distribution\nclass_distribution.plot(kind='bar')\nplt.title('Class Distribution')\nplt.show()\n\nLow     0.5\nHigh    0.5\nName: Eating disorders (%), dtype: float64"
  },
  {
    "objectID": "DT.html#baseline-model-for-comparison",
    "href": "DT.html#baseline-model-for-comparison",
    "title": "Decision Trees",
    "section": "",
    "text": "To understand the accuracy of the model, I created a baseline model to compare it to. A baseline model in machine learning and data science serves as a simple, often basic reference point for comparing the performance of more complex models.\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Prepare the data\nX = df_mental_health.drop('Eating disorders (%)', axis=1)\ny = df_mental_health['Eating disorders (%)']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit a random classifier\ndummy = DummyClassifier(strategy='stratified')\ndummy.fit(X_train, y_train)\n\n# Evaluate the classifier\ny_pred = dummy.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n        High       0.49      0.50      0.50       474\n         Low       0.52      0.51      0.52       501\n\n    accuracy                           0.51       975\n   macro avg       0.51      0.51      0.51       975\nweighted avg       0.51      0.51      0.51       975"
  },
  {
    "objectID": "DT.html#model-tuning",
    "href": "DT.html#model-tuning",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision Trees:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# Load and preprocess your data\ndf_mental_health.columns\nX = df_mental_health.drop(['Eating disorders (%)'],axis= 1) \ny = df_mental_health['Eating disorders (%)']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest_results = []\ntrain_results = []\n\nfor num_layer in range(1, 50):\n    # Include TfidfVectorizer in the pipeline\n    model = Pipeline([\n        ('dt', DecisionTreeClassifier(max_depth=num_layer))\n    ])\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    test_results.append([num_layer, accuracy_score(y_test, y_test_pred)])\n    train_results.append([num_layer, accuracy_score(y_train, y_train_pred)])\n\n# Create DataFrames for visualization\ntuning_train_df = pd.DataFrame(train_results, columns=[\"Tree_depth\", \"train_accuracy_score\"])\ntuning_test_df = pd.DataFrame(test_results, columns=[\"Tree_depth\", \"test_accuracy_score\"])\n# print(tuning_train_df)\n# print(tuning_test_df)\n#merge two dataframes\ntuning_df = pd.merge(tuning_train_df, tuning_test_df, on='Tree_depth')\nprint(tuning_df)\n# Plotting the results\nplt.figure(figsize=(12, 6))\nplt.plot(tuning_train_df['Tree_depth'], tuning_train_df['train_accuracy_score'], label='Train Accuracy')\nplt.plot(tuning_test_df['Tree_depth'], tuning_test_df['test_accuracy_score'], label='Test Accuracy')\nplt.xlabel('Tree Depth')\nplt.ylabel('Accuracy Score')\nplt.title('Model Tuning: Decision Tree Depth vs Accuracy')\nplt.legend()\nplt.show()\n\n    Tree_depth  train_accuracy_score  test_accuracy_score\n0            1              0.841930             0.827692\n1            2              0.867847             0.856410\n2            3              0.917372             0.902564\n3            4              0.928406             0.916923\n4            5              0.944573             0.937436\n5            6              0.957147             0.949744\n6            7              0.965615             0.953846\n7            8              0.976905             0.962051\n8            9              0.983577             0.969231\n9           10              0.988709             0.966154\n10          11              0.991789             0.970256\n11          12              0.993841             0.971282\n12          13              0.995894             0.972308\n13          14              0.996407             0.971282\n14          15              0.996664             0.972308\n15          16              0.996664             0.970256\n16          17              0.996664             0.972308\n17          18              0.996664             0.972308\n18          19              0.996664             0.973333\n19          20              0.996664             0.972308\n20          21              0.996664             0.973333\n21          22              0.996664             0.971282\n22          23              0.996664             0.970256\n23          24              0.996664             0.971282\n24          25              0.996664             0.970256\n25          26              0.996664             0.970256\n26          27              0.996664             0.970256\n27          28              0.996664             0.973333\n28          29              0.996664             0.971282\n29          30              0.996664             0.972308\n30          31              0.996664             0.970256\n31          32              0.996664             0.970256\n32          33              0.996664             0.971282\n33          34              0.996664             0.970256\n34          35              0.996664             0.971282\n35          36              0.996664             0.970256\n36          37              0.996664             0.972308\n37          38              0.996664             0.972308\n38          39              0.996664             0.971282\n39          40              0.996664             0.973333\n40          41              0.996664             0.970256\n41          42              0.996664             0.973333\n42          43              0.996664             0.970256\n43          44              0.996664             0.970256\n44          45              0.996664             0.973333\n45          46              0.996664             0.973333\n46          47              0.996664             0.971282\n47          48              0.996664             0.970256\n48          49              0.996664             0.971282\n\n\n\n\n\nRandom Forests:\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = df_mental_health.drop(['Eating disorders (%)'],axis= 1) \ny = df_mental_health['Eating disorders (%)']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n# Set the parameters by cross-validation\ntuned_parameters = {\n    'rf__n_estimators': [10, 20, 30],\n    'rf__max_depth': [None, 10, 20, 30],\n    'rf__min_samples_split': [2, 5, 10],\n    'rf__min_samples_leaf': [1, 2, 4],\n    'rf__max_features': ['sqrt', 'log2']\n}\n\n# Create a new pipeline with a RandomForestClassifier\nmodel_rf = Pipeline([\n    ('rf', RandomForestClassifier(random_state=42))\n])\n\n# Use GridSearchCV to tune the model\nclf = GridSearchCV(model_rf, tuned_parameters, cv=5, scoring='accuracy')\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint(clf.best_params_)\n\nBest parameters set found on development set:\n{'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 20}\n\n\n\n# Set the best parameters in the RandomForestClassifier\nbest_params_rf = {\n    'n_estimators': 20, \n    'max_depth': None,\n    'min_samples_leaf': 1, \n    'min_samples_split': 2, \n    'max_features': 'sqrt'\n}\n\n# Create the pipeline with TfidfVectorizer and RandomForestClassifier with the best parameters\nmodel_rf = Pipeline([\n    ('rf', RandomForestClassifier(**best_params_rf, random_state=42))\n])\n\n# Train the model with the best parameters on the training data\nmodel_rf.fit(X_train, y_train)\n\n# Predict on the training and test sets\ny_train_pred_rf = model_rf.predict(X_train)\ny_test_pred_rf = model_rf.predict(X_test)\n\n# Calculate the accuracy scores for the training and test sets\ntrain_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\ntest_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n\n# Print the accuracy scores\nprint(f\"Random Forest training accuracy with best parameters: {train_accuracy_rf * 100:.2f}%\")\nprint(f\"Random Forest testing accuracy with best parameters: {test_accuracy_rf * 100:.2f}%\")\n# Plot\nmodel_accuracies = pd.DataFrame({'Set':['Training set','Test set'], 'Accuracy (%)': [accuracy_score(y_train, y_train_pred) * 100, accuracy_score(y_test, y_test_pred)*100]})\nsns.barplot(data=model_accuracies, x=\"Set\", y=\"Accuracy (%)\").set(title = 'Accuracy of model for training vs test sets' )\n\nRandom Forest training accuracy with best parameters: 99.67%\nRandom Forest testing accuracy with best parameters: 97.85%\n\n\n[Text(0.5, 1.0, 'Accuracy of model for training vs test sets')]\n\n\n\n\n\n\n# print(model.named_steps['dt'].classes_)\n# # The first element is what '0' represents, and the second element is what '1' represents."
  },
  {
    "objectID": "DT.html#final-results",
    "href": "DT.html#final-results",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision Trees: The decesion tree has an accurcay of 0.996664 on the traning dataset and an accurcy of 0.973333 on the test dataset when the num_layer is set to 18.  Initially, as the tree depth increases, there’s an improvement in both training and testing accuracy. However, beyond a certain point, the increase in training accuracy does not translate to an improvement in testing accuracy. This suggests that the optimal tree depth for this model, where it balances learning from the training data without losing generalizability.\nRandom Forests: The random forest has an accurcay of 99.67% on the traning dataset and an accurcy of 97.85% on the test dataset.\nAnd also while a high training accuracy score may seem positive at first glance, it can be indicative of overfitting when it is not accompanied by a corresponding high test accuracy."
  },
  {
    "objectID": "DT.html#evaluation",
    "href": "DT.html#evaluation",
    "title": "Decision Trees",
    "section": "",
    "text": "Traning dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_train, model.predict(X_train))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29c3ff650&gt;\n\n\n\n\n\n\nTest dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_test, model.predict(X_test))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29c52c310&gt;\n\n\n\n\n\n\n\n\n\n\n#decesion tree visualization\nfrom sklearn.tree import plot_tree\n\n# Fit the model with desired tree depth(3)\ndesired_depth = 18\nmodel = Pipeline([\n    ('dt', DecisionTreeClassifier(max_depth=desired_depth))\n])\nmodel.fit(X_train, y_train)\n\n# Visualize the tree\nplt.figure(figsize=(20,10))\nplot_tree(model.named_steps['dt'], filled=True, class_names=True)\nplt.show()\n\n\n\n\n\n\n\nTraning dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_train, model_rf.predict(X_train))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29bf3b190&gt;\n\n\n\n\n\n\nTest dataset\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm1 = confusion_matrix(y_test, model_rf.predict(X_test))\n# Plot using ConfusionMatrixDisplay\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1)\ndisp1.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2996d1bd0&gt;"
  },
  {
    "objectID": "Conclusions.html",
    "href": "Conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Q: What are the range of mental health disorder types, and how prevalent are they in the population?\nA: There are 5 types ofmental health disorder, which includes Anxiety, Depression, Eating Disorders, Bipolar and Schizophrenia. Anxiety disorders and depression show higher prevalence rates compared to other disorders like schizophrenia, bipolar disorder, and eating disorders. This could indicate that anxiety and depression are more common mental health concerns in the population.\n\n\nQ:How has this prevalence changed over time in the population?\nA: The prevalence rates for disorders like schizophrenia, bipolar disorder, eating disorders and anxiety disorders keep increaseing over the years, while the depression retes does not have a liner increasement.it has a peak in 1997 and then decrease to a lower level.\n\n\nQ: What is the relationship between socialeconimic factors(income(high-income, lower-middle-income, upper-middle-income), GDP, GDP_per_capita, and average outcomes) correlate with mental health outcomes (depression,anxiety,eating disorders, bipolar disorder)?\n\n\nQ: How does the prevalence of mental disorders vary by income group?\nA: The Box Plot above shows the distribution of ‘Bipolar disorder (%)’, ‘Schizophrenia (%)’, ‘Eating disorders (%)’, ’Anxiety disorders (%)’prevalence vary differently by income group, while the Depression doesn’t vary accoringly by income group.\n\n\nQ: How does mental health vary by education level(average_adjusted_years_of_school)\nA: As education levels increase, the prevalence rates of Schizophrenia seem to spread out, while others don’t show a clear trend indicating a strong relationship with education level. This could indicate that other factors msy be more influential in determining the prevalence of these mental health disorders.\n\n\nQ: How does mental health vary by GDP and GDP_per_capita?\nA: The scatter plots show that they does not appear to be a clear trend or correlation between GDP and the prevalence of these mental health problems. As GDP per capita increases, the prevalence rates of eating disorder seems to spread out, while others don’t show a clear trend indicating a strong relationship with gdp per capita.\n\n\nQ: How does eating order prevalence changed over time in the male and female?\nA: The line plot above shows the global average prevalence of eating disorders over time for males, females, and all genders combined. The trends can be compared to see how the prevalence has changed over the years. Eating disorders were more than twice as prevalent among females (3.8%) than males (1.5%)\n\n\nQ: what is eating disorder prevalence in US increases? Or decrease?\nA: In years 1993 and 2012 show the highest prevalence rates. From 2013 onwards, there appears to be a slight decrease in prevalence, with rates consistently around 0.42% to 0.43%\n\n\nQ: In what age when people first have depression or anxiety in the world?\nA: The largest segment of the chart, making up 34.9%, is unspecified and labeled “Don’t know/Refused,” indicating a significant portion of respondents either did not know or refused to provide their age at first experience of anxiety or depression; The second-largest group is individuals aged 20–29, representing 22.2% of the distribution. This suggests that early adulthood is a common time for individuals to experience their first instance of these mental health issues; Those aged 30–39 make up 12.5% of the distribution, while individuals aged 40 and over account for 11.4%; Adolescents aged 13–19 represent 11.5% of the distribution, indicating that the onset of anxiety or depression can also begin during teenage years; The smallest identified age group is children under the age of 13, at 7.5%, which highlights that while less common, young children can also experience these mental health issues.Overall, the data suggests that the onset of anxiety or depression is reported most frequently during the ages of 20–29 and that there’s a notable amount of uncertainty or unwillingness to provide information regarding the age of first experience with these conditions.\n\n\nQ:Do age when people first have depression or anxiety in the world vary by continents and income level?\nA: High Uncertainty or Non-disclosure: ‘Don’t know/Refused’ responses are notably high across all entities, with Africa recording the highest at 42.7% ,lower-middle income countries at 40.6% and Europe the lowest at 14.5%. This indicates a significant amount of data is either not known or not disclosed.\nHigh-Income Countries: except for ‘Don’t know/Refused’, The highest ratios are observed in the ‘Ages 13_19’and lowest in the Age &lt; 13. In these countries, the age group ’13–19’ has the highest reported ratios of first anxiety or depression onset when compared to other age groups, with the lowest incidence reported in children under 13. This could suggest that adolescence is a particularly vulnerable period for the development of these conditions in high-income countries, potentially due to a combination of social pressures, awareness, and better reporting.\nUpper-middle income countries: except for ‘Don’t know/Refused’, The ratios are more balanced across the age categories, with the lowest ratio STILL in age &lt;13.\nlower-middle income countries: except for ‘Don’t know/Refused’,The highest ratios are observed in the ‘Ages 20_29’ and it is almost 3 times higher than other age categories.\nThe ‘Ages 20–29’ group shows the highest ratios, nearly tripling those of other age groups. This pronounced increase could reflect specific socio-economic stresses or life transitions that are characteristic of this age group, such as entering the workforce or higher education.\nEurope: The ratios are more balanced across the age categories, with the lowest ratio in age &lt;13.\nAfrica and Asia: The ratios are Highest in the ‘Ages 20_29’ and lowest in the Age &lt; 13, but the ratio in Asia is higher than Africa.\nThere is a global trend where the incidence of first anxiety or depression is lower in children under 13 compared to older age groups. This could be due to underdiagnosis or underreporting in this age group, or a genuine lower prevalence.\nThe high levels of ‘Don’t know/Refused’ responses call for improvements in mental health data collection and accessibility, as well as efforts to reduce stigma associated with mental health issues, to ensure more accurate and comprehensive data.\n\n\nQ: Which set of factors “better” explains mental health outcomes: socioeconomic or geographic or educational factors?\nA: Socioeconomic Factors: These include GDP (2022), GDP per capita, and income groups (high, low, lower middle, upper middle). High-income groups have strong positive correlations with mental health outcomes, particularly with eating disorders (0.74), and Schizophrenia(0.58). This suggests that in high-income groups, there may be a higher reported prevalence or better diagnosis of these conditions. On the other hand, low-income groups show a moderate negative correlation with some mental health outcomes, indicating a lower reported prevalence or underdiagnosis in these groups.\nEducational Factors: The variable ‘average_learning_Adjusted_of_school’ shows strong positive correlations with eating disorders (0.69), and Schizophrenia(0.68). This suggests that higher education levels are associated with a higher reported prevalence of these conditions, which could be due to increased awareness and reporting in more educated populations.\nGeographic Factors: The geographic regions show varied correlations with mental health outcomes. For example, being in Africa has a strong negative correlation with schizophrenia (-0.53) and a moderate negative correlation with eating disorder (-0.39). In contrast, Europe shows a moderate positive correlation with eating disorders (0.43). These correlations suggest that geographic location can have a significant impact on the reporting and prevalence of mental health conditions, possibly due to cultural, environmental, or healthcare-related factors.\nIn summary, educational factors, indicated by ‘average_learning_Adjusted_of_school’, show the most consistent strong positive correlations across several mental health outcomes, suggesting that education level may be a strong predictor of mental health outcomes, potentially due to greater awareness and reporting. Socioeconomic factors, particularly income level, also show strong correlations, especially in high-income groups. Geographic factors show both strong negative and moderate positive correlations, indicating a more complex relationship with mental health outcomes that could be influenced by a variety of regional factors.\n\n\nQ: Do the mental health indicators and educational outcomes necessarily align neatly with economic categories such as income levels?\nA: The essence of my findings reveals that the complexities of mental health, geolocation indicators and educational outcomes do not necessarily align neatly with economic categories such as income levels which are defined to 4 levels.\n\n\nQ: Can we use educational, economic, and geological factors to predict if a economy have a low or high eating disorders rate? \nA: Yes. We employed a Naive bayes, random forests algorithm and decesion trees to predict the prevalence of eating disorders based on a set of educational, socio-economic and geographical features. The confusion matrix results indicate a high level of accuracy, suggesting that the chosen features—average learning years of school, GDP per capita, total GDP, and continent—have significant predictive power."
  },
  {
    "objectID": "Exploration.html",
    "href": "Exploration.html",
    "title": "Exploration",
    "section": "",
    "text": "Code\nimport pandas as pd\n#load the data sets\ndf_age_first_depression=pd.read_csv('./Data_cleaned/age_when_first_anxiety_or_depression.csv')\ndf_eating_disorder=pd.read_csv('./Data_cleaned/eating_disorder_male_female.csv')\ndf_mental_health=pd.read_csv('./Data_cleaned/mental_health.csv')\ndf_mental_physical=pd.read_csv('./Data_cleaned/symptoms.csv')\n\ndf_gdp_mental_health_2017=pd.read_csv('./Data/GDP_percaptita_mental_health_2017.csv')"
  },
  {
    "objectID": "Exploration.html#data-understanding",
    "href": "Exploration.html#data-understanding",
    "title": "Exploration",
    "section": "Data Understanding",
    "text": "Data Understanding\n 1.Mental Health Dataset\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicators like income group, average learning adjusted years of school, continent, GDP for 2022, and a column related to comfort speaking about anxiety or depression, which has many missing values (NaN).\nEconomy: The name of the economy or country. Code: The country code.\nYear: The year of the data record.\nSchizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage.\nEating disorders (%): The prevalence of eating disorders as a percentage.\nAnxiety disorders (%): The prevalence of anxiety disorders as a percentage.\nDepression (%): The prevalence of depression as a percentage.\nIncome group: The income group classification of the economy.\naverage_learning_Adjusted_of_school: Some metric related to schooling, perhaps average years of schooling adjusted for learning.\nContinent: The continent where the economy is located.\nGDP(2022): The GDP of the economy for the year 2022.\nGDP_per_capita(2022): The GDP per capita of the economy.\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent: The percentage of people not at all comfortable speaking about anxiety or depression\nRecords: 5,488 Variables: 14 (1 integer, 9 floats, 4 objects) Features: Includes country data, year, percentages for various mental health conditions, income group, average learning, continent, GDP, GDP per capita, and comfort speaking about anxiety/depression.\n\n\nCode\nname='mental_health'\ndf=df_mental_health\nprint(f\"{'mental_health'} Dataset - First 5 Rows:\")\ndisplay(df_mental_health.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\n\nmental_health Dataset - First 5 Rows:\n\nmental_health Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5488 entries, 0 to 5487\nData columns (total 14 columns):\n #   Column                                                         Non-Null Count  Dtype  \n---  ------                                                         --------------  -----  \n 0   Economy                                                        5488 non-null   object \n 1   Code                                                           5488 non-null   object \n 2   Year                                                           5488 non-null   int64  \n 3   Schizophrenia (%)                                              5488 non-null   float64\n 4   Bipolar disorder (%)                                           5488 non-null   float64\n 5   Eating disorders (%)                                           5488 non-null   float64\n 6   Anxiety disorders (%)                                          5488 non-null   float64\n 7   Depression (%)                                                 5488 non-null   float64\n 8   Income group                                                   5432 non-null   object \n 9   average_learning_Adjusted_of_school                            4676 non-null   float64\n 10  Continent                                                      4900 non-null   object \n 11  GDP(2022)                                                      5264 non-null   float64\n 12  not_at_all_comfortable_speaking_anxiety_or_depression_percent  3108 non-null   float64\n 13  GDP_per_capita                                                 4468 non-null   float64\ndtypes: float64(9), int64(1), object(4)\nmemory usage: 600.4+ KB\nmental_health Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent\nGDP_per_capita\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.160560\n0.697779\n0.101855\n4.828830\n4.071831\nLow income\n4.957542\nAsia\n14583.0\nNaN\nNaN\n\n\n1\nAfghanistan\nAFG\n1991\n0.160312\n0.697961\n0.099313\n4.829740\n4.079531\nLow income\n4.957542\nAsia\n14583.0\nNaN\nNaN\n\n\n2\nAfghanistan\nAFG\n1992\n0.160135\n0.698107\n0.096692\n4.831108\n4.088358\nLow income\n4.957542\nAsia\n14583.0\nNaN\nNaN\n\n\n3\nAfghanistan\nAFG\n1993\n0.160037\n0.698257\n0.094336\n4.830864\n4.096190\nLow income\n4.957542\nAsia\n14583.0\nNaN\nNaN\n\n\n4\nAfghanistan\nAFG\n1994\n0.160022\n0.698469\n0.092439\n4.829423\n4.099582\nLow income\n4.957542\nAsia\n14583.0\nNaN\nNaN\n\n\n\n\n\n\n\nNone\n\n\n(5488, 14)\n\n\n 2. Eating Disorder Dataset \nThis dataset includes the prevalence of eating disorders among males and females, as well as a combined figure for all genders, across different countries and years.\nCountry: The name of the country.\nCountry Code: The corresponding country code.\nYear: The year of the observation.\nEating_disorders_Male: The prevalence of eating disorders among males.\nEating_disorders_Female: The prevalence of eating disorders among females.\nAll_gender: The prevalence of eating disorders across all genders.\nRecords: 6,420 Variables: 6 (1 integer, 3 floats, 2 objects) Features: Includes country data, year, and eating disorder prevalence separated by male, female, and all genders.\n\n\nCode\nname='eating_disorder'\ndf=df_eating_disorder\n\nprint(f\"{'eating_disorder'} Dataset - First 5 Rows:\")\ndisplay(df_eating_disorder.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\n\neating_disorder Dataset - First 5 Rows:\n\neating_disorder Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6420 entries, 0 to 6419\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Economy                  6420 non-null   object \n 1   Code                     6150 non-null   object \n 2   Year                     6420 non-null   int64  \n 3   Eating_disorders_Male    6420 non-null   float64\n 4   Eating_disorders_Female  6420 non-null   float64\n 5   All_gender               6420 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 301.1+ KB\neating_disorder Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nEating_disorders_Male\nEating_disorders_Female\nAll_gender\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.088487\n0.161867\n0.125177\n\n\n1\nAfghanistan\nAFG\n1991\n0.086048\n0.156910\n0.121479\n\n\n2\nAfghanistan\nAFG\n1992\n0.083625\n0.152412\n0.118018\n\n\n3\nAfghanistan\nAFG\n1993\n0.081628\n0.147938\n0.114783\n\n\n4\nAfghanistan\nAFG\n1994\n0.079439\n0.143980\n0.111710\n\n\n\n\n\n\n\nNone\n\n\n(6420, 6)\n\n\n 3. Age When First Anxiety or Depression Dataset\nThe dataset contains information on the age at which individuals first experienced anxiety or depression, categorized by different entities (which seem to represent regions or income categories).\nEntity: The region or income category.\nAge: The age category when anxiety or depression was first experienced.\nPercentage: The percentage of individuals in that entity and age category.\nRecords: 42 Variables: 3 (2 objects, 1 float) Features: Includes entity (could be country or other types of entities), age category, and percentage of individuals with first anxiety or depression experience.\n\n\nCode\nname='age_first_depression'\ndf=df_age_first_depression\nprint(f\"{'age_first_depression'} Dataset - First 5 Rows:\")\ndisplay(df_age_first_depression.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\n\nage_first_depression Dataset - First 5 Rows:\n\nage_first_depression Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Entity      42 non-null     object \n 1   Age         42 non-null     object \n 2   Percentage  42 non-null     float64\ndtypes: float64(1), object(2)\nmemory usage: 1.1+ KB\nage_first_depression Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEntity\nAge\nPercentage\n\n\n\n\n0\nAfrica\nAges &lt;13\n1.271836\n\n\n1\nAsia\nAges &lt;13\n7.795371\n\n\n2\nEurope\nAges &lt;13\n9.083381\n\n\n3\nHigh-income countries\nAges &lt;13\n2.473921\n\n\n4\nLower-middle-income countries\nAges &lt;13\n8.800553\n\n\n\n\n\n\n\nNone\n\n\n(42, 3)\n\n\n\n\nCode\nname='df_gdp_mental_health_2017'\ndf=df_gdp_mental_health_2017\nprint(f\"{'name'} Dataset - First 5 Rows:\")\ndisplay(df.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\n\nname Dataset - First 5 Rows:\n\ndf_gdp_mental_health_2017 Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 166 entries, 0 to 165\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Economy                166 non-null    object \n 1   2017                   166 non-null    object \n 2   Schizophrenia (%)      166 non-null    float64\n 3   Bipolar disorder (%)   166 non-null    float64\n 4   Eating disorders (%)   166 non-null    float64\n 5   Anxiety disorders (%)  166 non-null    float64\n 6   Depression (%)         166 non-null    float64\ndtypes: float64(5), object(2)\nmemory usage: 9.2+ KB\ndf_gdp_mental_health_2017 Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n\n\n\n\n\n\n\nNone\n\n\n(166, 7)\n\n\n 4.Symtoms dataset\nThis dataset contains 2 columns,one is symtoms of health problem and another is related to the respective health problem(mental or Physical).\n\n\nCode\nname='df_mental_physical'\ndf=df_mental_physical\nprint(f\"{'name'} Dataset - First 5 Rows:\")\ndisplay(df.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\n\nname Dataset - First 5 Rows:\n\ndf_mental_physical Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 186 entries, 0 to 185\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  186 non-null    int64 \n 1   Symptoms    186 non-null    object\n 2   Label       186 non-null    object\ndtypes: int64(1), object(2)\nmemory usage: 4.5+ KB\ndf_mental_physical Dataset -Shape:\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nSymptoms\nLabel\n\n\n\n\n0\n0\nI often experience shortness of breath.\nPhysical health\n\n\n1\n1\nI have been feeling persistent fatigue lately.\nPhysical health\n\n\n2\n2\nI am experiencing unexplained weight loss.\nPhysical health\n\n\n3\n3\nI have a consistent cough that won't go away.\nPhysical health\n\n\n4\n4\nI am dealing with frequent headaches.\nPhysical health\n\n\n\n\n\n\n\nNone\n\n\n(186, 3)"
  },
  {
    "objectID": "Exploration.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "href": "Exploration.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "title": "Exploration",
    "section": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation",
    "text": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation\nCalculate and report basic summary statistics for the datasets.\nThis will include mean, median, mode, standard deviation, and variance for numerical variables.\nFor categorical variables, I will use frequency distributions.​\n\n\nCode\ndescription_stats={\n    \"age_first_depression_or_anxiety\":df_age_first_depression.describe(include='all'),\n    \"eating_disorder\":df_eating_disorder.describe(include='all'),\n    \"mental_health\":df_mental_health.describe(include='all'),\n    'symptoms_mental_vs_physical':df_mental_physical.describe(include='all'),\n}\ndescription_stats\n\n\n{'age_first_depression_or_anxiety':         Entity       Age  Percentage\n count       42        42   42.000000\n unique       7         6         NaN\n top     Africa  Ages &lt;13         NaN\n freq         6         7         NaN\n mean       NaN       NaN   16.666667\n std        NaN       NaN   10.060695\n min        NaN       NaN    1.271836\n 25%        NaN       NaN    8.871260\n 50%        NaN       NaN   14.426575\n 75%        NaN       NaN   22.251861\n max        NaN       NaN   42.724724,\n 'eating_disorder':             Economy  Code         Year  Eating_disorders_Male  \\\n count          6420  6150  6420.000000            6420.000000   \n unique          214   205          NaN                    NaN   \n top     Afghanistan   AFG          NaN                    NaN   \n freq             30    30          NaN                    NaN   \n mean            NaN   NaN  2004.500000               0.119775   \n std             NaN   NaN     8.656116               0.068943   \n min             NaN   NaN  1990.000000               0.033360   \n 25%             NaN   NaN  1997.000000               0.071057   \n 50%             NaN   NaN  2004.500000               0.098256   \n 75%             NaN   NaN  2012.000000               0.148705   \n max             NaN   NaN  2019.000000               0.672270   \n \n         Eating_disorders_Female   All_gender  \n count               6420.000000  6420.000000  \n unique                      NaN          NaN  \n top                         NaN          NaN  \n freq                        NaN          NaN  \n mean                   0.273787     0.196781  \n std                    0.214920     0.140066  \n min                    0.056762     0.045083  \n 25%                    0.121105     0.096171  \n 50%                    0.187430     0.144395  \n 75%                    0.352395     0.252282  \n max                    1.395754     1.034012  ,\n 'mental_health':             Economy  Code         Year  Schizophrenia (%)  \\\n count          5488  5488  5488.000000        5488.000000   \n unique          196   196          NaN                NaN   \n top     Afghanistan   AFG          NaN                NaN   \n freq             28    28          NaN                NaN   \n mean            NaN   NaN  2003.500000           0.208183   \n std             NaN   NaN     8.078483           0.041998   \n min             NaN   NaN  1990.000000           0.146902   \n 25%             NaN   NaN  1996.750000           0.179452   \n 50%             NaN   NaN  2003.500000           0.198509   \n 75%             NaN   NaN  2010.250000           0.230554   \n max             NaN   NaN  2017.000000           0.375110   \n \n         Bipolar disorder (%)  Eating disorders (%)  Anxiety disorders (%)  \\\n count            5488.000000           5488.000000            5488.000000   \n unique                   NaN                   NaN                    NaN   \n top                      NaN                   NaN                    NaN   \n freq                     NaN                   NaN                    NaN   \n mean                0.716884              0.234023               3.946979   \n std                 0.164246              0.154147               1.134810   \n min                 0.314535              0.073908               2.023393   \n 25%                 0.615732              0.121760               3.178912   \n 50%                 0.693954              0.180378               3.515140   \n 75%                 0.830217              0.278681               4.659540   \n max                 1.206597              0.943991               8.967330   \n \n         Depression (%) Income group  average_learning_Adjusted_of_school  \\\n count      5488.000000         5432                          4676.000000   \n unique             NaN            4                                  NaN   \n top                NaN  High income                                  NaN   \n freq               NaN         1764                                  NaN   \n mean          3.474504          NaN                             7.760170   \n std           0.671741          NaN                             2.528810   \n min           2.139903          NaN                             2.251002   \n 25%           2.955355          NaN                             5.684793   \n 50%           3.461421          NaN                             7.855914   \n 75%           3.877343          NaN                            10.048229   \n max           6.602754          NaN                            12.775495   \n \n        Continent     GDP(2022)  \\\n count       4900  5.264000e+03   \n unique         6           NaN   \n top       Africa           NaN   \n freq        1428           NaN   \n mean         NaN  3.929245e+05   \n std          NaN  1.451071e+06   \n min          NaN  2.230000e+02   \n 25%          NaN  1.240875e+04   \n 50%          NaN  4.239550e+04   \n 75%          NaN  2.550042e+05   \n max          NaN  1.796317e+07   \n \n         not_at_all_comfortable_speaking_anxiety_or_depression_percent  \\\n count                                         3108.000000               \n unique                                                NaN               \n top                                                   NaN               \n freq                                                  NaN               \n mean                                            30.009805               \n std                                             11.940808               \n min                                              5.833535               \n 25%                                             21.170624               \n 50%                                             30.920343               \n 75%                                             37.729200               \n max                                             58.778120               \n \n         GDP_per_capita  \n count      4468.000000  \n unique             NaN  \n top                NaN  \n freq               NaN  \n mean       9957.671571  \n std       15590.325250  \n min          31.321000  \n 25%         982.891500  \n 50%        3309.844000  \n 75%       11159.947250  \n max      125200.137000  ,\n 'symptoms_mental_vs_physical':         Unnamed: 0                                 Symptoms            Label\n count   186.000000                                      186              186\n unique         NaN                                      186                2\n top            NaN  I often experience shortness of breath.  Physical health\n freq           NaN                                        1              100\n mean     92.500000                                      NaN              NaN\n std      53.837719                                      NaN              NaN\n min       0.000000                                      NaN              NaN\n 25%      46.250000                                      NaN              NaN\n 50%      92.500000                                      NaN              NaN\n 75%     138.750000                                      NaN              NaN\n max     185.000000                                      NaN              NaN}\n\n\n\n1.Mental Health Dataset\n\nHistograms Plot\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the aesthetics for the plots\nsns.set(style=\"whitegrid\")\n\n# Define a function to create histograms for given columns\ndef plot_histograms(data, columns, bins=20, figsize=(15, 5)):\n    fig, axes = plt.subplots(1, len(columns), figsize=figsize)\n    for ax, col in zip(axes, columns):\n        sns.histplot(data[col].dropna(), bins=bins, ax=ax, kde=True)  # Drop NaN for plotting\n        ax.set_title(f'Distribution of {col}')\n    plt.tight_layout()\n    return fig\n\n# Select columns to plot for Mental Health Data (excluding 'Year' and non-numerical columns)\nmental_health_columns = ['Schizophrenia (%)', 'Bipolar disorder (%)', \n                         'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)']\n\n# Plot histograms for the selected columns\nhistograms_mental_health = plot_histograms(df_mental_health, mental_health_columns)\n\n\n\n\n\n\n\nTime Series Plot\nTo observe the trend of a specific disorder over time aggregated globally.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Aggregate the data by Year and calculate the mean for each disorder\nannual_prevalence = df_mental_health.groupby('Year').mean().reset_index()\nannual_prevalence.head()\n# Determine the number of disorders to plot\nnum_disorders = len(annual_prevalence.columns[1:6])\n\n# Creating subplots\nfig, axes = plt.subplots(num_disorders, 1, figsize=(14, 6 * num_disorders))\n\n# Plotting each disorder in a separate subplot\nfor i, column in enumerate(annual_prevalence.columns[1:6]):\n    axes[i].plot(annual_prevalence['Year'], annual_prevalence[column], label=column)\n    axes[i].set_title(column)\n    axes[i].set_xlabel('Year')\n    axes[i].set_ylabel('Prevalence (%)')\n    axes[i].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/527416444.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  annual_prevalence = df_mental_health.groupby('Year').mean().reset_index()\n\n\n\n\n\nAnxiety disorders and depression show higher prevalence rates compared to other disorders like schizophrenia, bipolar disorder, and eating disorders.This could indicate that anxiety and depression are more common mental health concerns in the population.\nThe prevalence rates for disorders like schizophrenia, bipolar disorder, eating disorders and anxiety disorders keep increaseing over the years, while the depression retes does not have a liner increasement.it has a peak in 1997 and then decrease to a lower level.\n\n\nThe Bar plot\nTo compare the prevalence of different disorders across different income groups.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_income_group=df_mental_health.groupby('Income group').mean()\n# print(df_income_group)\ndf_income_group=df_income_group.drop(columns=['Year','GDP(2022)','not_at_all_comfortable_speaking_anxiety_or_depression_percent','average_learning_Adjusted_of_school','GDP_per_capita'])\n# print(type(df_income_group))\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 6))\ndf_income_group.plot(kind='bar', ax=ax)\nax.set_title('Mean Percentage of Mental Health Disorders by Income Group')\nax.set_ylabel('Mean Percentage')\nax.set_xlabel('Income Group')\nplt.xticks(rotation=45)\nplt.legend(title='Mental Health Disorders')\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/3638934320.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_income_group=df_mental_health.groupby('Income group').mean()\n\n\n\n\n\n\n\nThe Box Plot\nBox Plot: Distribution of mental health disorders prevalence rates across different income groups in 2017.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Assuming df_mental_health is your DataFrame\n# Replace 'df_mental_health' with the actual name of your DataFrame if different\n\nmost_recent_year = df_mental_health['Year'].max()\n\n# Disorders to plot\ndisorders = ['Bipolar disorder (%)', 'Schizophrenia (%)', 'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)']\n\n# Creating box plots for each disorder\nfor disorder in disorders:\n    # Filter the data for the disorder for the most recent year\n    disorder_data_recent_year = df_mental_health[df_mental_health['Year'] == most_recent_year]\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(data=disorder_data_recent_year, x='Income group', y=disorder)\n    plt.title(f'Distribution of {disorder} Across Income Groups in {most_recent_year}')\n    plt.xlabel('Income Group')\n    plt.ylabel(f'{disorder} Prevalence (%)')\n    plt.grid(axis='y')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot provides insights into the median prevalence rates, the interquartile ranges, and any potential outliers within each income group category.\nThe Box Plot above shows the distribution of ‘Bipolar disorder (%)’, ‘Schizophrenia (%)’, ‘Eating disorders (%)’, ’Anxiety disorders (%)’prevalence vary by income group level, while the Depression doesn’t vary by income group level.\n\n\nScatter plot:\nRelationship between ‘average learning adjusted years of school’ and mental health disorders prevalence rates\n\n\nCode\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\neducation_col = 'average_learning_Adjusted_of_school'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\ncreate_scatter_plots(df_mental_health, education_col, mental_health_disorders, 2)\n\n\n\n\n\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone.\nIn general, the scatter plots show that there is no clear relationship between education and the prevalence of mental health disorders. This could indicate that other factors may be more influential in determining the prevalence of mental health disorders.\nRelationship between ‘GDP(2022)’ and mental health disorders prevalence rates\n\n\nCode\n# This function will adjust the number of subplots based on the number of disorders.\n\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\ngdp_col = 'GDP(2022)'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\n# Now let's use the function to create scatter plots\ncreate_scatter_plots(df_mental_health, gdp_col, mental_health_disorders, 2)\n\n\n\n\n\nThe scatter plots show that they does not appear to be a clear trend or correlation between GDP and the prevalence of these mental health problems.\nRelationship between ‘GDP per capita’ and mental health disorders prevalence rates.\n\n\nCode\n# This function will adjust the number of subplots based on the number of disorders.\n\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\ngdp_col = 'GDP_per_capita'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\n#use the function to create scatter plots\ncreate_scatter_plots(df_mental_health, gdp_col, mental_health_disorders, 2)\n\n\n\n\n\nAs GDP per capita increases, the prevalence rates of eating disorder seems to spread out, while others don’t show a clear trend indicating a strong relationship with gdp per capita.\n\n\nCorrelation Heatmap\n‘Income group’ and ‘Continent’ to the list of columns for the correlation matrix, which are categorical variables. The correlation matrix typically requires numerical variables. To include these categorical variables, I convert them into a numerical format using techniques such as one-hot encoding.\n\n\nCode\nimport numpy as np\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    'GDP_per_capita',\n    'not_at_all_comfortable_speaking_anxiety_or_depression_percent',\n    # 'Income group',\n    # 'Continent',\n\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, annot=True, mask=mask, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5},cbar=True)\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\n\n\n\n\n\nSocioeconomic Factors: These include GDP (2022), GDP per capita, and income groups (high, low, lower middle, upper middle). High-income groups have strong positive correlations with mental health outcomes, particularly with eating disorders (0.74), and Schizophrenia(0.58). This suggests that in high-income groups, there may be a higher reported prevalence or better diagnosis of these conditions. On the other hand, low-income groups show a moderate negative correlation with some mental health outcomes, indicating a lower reported prevalence or underdiagnosis in these groups.\nEducational Factors: The variable ‘average_learning_Adjusted_of_school’ shows strong positive correlations with eating disorders (0.69), and Schizophrenia(0.68). This suggests that higher education levels are associated with a higher reported prevalence of these conditions, which could be due to increased awareness and reporting in more educated populations.\nGeographic Factors: The geographic regions show varied correlations with mental health outcomes. For example, being in Africa has a strong negative correlation with schizophrenia (-0.53) and a moderate negative correlation with eating disorder (-0.39). In contrast, Europe shows a moderate positive correlation with eating disorders (0.43). These correlations suggest that geographic location can have a significant impact on the reporting and prevalence of mental health conditions, possibly due to cultural, environmental, or healthcare-related factors.\nIn summary, educational factors, indicated by ‘average_learning_Adjusted_of_school’, show the most consistent strong positive correlations across several mental health outcomes, suggesting that education level may be a strong predictor of mental health outcomes, potentially due to greater awareness and reporting. Socioeconomic factors, particularly income level, also show strong correlations, especially in high-income groups. Geographic factors show both strong negative and moderate positive correlations, indicating a more complex relationship with mental health outcomes that could be influenced by a variety of regional factors.\n\n\n\n2.Eating disorder Dataset\n\nTime Series Plot:\nShowing the trend of eating disorder prevalence over years averaged globally between male and female.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the average prevalence of eating disorders for each year across all countries\naverage_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n# Time Series Line Plot for the global trend\nplt.figure(figsize=(14, 7))\n\n# Plotting the trends for each gender and all genders combined\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Male', label='Male')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Female', label='Female')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='All_gender', label='All Genders')\n\nplt.title('Global Average Prevalence of Eating Disorders Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Prevalence')\nplt.legend()\nplt.grid(True)\n\n# Show the line plot\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/758625183.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  average_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n\n\n\n\nThe line plot above shows the global average prevalence of eating disorders over time for males, females, and all genders combined. The trends can be compared to see how the prevalence has changed over the years. Eating disorders were more than twice as prevalent among females (3.8%) than males (1.5%)\n\n\nGrouped Bar Chart:\nCompare the prevalence of eating disorders between males and females across different countries. For clarity,I will take a subset of countries to compare.\n\n\nCode\n# Randomly select 10 countries for comparison\nrandom_countries = df_eating_disorder['Economy'].drop_duplicates().sample(10, random_state=1)\n\n# Filter the data for the selected countries and the latest year available\nlatest_year = df_eating_disorder['Year'].max()\ncomparison_data = df_eating_disorder[(df_eating_disorder['Economy'].isin(random_countries)) & (df_eating_disorder['Year'] == latest_year)]\n\n# Prepare data for grouped bar chart\ncomparison_data['Economy'] = comparison_data['Economy'].astype('category')\ncomparison_data['Economy'].cat.set_categories(random_countries, inplace=True)\ncomparison_data.sort_values('Economy', inplace=True)\n\n# Determine the positions of the bars\npos = np.arange(len(random_countries))\nbar_width = 0.4\n\n# Create the bar chart\nplt.figure(figsize=(14, 7))\n\n# Plot for males\nplt.bar(pos - bar_width/2, comparison_data['Eating_disorders_Male'], bar_width, color='lightblue', label='Male')\n\n# Plot for females\nplt.bar(pos + bar_width/2, comparison_data['Eating_disorders_Female'], bar_width, color='lightgreen', label='Female')\n\nplt.title(f'Comparison of Eating Disorders Prevalence by Gender in {latest_year}')\nplt.xlabel('Economy')\nplt.ylabel('Prevalence of Eating Disorders')\nplt.xticks(pos, random_countries, rotation=45, ha='right')\nplt.legend()\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/2505907414.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  comparison_data['Economy'] = comparison_data['Economy'].astype('category')\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/2505907414.py:10: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  comparison_data['Economy'].cat.set_categories(random_countries, inplace=True)\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/2505907414.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  comparison_data.sort_values('Economy', inplace=True)\n\n\n\n\n\nThe bar chart above compares the prevalence of eating disorders between males (in light blue) and females (in pink) for a randomly selected subset of 10 countries in the latest year available in the dataset.\n\n\nBox plot\n\nIdentifying Outliers\nlooking for countries with prevalence rates that are significantly higher or lower than the global average for year 2019 for example\n\n\nCode\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nMale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.11%. The range of prevalence rates is quite broad, with the lowest around 0.03% and the highest at about 0.67%. The interquartile range (middle 50% of the data) spans from approximately 0.08% to 0.16%, indicating that half of the reported rates fall within this range.\nFemale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.21%, which is notably higher than that of males. The prevalence rates for females also show a broad range, from about 0.06% to 1.40%. The interquartile range for females is wider than for males, ranging from about 0.14% to 0.40%, reflecting greater variability in the rates reported for females.\nOutlier Identification:\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\n\n\nRemove the outliers\n\n\nCode\n# Calculate the interquartile range for males\nQ1_male = df_eating_disorder['Eating_disorders_Male'].quantile(0.25)\nQ3_male = df_eating_disorder['Eating_disorders_Male'].quantile(0.75)\nIQR_male = Q3_male - Q1_male\nlower_bound_male = Q1_male - 1.5 * IQR_male\nupper_bound_male = Q3_male + 1.5 * IQR_male\n\n# Remove outliers for males\nlatest_data_no_outliers_male = df_eating_disorder[(df_eating_disorder['Eating_disorders_Male'] &gt;= lower_bound_male) & \n                                           (df_eating_disorder['Eating_disorders_Male'] &lt;= upper_bound_male)]\n\n# Calculate the interquartile range for females\nQ1_female = df_eating_disorder['Eating_disorders_Female'].quantile(0.25)\nQ3_female = df_eating_disorder['Eating_disorders_Female'].quantile(0.75)\nIQR_female = Q3_female - Q1_female\nlower_bound_female = Q1_female - 1.5 * IQR_female\nupper_bound_female = Q3_female + 1.5 * IQR_female\n\n# Remove outliers for females\nlatest_data_no_outliers_female = latest_data[(df_eating_disorder['Eating_disorders_Female'] &gt;= lower_bound_female) & \n                                             (df_eating_disorder['Eating_disorders_Female'] &lt;= upper_bound_female)]\n\n# Now let's plot the data without outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence without outliers\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data_no_outliers_male['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + ' (No Outliers)')\n\n# Box plot for female prevalence without outliers\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data_no_outliers_female['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + ' (No Outliers)')\n\nplt.tight_layout()\nplt.show()\n#save the data sets\n# latest_data.to_csv('./Data/eating_disorder_male_female_rm_outliers.csv',index=False)\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/2046586742.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  latest_data_no_outliers_female = latest_data[(df_eating_disorder['Eating_disorders_Female'] &gt;= lower_bound_female) &\n\n\n\n\n\n\n\n\nHeatmap\nVisualizing the prevalence of eating disorders in US and years in a color-coded format.\n\n\nCode\n# For the heatmap , select all years of data for US only.\nus_data = df_eating_disorder[df_eating_disorder['Economy'] == 'United States'].pivot('Year', 'Code', 'All_gender')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(us_data, cmap=\"YlOrRd\", linewidths=.5, annot=True, fmt=\".2f\")\n\nplt.title('Heatmap of Eating Disorders Prevalence Across Years in US')\nplt.xlabel('Country Code')\nplt.ylabel('Year')\n# Show the heatmap\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/671262145.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  us_data = df_eating_disorder[df_eating_disorder['Economy'] == 'United States'].pivot('Year', 'Code', 'All_gender')\n\n\n\n\n\nThe heatmap above visualizes the prevalence of eating disorders across different years in US.\nIn years 1993 and 2012 show the highest prevalence rates. From 2013 onwards, there appears to be a slight decrease in prevalence, with rates consistently around 0.42% to 0.43%.\n\n\n\n3.Age when first have depression or anxiety Data\n\nPie chart\nShowing the distribution of age categories for the onset of anxiety or depression for the “World” entity, I’ll first filter the dataset for the ‘World’ entity, and then plot the data.\n\n\nCode\n# Filter the data for the 'World' entity\nworld_data = df_age_first_depression[df_age_first_depression['Entity'] == 'World']\nplt.figure(figsize=(8, 8))\nplt.pie(world_data['Percentage'], labels=world_data['Age'], autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Age Categories for First Anxiety or Depression in the World')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n# Show the pie chart\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe largest segment of the chart, making up 34.9%, is unspecified and labeled “Don’t know/Refused,” indicating a significant portion of respondents either did not know or refused to provide their age at first experience of anxiety or depression.\nThe second-largest group is individuals aged 20–29, representing 22.2% of the distribution. This suggests that early adulthood is a common time for individuals to experience their first instance of these mental health issues.\nThose aged 30–39 make up 12.5% of the distribution, while individuals aged 40 and over account for 11.4%.\nAdolescents aged 13–19 represent 11.5% of the distribution, indicating that the onset of anxiety or depression can also begin during teenage years.\nThe smallest identified age group is children under the age of 13, at 7.5%, which highlights that while less common, young children can also experience these mental health issues.\nOverall, the data suggests that the onset of anxiety or depression is reported most frequently during the ages of 20–29 and that there’s a notable amount of uncertainty or unwillingness to provide information regarding the age of first experience with these conditions.\n\n\nHeatmap\na.Visualize the relationship between entities and age groups\n\n\nCode\n# Create a pivot table for the heatmap\nheatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n# Heatmap: Visualize the percentage of individuals by entity and age category\nplt.figure(figsize=(12, 10))\nheatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Heatmap of Percentage by Entity and Age Category for First Anxiety or Depression')\nplt.xlabel('Age Category')\nplt.ylabel('Entity')\n\n# Show the heatmap\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_19940/3650547391.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  heatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n\n\n\n\n\n\nStacked bar chart\nGrouping the data by Entity and visualizing the distribution of the Percentage across different Age brackets.\n\n\nCode\n# Stacked Bar Chart: Create a crosstab to prepare data for the stacked plot\nstacked_data = pd.crosstab(index=df_age_first_depression['Entity'], columns=df_age_first_depression['Age'], values=df_age_first_depression['Percentage'], aggfunc='sum', normalize='index')\n\n# Plotting the stacked bar chart\nstacked_data.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='viridis')\nplt.title('Stacked Bar Chart of Age Categories for First Anxiety or Depression by Entity')\nplt.xlabel('Entity')\nplt.ylabel('Percentage')\nplt.legend(title='Age Category', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xticks(rotation=45, ha='right')\n\n# Show the stacked bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nHigh Uncertainty or Non-disclosure: ‘Don’t know/Refused’ responses are notably high across all entities, with Africa recording the highest at 42.7% ,lower-middle income countries at 40.6% and Europe the lowest at 14.5%. This indicates a significant amount of data is either not known or not disclosed.\nHigh-Income Countries: except for ‘Don’t know/Refused’, The highest ratios are observed in the ’Ages 13_19’and lowest in the Age &lt; 13\nUpper-middle income countries: except for ‘Don’t know/Refused’, The ratios are more balanced across the age categories, with the lowest ratio STILL in age &lt;13.\nlower-middle income countries: except for ‘Don’t know/Refused’,The highest ratios are observed in the ‘Ages 20_29’ and it is almost 3 times higher than other age categories.\nEurope: The ratios are more balanced across the age categories, with the lowest ratio in age &lt;13.\nAfrica and Asia: The ratios are Highest in the ‘Ages 20_29’ and lowest in the Age &lt; 13, but the ratio in Asia is higher than Africa."
  },
  {
    "objectID": "Exploration.html#hypothesis-generation",
    "href": "Exploration.html#hypothesis-generation",
    "title": "Exploration",
    "section": "Hypothesis Generation",
    "text": "Hypothesis Generation\n\nAge First Depression or Anxiety Data\n\nThe cultural, economic, and social environments in high-income countries might contribute to an earlier age of onset (The highest ratios are observed in the ’Ages 13_19) for anxiety or depression.\nIn regions with lower-middle-income levels, the onset of anxiety or depression might occur later, potentially due to different life stressors or lower levels of mental health awareness and reporting.\n\n\n\nEating disorder Dataset\n\nThe prevalence of eating disorders is higher in countries with higher GDP per capita, which could reflect better diagnostic capabilities or different societal pressures.\nThere may be significant differences in the prevalence of eating disorders between genders across different countries, potentially reflecting cultural, social, or biological factors.\n\n\n\nMental Health Dataset\n\nEducational outcomes may be associated with the prevalence or reporting of certain mental health conditions, such as schizophrenia and eating disorders.\nThe correlations between different mental health conditions suggest the possibility of common underlying risk factors or increased comorbidity."
  },
  {
    "objectID": "Exploration.html#report-and-discuss-methods-and-findings",
    "href": "Exploration.html#report-and-discuss-methods-and-findings",
    "title": "Exploration",
    "section": "Report and Discuss methods and findings",
    "text": "Report and Discuss methods and findings\n\nAge First Depression or Anxiety Data\n\nThe largest segment of the data, making up 34.9%, is unspecified and labeled “Don’t know/Refused,” indicating a significant portion of respondents either did not know or refused to provide their age at first experience of anxiety or depression.(High Uncertainty or Non-disclosure: ‘Don’t know/Refused’ responses are notably high across all entities, with Africa recording the highest at 42.7% ,lower-middle income countries at 40.6% and Europe the lowest at 14.5%. This indicates a significant amount of data is either not known or not disclosed.) The second-largest group is individuals aged 20–29, representing 22.2% of the distribution. This suggests that early adulthood is a common time for individuals to experience their first instance of these mental health issues.Those aged 30–39 make up 12.5% of the distribution, while individuals aged 40 and over account for 11.4%.Adolescents aged 13–19 represent 11.5% of the distribution, indicating that the onset of anxiety or depression can also begin during teenage years.The smallest identified age group is children under the age of 13, at 7.5%, which highlights that while less common, young children can also experience these mental health issues. Overall, the data suggests that the onset of anxiety or depression is reported most frequently during the ages of 20–29 and that there’s a notable amount of uncertainty or unwillingness to provide information regarding the age of first experience with these conditions.\nHigh-Income Countries: except for ‘Don’t know/Refused’, The highest ratios are observed in the ‘Ages 13_19’and lowest in the Age &lt; 13; Upper-middle income countries: except for ’Don’t know/Refused’, The ratios are more balanced across the age categories, with the lowest ratio STILL in age &lt;13; lower-middle income countries: except for ‘Don’t know/Refused’,The highest ratios are observed in the ‘Ages 20_29’ and it is almost 3 times higher than other age categories; Europe: The ratios are more balanced across the age categories, with the lowest ratio in age &lt;13; Africa and Asia: The ratios are Highest in the ‘Ages 20_29’ and lowest in the Age &lt; 13, but the ratio in Asia is higher than Africa.\n\n\n\nEating disorder Dataset\n\nThe prevalence of eating disorders is significantly higher in females than in males, which aligns with the general understanding of these conditions.\nThe variability in prevalence rates among females is greater, indicating that eating disorders in females may be more influenced by a variety of factors, possibly including cultural aspects, societal pressures, or biological predispositions.\nThe global average prevalence of eating disorders over time for males, females, and all genders combined is growing up.\n\n\n\nMental Health Dataset\n\nEducational factors, indicated by ‘average_learning_Adjusted_of_school’, show the most consistent strong positive correlations across several mental health outcomes, suggesting that education level may be a strong predictor of mental health outcomes, potentially due to greater awareness and reporting.\nSocioeconomic factors, particularly income level, also show strong correlations, especially in high-income groups.\nGeographic factors show both strong negative and moderate positive correlations, indicating a more complex relationship with mental health outcomes that could be influenced by a variety of regional factors."
  },
  {
    "objectID": "Exploration.html#tools-and-software",
    "href": "Exploration.html#tools-and-software",
    "title": "Exploration",
    "section": "Tools and Software",
    "text": "Tools and Software\n\nPandas: For data manipulation and analysis.\nMatplotlib: For creating the bar chart visualization.\nSeaborn: Although not used in the last plot, it is an excellent tool for creating heatmaps and other complex visualizations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Mental illness is a general term for a group of illnesses that may include symptoms that can affect a person’s thinking, perceptions, mood or behaviour. Mental illness can make it difficult for someone to cope with work, relationships and other demands.There are 5 types of mental health issues will be discussed in this project\n\n\nDepression is a mood disorder characterised by lowering of mood, loss of interest and enjoyment, and reduced energy. It is not just feeling sad.Depression affects how people think, feel and act. Depression makes it more difficult to manage from day to day and interferes with study, work and relationships.\n\n\n\nAnxiety disorders are a group of mental health problems. They include generalised anxiety disorders, social phobias, specific phobias (for example, agoraphobia and claustrophobia), and panic disorders. Depression is often related to anxiety disorders.\nAnxiety disorders are common mental health problems that affect many people. Approximately 25% of the population have an anxiety disorder that warrants treatment at some time in their life and up to another 25% have less severe anxieties such as fears of spider and snakes.\n\n\n\nEating disorders include anorexia, bulimia nervosa and other binge eating disorders. Eating disorders can affect people of all ages and genders, and can have serious psychological and physical consequences.\n\n\n\nBipolar affective disorder is a type of mood disorder, previously referred to as ‘manic depression’. A person with bipolar disorder experiences episodes of mania (elation) and depression. The person may or may not experience psychotic symptoms.\n\n\n\nSchizophrenia is a complex psychotic disorder characterised by disruptions to thinking and emotions, and a distorted perception of reality. Symptoms of schizophrenia vary widely but may include hallucinations, delusions, thought disorder, social withdrawal, lack of motivation and impaired thinking and memory."
  },
  {
    "objectID": "index.html#depression",
    "href": "index.html#depression",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Depression is a mood disorder characterised by lowering of mood, loss of interest and enjoyment, and reduced energy. It is not just feeling sad.Depression affects how people think, feel and act. Depression makes it more difficult to manage from day to day and interferes with study, work and relationships."
  },
  {
    "objectID": "index.html#anxiety-disorder",
    "href": "index.html#anxiety-disorder",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Anxiety disorders are a group of mental health problems. They include generalised anxiety disorders, social phobias, specific phobias (for example, agoraphobia and claustrophobia), and panic disorders. Depression is often related to anxiety disorders.\nAnxiety disorders are common mental health problems that affect many people. Approximately 25% of the population have an anxiety disorder that warrants treatment at some time in their life and up to another 25% have less severe anxieties such as fears of spider and snakes."
  },
  {
    "objectID": "index.html#eating-disorders",
    "href": "index.html#eating-disorders",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Eating disorders include anorexia, bulimia nervosa and other binge eating disorders. Eating disorders can affect people of all ages and genders, and can have serious psychological and physical consequences."
  },
  {
    "objectID": "index.html#bipolar",
    "href": "index.html#bipolar",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Bipolar affective disorder is a type of mood disorder, previously referred to as ‘manic depression’. A person with bipolar disorder experiences episodes of mania (elation) and depression. The person may or may not experience psychotic symptoms."
  },
  {
    "objectID": "index.html#schizophrenia",
    "href": "index.html#schizophrenia",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Schizophrenia is a complex psychotic disorder characterised by disruptions to thinking and emotions, and a distorted perception of reality. Symptoms of schizophrenia vary widely but may include hallucinations, delusions, thought disorder, social withdrawal, lack of motivation and impaired thinking and memory."
  },
  {
    "objectID": "ARM.html",
    "href": "ARM.html",
    "title": "ARM and Networking",
    "section": "",
    "text": "We are using the all_mental_health_symptoms_nb.csv data and Our goal is to find associations between different symptoms, like which symptoms often occur together. First, we will convert the continuous variables into three categories: ‘Low’, ‘Middle’, and ‘High’. The categorization is based on the lower and upper quartiles of the data. Values below the lower quartile are labeled ‘Low’, values between the lower and upper quartile are labeled ‘Middle’, and values above the upper quartile are labeled ‘High’. so this sectioning is well-suited for ARM as it transforms the dataset into a format where patterns can be more easily discovered.\n\n\nCode\nimport pandas as pd\n\n# Function to convert continuous variables into three categories ('High', 'Middle', 'Low')\ndef trinarize_data(df, columns):\n    \"\"\"\n    Trinarizes the specified columns of the dataframe based on percentiles.\n\n    Args:\n    df (DataFrame): The dataframe to trinarize.\n    columns (list of str): The columns to trinarize.\n\n    Returns:\n    DataFrame: The dataframe with the trinarized columns.\n    \"\"\"\n    trinarized_df = df.copy()\n    for col in columns:\n        # Define the lower and upper quartiles\n        lower_quartile = df[col].quantile(0.25)\n        upper_quartile = df[col].quantile(0.75)\n        \n        # Trinarize the column\n        trinarized_df[col] = pd.cut(df[col], bins=[-float('inf'), lower_quartile, upper_quartile, float('inf')],\n                                    labels=['Low', 'Middle', 'High'])\n    return trinarized_df\n\n# Load the dataset\nfile_path = './Data_cleaned/mental_health_DR.csv'\ndata = pd.read_csv(file_path)\n\n# List of columns to trinarize\ncolumns_to_trinarize = ['Schizophrenia (%)', 'Bipolar disorder (%)', \n                        'Eating disorders (%)', 'Anxiety disorders (%)', \n                        'Depression (%)', 'average_learning_Adjusted_of_school', 'GDP(2022)']\n\n# Apply trinarization\ntrinarized_data = trinarize_data(data, columns_to_trinarize)\n\n\n# Save the trinarized dataframe\n# trinarized_data.to_csv('../mental_health_DT_trinarized_ARM.csv', index=False)\n\n# Display the first few rows of the trinarized dataframe\ntrinarized_data.head()\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nGDP_per_capita\n\n\n\n\n0\nAfghanistan\nAFG\n1990\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n1\nAfghanistan\nAFG\n1991\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n2\nAfghanistan\nAFG\n1992\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n3\nAfghanistan\nAFG\n1993\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n4\nAfghanistan\nAFG\n1994\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844"
  },
  {
    "objectID": "ARM.html#prepare-the-data",
    "href": "ARM.html#prepare-the-data",
    "title": "ARM and Networking",
    "section": "",
    "text": "We are using the all_mental_health_symptoms_nb.csv data and Our goal is to find associations between different symptoms, like which symptoms often occur together. First, we will convert the continuous variables into three categories: ‘Low’, ‘Middle’, and ‘High’. The categorization is based on the lower and upper quartiles of the data. Values below the lower quartile are labeled ‘Low’, values between the lower and upper quartile are labeled ‘Middle’, and values above the upper quartile are labeled ‘High’. so this sectioning is well-suited for ARM as it transforms the dataset into a format where patterns can be more easily discovered.\n\n\nCode\nimport pandas as pd\n\n# Function to convert continuous variables into three categories ('High', 'Middle', 'Low')\ndef trinarize_data(df, columns):\n    \"\"\"\n    Trinarizes the specified columns of the dataframe based on percentiles.\n\n    Args:\n    df (DataFrame): The dataframe to trinarize.\n    columns (list of str): The columns to trinarize.\n\n    Returns:\n    DataFrame: The dataframe with the trinarized columns.\n    \"\"\"\n    trinarized_df = df.copy()\n    for col in columns:\n        # Define the lower and upper quartiles\n        lower_quartile = df[col].quantile(0.25)\n        upper_quartile = df[col].quantile(0.75)\n        \n        # Trinarize the column\n        trinarized_df[col] = pd.cut(df[col], bins=[-float('inf'), lower_quartile, upper_quartile, float('inf')],\n                                    labels=['Low', 'Middle', 'High'])\n    return trinarized_df\n\n# Load the dataset\nfile_path = './Data_cleaned/mental_health_DR.csv'\ndata = pd.read_csv(file_path)\n\n# List of columns to trinarize\ncolumns_to_trinarize = ['Schizophrenia (%)', 'Bipolar disorder (%)', \n                        'Eating disorders (%)', 'Anxiety disorders (%)', \n                        'Depression (%)', 'average_learning_Adjusted_of_school', 'GDP(2022)']\n\n# Apply trinarization\ntrinarized_data = trinarize_data(data, columns_to_trinarize)\n\n\n# Save the trinarized dataframe\n# trinarized_data.to_csv('../mental_health_DT_trinarized_ARM.csv', index=False)\n\n# Display the first few rows of the trinarized dataframe\ntrinarized_data.head()\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nGDP_per_capita\n\n\n\n\n0\nAfghanistan\nAFG\n1990\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n1\nAfghanistan\nAFG\n1991\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n2\nAfghanistan\nAFG\n1992\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n3\nAfghanistan\nAFG\n1993\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844\n\n\n4\nAfghanistan\nAFG\n1994\nLow\nMiddle\nLow\nHigh\nHigh\nLow income\nLow\nAsia\nMiddle\n3309.844"
  },
  {
    "objectID": "ARM.html#introduction",
    "href": "ARM.html#introduction",
    "title": "ARM and Networking",
    "section": "Introduction",
    "text": "Introduction\nOur dataset encapsulates a range of mental health indicators across various economies, categorized into ‘Low’, ‘Middle’, and ‘High’ based on their reported percentages. These indicators provide a holistic view of mental health status in correlation with economic factors such as GDP and income groups. The ARM analysis on this dataset is designed to unearth associations between different mental health conditions and these economic indicators. By applying ARM, we aim to reveal potential patterns that could inform healthcare policy, indicating, for example, if higher GDP correlates with lower rates of certain mental health conditions, or if specific disorders are more prevalent in certain economic classes."
  },
  {
    "objectID": "ARM.html#theory",
    "href": "ARM.html#theory",
    "title": "ARM and Networking",
    "section": "Theory",
    "text": "Theory\nAssociation Rule Mining (ARM)is akin to finding a needle in a haystack, except instead of a needle, we’re searching for golden threads that connect different pieces of hay. In data terms, these threads are the rules that link various items within our dataset. ARM operates by looking for these rules within large volumes of data, where the presence of one set of items may influence the presence of another. It’s how, for instance, a supermarket might notice that people who buy pasta also often buy tomato sauce. The ‘support’ tells us how often these item combinations appear, the ‘confidence’ gives the likelihood that the items are purchased together, and the ‘lift’ compares the observed frequency of the combination with the frequency we’d expect if the items were independent. It’s through these metrics that ARM provides a glimpse into the hidden patterns of our behaviors, preferences, and even health."
  },
  {
    "objectID": "ARM.html#method",
    "href": "ARM.html#method",
    "title": "ARM and Networking",
    "section": "Method",
    "text": "Method\nOur methodology involved first preparing the data by categorizing continuous variables into ‘Low’, ‘Middle’, and ‘High’ categories. This trinarization was carried out using Python’s Pandas library, which enabled us to manipulate the data effectively. Following this, we utilized the Apriori algorithm to identify frequent itemsets within the data. These itemsets were then used to generate association rules, which were subsequently evaluated based on their support, confidence, and lift.\n\n\nCode\nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\n\n## Since ARM requires a list of lists where each inner list is a transaction, we will convert the DataFrame\n# First, we'll create a new DataFrame where each row represents a transaction and each cell represents a categorical item\n\n# Creating a list of lists from the DataFrame transactions\ntransactions = trinarized_data.apply(lambda x: [col+'_'+str(x[col]) for col in trinarized_data.columns], axis=1).tolist()\n\n# Instantiating the TransactionEncoder\nte = TransactionEncoder()\nte_ary = te.fit(transactions).transform(transactions)\ndf = pd.DataFrame(te_ary, columns=te.columns_)\n\n# Applying Apriori algorithm to find frequent itemsets with a minimum support of 0.01\nfrequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n\n# Generating association rules with a minimum confidence of 0.6\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n\n# Displaying the first few association rules\nrules.head()\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nzhangs_metric\n\n\n\n\n0\n(Eating disorders (%)_High)\n(Anxiety disorders (%)_High)\n0.25\n0.250000\n0.159893\n0.639573\n2.558292\n0.097393\n2.080866\n0.812152\n\n\n1\n(Anxiety disorders (%)_High)\n(Eating disorders (%)_High)\n0.25\n0.250000\n0.159893\n0.639573\n2.558292\n0.097393\n2.080866\n0.812152\n\n\n2\n(Anxiety disorders (%)_High)\n(Income group_High income)\n0.25\n0.298851\n0.163793\n0.655172\n2.192308\n0.089080\n2.033333\n0.725146\n\n\n3\n(Anxiety disorders (%)_Low)\n(Bipolar disorder (%)_Middle)\n0.25\n0.500000\n0.215107\n0.860427\n1.720854\n0.090107\n3.582353\n0.558524\n\n\n4\n(Anxiety disorders (%)_Low)\n(GDP(2022)_Middle)\n0.25\n0.500000\n0.155583\n0.622332\n1.244663\n0.030583\n1.323913\n0.262093"
  },
  {
    "objectID": "ARM.html#results",
    "href": "ARM.html#results",
    "title": "ARM and Networking",
    "section": "Results",
    "text": "Results\nResults: The Association Rule Mining (ARM) analysis of our dataset revealed intriguing associations between various mental health conditions and socio-economic factors. These insights go beyond simple correlations, illustrating complex relationships within the data.\nA key finding is the strong association between high levels of anxiety disorders and eating disorders. This suggests that in environments where anxiety disorders are prevalent, there’s a significant likelihood of concurrent high levels of eating disorders. This pattern, indicated by a lift value greater than 2, provides a clear indication of interlinked mental health challenges.\nAnother interesting rule shows the connection between anxiety disorders and income groups. Notably, high levels of anxiety disorders are more commonly associated with high-income groups. This counters a common misconception that higher income always correlates with better mental health outcomes.\nWe also observed a pronounced relationship between low levels of anxiety disorders and a middle range of bipolar disorders. This could suggest a nuanced interaction between these two conditions that warrants further investigation.\nIn terms of socio-economic indicators, the association between low levels of anxiety disorders and middle-level GDP for 2022 was moderate, suggesting other factors might play a more significant role in determining GDP levels.\n\n\nCode\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# pivot_table = rules.pivot(\"antecedents\", \"consequents\", \"lift\")\n# # Creating the heatmap\n# plt.figure(figsize=(10, 8))\n# sns.heatmap(pivot_table, annot=True, cmap=\"YlGnBu\")\n# plt.title('Heatmap of Lift Values for Mental Health Conditions')\n# plt.xlabel('Consequents')\n# plt.ylabel('Antecedents')\n# plt.show()"
  },
  {
    "objectID": "ARM.html#conclusions",
    "href": "ARM.html#conclusions",
    "title": "ARM and Networking",
    "section": "Conclusions",
    "text": "Conclusions\nThe ARM analysis of our mental health dataset has unearthed patterns that have real-world implications. The strong association between anxiety and eating disorders, particularly in high-income groups, challenges the conventional understanding of mental health as purely a medical issue, suggesting a more complex interplay of psychological, environmental, and socio-economic factors.\nThese findings highlight the need for a holistic approach to mental health policy and intervention strategies. The recognition of these interdependencies can guide healthcare providers and policymakers in designing targeted interventions and preventive measures. For instance, mental health programs in high-income areas might benefit from integrating strategies that address both anxiety and eating disorders concurrently.\nMoreover, the connection between lower anxiety levels and moderate bipolar disorder prevalence suggests that mental health conditions manifest in various forms across different socio-economic spectrums. This underlines the importance of personalized and context-sensitive mental health care.\nIn essence, my analysis not only sheds light on the intricate relationship between mental health and socio-economic factors but also underscores the importance of a nuanced approach to mental health issues, considering the multifaceted nature of these challenges."
  },
  {
    "objectID": "ARM.html#pubilication",
    "href": "ARM.html#pubilication",
    "title": "ARM and Networking",
    "section": "Pubilication",
    "text": "Pubilication\n(Kumbhare and Chobe 2014)"
  },
  {
    "objectID": "Cleaning.html",
    "href": "Cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\n#https://pdas.samhsa.gov/saes/substate#\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\n#load data from csv file \nall_data_frames=glob.glob(\"./Data_uncleaned/*.csv\")+glob.glob(\"./Data_uncleaned/*.xlsx\")\nprint(all_data_frames)\nfor fname_index, fname in enumerate(all_data_frames):\n    print(fname_index, fname)\n    \ndf_GDP = pd.read_csv(\"./Data_uncleaned/GDP.csv\") \ndf_income_group = pd.read_excel(\"./Data_uncleaned/Income_group.xlsx\")\ndf_Countries_Continents = pd.read_csv(\"./Data_uncleaned/Countries_Continents.csv\")\ndf_education = pd.read_csv(\"./Data_uncleaned/education.csv\")\ndf_discomfort_speaking_anxiety_depression_2020 = pd.read_csv(\"./Data_uncleaned/discomfort_speaking_anxiety_depression_2020.csv\")\ndf_eating_disorder = pd.read_csv(\"./Data_uncleaned/eating_disorders_prevalence_males_vs_females.csv\")\ndf_first_anxiety_or_depression = pd.read_csv(\"./Data_uncleaned/age_when_first_had_anxiety_depression.csv\")\ndf_mental_health= pd.read_excel(\"./Data_uncleaned/mental_health.xlsx\")\ndf_gdp_per_captita=pd.read_csv('./Data_uncleaned/GDP_per_captita.csv')\ndf_all_mental_health_symtoms = pd.read_csv(\"./Data_uncleaned/all_mental_health_symptoms.csv\")\ndf_symtoms= pd.read_csv(\"./Data_uncleaned/symptoms.csv\")\n\n\n['./Data_uncleaned/education.csv', './Data_uncleaned/Countries_Continents.csv', './Data_uncleaned/symptoms.csv', './Data_uncleaned/age_when_first_had_anxiety_depression.csv', './Data_uncleaned/all_mental_health_symptoms.csv', './Data_uncleaned/GDP_per_captita.csv', './Data_uncleaned/discomfort_speaking_anxiety_depression_2020.csv', './Data_uncleaned/eating_disorders_prevalence_males_vs_females.csv', './Data_uncleaned/GDP.csv', './Data_uncleaned/Income_group.xlsx', './Data_uncleaned/mental_health.xlsx']\n0 ./Data_uncleaned/education.csv\n1 ./Data_uncleaned/Countries_Continents.csv\n2 ./Data_uncleaned/symptoms.csv\n3 ./Data_uncleaned/age_when_first_had_anxiety_depression.csv\n4 ./Data_uncleaned/all_mental_health_symptoms.csv\n5 ./Data_uncleaned/GDP_per_captita.csv\n6 ./Data_uncleaned/discomfort_speaking_anxiety_depression_2020.csv\n7 ./Data_uncleaned/eating_disorders_prevalence_males_vs_females.csv\n8 ./Data_uncleaned/GDP.csv\n9 ./Data_uncleaned/Income_group.xlsx\n10 ./Data_uncleaned/mental_health.xlsx\n\n\n\n\n\n\n\n\n\nCode\n#A.clean the data named eating_disorders\n# show all the columns of the dataframe\nprint('Before : ',df_eating_disorder.head())\n# print(df_eating_disorder.head())\n# drop the columns Continent and Population because they are meaningless to the analysis\ndf_eating_disorder = df_eating_disorder.drop(columns=['Continent','Population'])\n# print(df_eating_disorder.shape)\n# only keep the rows of the dataframe when the column 'Year' is between 1990-2021\n# eating_disorder_df = eating_disorder_df.loc[eating_disorder_df['Year'] == range(1990,2022)]\ndf_eating_disorder = df_eating_disorder[df_eating_disorder['Year'].isin(range(1990,2022))]\nrename_map={\n    'Entity':'Economy',\n    'Year':'Year',\n}\ndf_eating_disorder.rename(columns=rename_map, inplace=True)\n#drop the rows when the column named Eating_disorders_Male is null in the dataframe df_eating_disorder\ndf_eating_disorder= df_eating_disorder.dropna(subset=['Eating_disorders_Male'])\n\n#caculate the mean of the column Male and Female of the dataframe and save it to a new column named All_gender\ndf_eating_disorder['All_gender'] = df_eating_disorder[['Eating_disorders_Male', 'Eating_disorders_Female']].mean(axis=1)\nprint('After: ',df_eating_disorder.head())\n#save the dataframe to csv file\ndf_eating_disorder.to_csv(\"./Data_cleaned/eating_disorder_male_female.csv\", index=False)\n\n\nBefore :          Entity      Code  Year  Eating_disorders_Male  \\\n0     Abkhazia  OWID_ABK  2015                    NaN   \n1  Afghanistan       AFG  1990               0.088487   \n2  Afghanistan       AFG  1991               0.086048   \n3  Afghanistan       AFG  1992               0.083625   \n4  Afghanistan       AFG  1993               0.081628   \n\n   Eating_disorders_Female  Population Continent  \n0                      NaN         NaN      Asia  \n1                 0.161867  10694804.0       NaN  \n2                 0.156910  10745168.0       NaN  \n3                 0.152412  12057436.0       NaN  \n4                 0.147938  14003764.0       NaN  \n        Entity      Code  Year  Eating_disorders_Male  \\\n0     Abkhazia  OWID_ABK  2015                    NaN   \n1  Afghanistan       AFG  1990               0.088487   \n2  Afghanistan       AFG  1991               0.086048   \n3  Afghanistan       AFG  1992               0.083625   \n4  Afghanistan       AFG  1993               0.081628   \n\n   Eating_disorders_Female  Population Continent  \n0                      NaN         NaN      Asia  \n1                 0.161867  10694804.0       NaN  \n2                 0.156910  10745168.0       NaN  \n3                 0.152412  12057436.0       NaN  \n4                 0.147938  14003764.0       NaN  \nAfter:         Economy Code  Year  Eating_disorders_Male  Eating_disorders_Female  \\\n1  Afghanistan  AFG  1990               0.088487                 0.161867   \n2  Afghanistan  AFG  1991               0.086048                 0.156910   \n3  Afghanistan  AFG  1992               0.083625                 0.152412   \n4  Afghanistan  AFG  1993               0.081628                 0.147938   \n5  Afghanistan  AFG  1994               0.079439                 0.143980   \n\n   All_gender  \n1    0.125177  \n2    0.121479  \n3    0.118018  \n4    0.114783  \n5    0.111710  \n\n\n\n\n\n\n\nCode\n#B.clean the data named discomfort_speaking_anxiety_depression_2020\nprint('Before : ',df_discomfort_speaking_anxiety_depression_2020.head())\n#rename the column 'Entity' to 'Country' and 'Share - Question: mh5 - Someone local comfortable speaking about anxiety/depression with someone they know - Answer: Not at all comfortable - Gender: all - Age_group: all' to 'Not_Comfortable'\ndf_discomfort_speaking_anxiety_depression_2020.rename(columns={'Entity':'Economy'}, inplace=True)\nprint('After ', df_discomfort_speaking_anxiety_depression_2020.head())\ndf_discomfort_speaking_anxiety_depression_2020.to_csv(\"./Data_Cleaned/discomfort_speaking_anxiety_depression_2020.csv\", index=False)\n\n\nBefore :       Economy Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \nAfter       Economy Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \n\n\n\n\n\n\n\nCode\n#C.clean the data named GDP\n#add column names to the dataframe df_GDP\nprint('Before: ',df_GDP.head())\ndf_GDP.columns=['Code', 'Ranking','unnamed:2','Economy','GDP(2022)','unnamed:5']\ndf_GDP=df_GDP.drop(columns=['unnamed:2','unnamed:5'])\n#check the data type of the column GDP\n# print(df_GDP.dtypes)\n#convert the column GDP to numeric type\ndf_GDP['GDP(2022)']=df_GDP['GDP(2022)'].str.replace(',','')\n#convert the column GDP from object type to numeric type\ndf_GDP['GDP(2022)']=pd.to_numeric(df_GDP['GDP(2022)'],errors='coerce')\nprint('After: ',df_GDP.head())\ndf_GDP.to_csv(\"./Data_cleaned//GDP.csv\", index=False)\n\n\nBefore:     USA    1  Unnamed: 2   United States   25,462,700   Unnamed: 5\n0  CHN  2.0         NaN           China   17,963,171          NaN\n1  JPN  3.0         NaN           Japan    4,231,141          NaN\n2  DEU  4.0         NaN         Germany    4,072,192          NaN\n3  IND  5.0         NaN           India    3,385,090          NaN\n4  GBR  6.0         NaN  United Kingdom    3,070,668          NaN\nAfter:    Code  Ranking         Economy   GDP(2022)\n0  CHN      2.0           China  17963171.0\n1  JPN      3.0           Japan   4231141.0\n2  DEU      4.0         Germany   4072192.0\n3  IND      5.0           India   3385090.0\n4  GBR      6.0  United Kingdom   3070668.0\n\n\n\n\n\n\n\nCode\n#D.clean the data named Countries_Continents\n#rename the column Country to Economy\nprint('Before: ',df_Countries_Continents.head())\ndf_Countries_Continents.rename(columns={'Country':'Economy'},inplace=True)\nprint('After: ',df_Countries_Continents.head())\ndf_Countries_Continents.to_csv(\"./Data_cleaned/Countries_Continents.csv\", index=False)\n\n\nBefore:    Continent   Country\n0    Africa   Algeria\n1    Africa    Angola\n2    Africa     Benin\n3    Africa  Botswana\n4    Africa   Burkina\nAfter:    Continent   Economy\n0    Africa   Algeria\n1    Africa    Angola\n2    Africa     Benin\n3    Africa  Botswana\n4    Africa   Burkina\n\n\n\n\n\n\n\nCode\n#D.clean the data named education\nprint('Before: ',df_education.head())\n#create a new column named 'average_learning_Adjusted_of_school' to calculate the average of the columns 'Average years of schooling' of each Entity\ndf_education['average_learning_Adjusted_of_school'] = df_education.groupby('Entity')['Learning-Adjusted Years of School'].transform('mean')\n# This method keeps the first occurrence of each entity and drops the subsequent duplicates.\ndf_education = df_education.drop_duplicates(subset='Entity', keep='first')\n# rename the column entity to Economy\ndf_education.rename(columns={'Entity':'Economy'},inplace=True)\nprint('After: ',df_education.head())\ndf_education.to_csv(\"./data_cleaned/Education.csv\", index=False)\n\n\nBefore:          Entity Code  Year  Learning-Adjusted Years of School\n0  Afghanistan  AFG  2017                           4.870000\n1  Afghanistan  AFG  2018                           4.949788\n2  Afghanistan  AFG  2020                           5.052838\n3       Africa  NaN  2017                           4.931378\n4       Africa  NaN  2018                           5.123904\nAfter:          Economy Code  Year  Learning-Adjusted Years of School  \\\n0   Afghanistan  AFG  2017                           4.870000   \n3        Africa  NaN  2017                           4.931378   \n6       Albania  ALB  2010                           7.382478   \n10      Algeria  DZA  2010                           7.202200   \n14       Angola  AGO  2017                           4.110000   \n\n    average_learning_Adjusted_of_school  \n0                              4.957542  \n3                              5.058634  \n6                              8.526723  \n10                             7.045445  \n14                             4.193318  \n\n\n\n\n\n\n\nCode\n#E.clean the data named age_when_first_anxiety_depression.csv\nprint('Before: ',df_first_anxiety_or_depression.head())\ndf_first_anxiety_or_depression=df_first_anxiety_or_depression.drop(columns=['Code'])\n# print(df_first_anxiety_or_depression.columns)\n#melt the dataframe\ndf_first_anxiety_or_depression=pd.melt(df_first_anxiety_or_depression, id_vars=['Entity'], value_vars=['Ages &lt;13','Ages_13_19','Ages_20_29','Ages_30_39','Ages ≥40','Dont_know/Refused'], var_name='Age', value_name='Percentage')\nprint('After: ',df_first_anxiety_or_depression.head())\ndf_first_anxiety_or_depression.to_csv(\"./Data_cleaned/age_when_first_anxiety_or_depression.csv\", index=False)\n\n# # Sample dataframe\n# df = pd.DataFrame({\n#     'A': ['a1', 'a2', 'a3'],\n#     'B': [1, 2, 3],\n#     'C': [4, 5, 6],\n#     'D': [7, 8, 9]\n# })\n\n# print(\"Original DataFrame:\")\n# print(df)\n\n# # Use melt to transform the dataframe\n# melted_df = pd.melt(df, id_vars=['A'], value_vars=['B', 'C', 'D'], var_name='Variable', value_name='Value')\n\n# print(\"\\nMelted DataFrame:\")\n# print(melted_df)\n\n\nBefore:                            Entity Code  Year  Ages &lt;13  Ages_13_19  Ages_20_29  \\\n0                         Africa  NaN  2020  1.271836   14.229228   22.264650   \n1                           Asia  NaN  2020  7.795371    9.628078   23.403633   \n2                         Europe  NaN  2020  9.083381   22.210781   22.034777   \n3          High-income countries  NaN  2020  2.473921   24.484507   10.832585   \n4  Lower-middle-income countries  NaN  2020  8.800553    8.485406   26.288944   \n\n   Ages_30_39   Ages ≥40  Dont_know/Refused  \n0   16.803090   2.706476          42.724724  \n1   12.663021   9.983976          36.525925  \n2    7.628320  24.531280          14.511458  \n3   16.975899  16.115680          29.117413  \n4    8.428943   7.348721          40.647434  \nAfter:                            Entity       Age  Percentage\n0                         Africa  Ages &lt;13    1.271836\n1                           Asia  Ages &lt;13    7.795371\n2                         Europe  Ages &lt;13    9.083381\n3          High-income countries  Ages &lt;13    2.473921\n4  Lower-middle-income countries  Ages &lt;13    8.800553\n\n\n\n\n\n\n\nCode\nprint('Before: ',df_gdp_per_captita.head())\n#make a new dataframe with the column from 1990 to 2017\ndf_gdp_per_captita_subset=df_gdp_per_captita[['GDP per capita, current prices(U.S. dollars per capita)','1990','1991','1992','1993','1994','1995','1996','1997','1998','1999', \n'2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',  \n'2010','2011','2012','2013','2014','2015','2016','2017']]\ndf_gdp_per_captita_subset=df_gdp_per_captita_subset.rename(columns={'GDP per capita, current prices(U.S. dollars per capita)':'Economy'})\n#melt the dataframe\ndf_gdp_per_captita_subset=pd.melt(df_gdp_per_captita_subset, id_vars=['Economy'], value_vars=['1990','1991','1992','1993','1994','1995','1996','1997','1998','1999',    \n'2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',\n'2010','2011','2012','2013','2014','2015','2016','2017'], var_name='Year', value_name='GDP_per_capita')\nprint('After: ',df_gdp_per_captita_subset.head())\n# print the data type of the column GDP_per_capita\n# print(df_gdp_per_captita_subset.dtypes)\n#convert the column GDP_per_capita and Year from object type to numeric type\ndf_gdp_per_captita_subset['GDP_per_capita']=pd.to_numeric(df_gdp_per_captita_subset['GDP_per_capita'],errors='coerce')\ndf_gdp_per_captita_subset['Year']=pd.to_numeric(df_gdp_per_captita_subset['Year'],errors='coerce')\n\n\nBefore:    GDP per capita, current prices(U.S. dollars per capita)      1980      1981  \\\n0                                        Afghanistan        no data   no data   \n1                                            Albania        728.359   817.734   \n2                                            Algeria       2268.607  2305.505   \n3                                            Andorra        no data   no data   \n4                                             Angola        802.627   731.427   \n\n       1982      1983      1984      1985      1986      1987      1988  ...  \\\n0   no data   no data   no data   no data   no data   no data   no data  ...   \n1   824.542   815.529   788.332   788.801   855.724   832.245   805.046  ...   \n2  2254.328  2316.679  2432.717  2753.697  2698.915  2705.111  2143.742  ...   \n3   no data   no data   no data   no data   no data   no data   no data  ...   \n4   712.576   723.654   747.325   817.068   743.735   828.958   875.526  ...   \n\n        2019       2020       2021       2022       2023       2024  \\\n0    586.204    611.268    443.385    no data    no data    no data   \n1   5345.058   5278.986   6259.762   6657.637    8057.49   8877.337   \n2   3953.402   3321.601   3659.709    4306.82   4874.706    5130.36   \n3  40688.491  36973.845  41806.876  41084.874  44107.317  45642.103   \n4   2612.246   1709.283   2169.648   3438.147   2550.001   2452.737   \n\n        2025       2026       2027       2028  \n0    no data    no data    no data    no data  \n1   9280.572   9818.569   10470.18  11187.728  \n2   5243.056   5335.671   5355.284    5365.19  \n3  46215.307  46546.578  46713.341  46920.949  \n4   2483.027    2507.49   2570.826   2622.394  \n\n[5 rows x 50 columns]\nAfter:         Economy  Year GDP_per_capita\n0  Afghanistan  1990        no data\n1      Albania  1990        675.736\n2      Algeria  1990       2473.512\n3      Andorra  1990        no data\n4       Angola  1990       1062.788\n\n\n\n\n\n\n\nCode\n#F.clean the data named mental_health.xlsx and merge the dataframes\n\n# show all the columns of the dataframe\n# mental_health_df.columns\nprint('Before:', df_mental_health.head())\nprint('Before:', df_mental_health.columns)\n# # drop the columns of the dataframes that do not belong to mental health disorders\ndf_mental_health = df_mental_health.drop(columns=['Drug use disorders (%)', 'Alcohol use disorders (%)'])\n\n#rename the column Entity to Economy\ndf_mental_health.rename(columns={'Entity':'Economy'},inplace=True)\n\n#drop the code column when the column Code is null\ndf_mental_health=df_mental_health.dropna(subset=['Code'])\n\n#merge the dataframe df_mental_health to df_income_group\ndf_mental_health_merge= df_mental_health.merge(df_income_group[['Code','Income group']], on='Code', how='left')\n\n#merge the dataframe df_education to df_mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_education[['Code','average_learning_Adjusted_of_school']], on='Code', how='left')\n\n# merge the two dataframes countries_continents and mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_Countries_Continents[['Economy','Continent']], on='Economy', how='left')\nprint(df_mental_health_merge.head())\n\n#merge the dataframe df_GDP to df_mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_GDP[['Code','GDP(2022)']], on='Code', how='left')\nprint(df_discomfort_speaking_anxiety_depression_2020.head())\n\n#merge the dataframe df_discomfort_speaking_anxiety_depression_2020 to df_mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_discomfort_speaking_anxiety_depression_2020[['Code','not_at_all_comfortable_speaking_anxiety_or_depression_percent']], on='Code', how='left')\nprint(df_mental_health_merge.head())\n\n# print the shape of the dataframe df_mental_health_merge\nprint(df_mental_health_merge.shape)\n\n#merge the dafaqframe df_gdp_per_captita_subset with df_mental_health_merge based on the column Economy and Year\ndf_mental_health_merge=df_mental_health_merge.merge(df_gdp_per_captita_subset[['Economy','Year','GDP_per_capita']], on=['Economy','Year'], how='left')\nprint('After: ', df_mental_health_merge.head())\nprint('After: ', df_mental_health_merge.columns)\n\n# print the shape of the dataframe df_mental_health_merge\nprint(df_mental_health_merge.shape)\n#save the dataframe to csv file\ndf_mental_health_merge.to_csv(\"./Data_cleaned/mental_health.csv\", index=False)\n\n\nBefore:         Entity Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Drug use disorders (%)  \\\n0              0.101855               4.828830                1.677082   \n1              0.099313               4.829740                1.684746   \n2              0.096692               4.831108                1.694334   \n3              0.094336               4.830864                1.705320   \n4              0.092439               4.829423                1.716069   \n\n   Depression (%)  Alcohol use disorders (%)  \n0        4.071831                   0.672404  \n1        4.079531                   0.671768  \n2        4.088358                   0.670644  \n3        4.096190                   0.669738  \n4        4.099582                   0.669260  \nBefore: Index(['Entity', 'Code', 'Year', 'Schizophrenia (%)', 'Bipolar disorder (%)',\n       'Eating disorders (%)', 'Anxiety disorders (%)',\n       'Drug use disorders (%)', 'Depression (%)',\n       'Alcohol use disorders (%)'],\n      dtype='object')\n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%) Income group  \\\n0              0.101855               4.828830        4.071831   Low income   \n1              0.099313               4.829740        4.079531   Low income   \n2              0.096692               4.831108        4.088358   Low income   \n3              0.094336               4.830864        4.096190   Low income   \n4              0.092439               4.829423        4.099582   Low income   \n\n   average_learning_Adjusted_of_school Continent  \n0                             4.957542      Asia  \n1                             4.957542      Asia  \n2                             4.957542      Asia  \n3                             4.957542      Asia  \n4                             4.957542      Asia  \n     Economy Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%) Income group  \\\n0              0.101855               4.828830        4.071831   Low income   \n1              0.099313               4.829740        4.079531   Low income   \n2              0.096692               4.831108        4.088358   Low income   \n3              0.094336               4.830864        4.096190   Low income   \n4              0.092439               4.829423        4.099582   Low income   \n\n   average_learning_Adjusted_of_school Continent  GDP(2022)  \\\n0                             4.957542      Asia    14583.0   \n1                             4.957542      Asia    14583.0   \n2                             4.957542      Asia    14583.0   \n3                             4.957542      Asia    14583.0   \n4                             4.957542      Asia    14583.0   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                                NaN              \n1                                                NaN              \n2                                                NaN              \n3                                                NaN              \n4                                                NaN              \n(5488, 13)\nAfter:         Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%) Income group  \\\n0              0.101855               4.828830        4.071831   Low income   \n1              0.099313               4.829740        4.079531   Low income   \n2              0.096692               4.831108        4.088358   Low income   \n3              0.094336               4.830864        4.096190   Low income   \n4              0.092439               4.829423        4.099582   Low income   \n\n   average_learning_Adjusted_of_school Continent  GDP(2022)  \\\n0                             4.957542      Asia    14583.0   \n1                             4.957542      Asia    14583.0   \n2                             4.957542      Asia    14583.0   \n3                             4.957542      Asia    14583.0   \n4                             4.957542      Asia    14583.0   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \\\n0                                                NaN               \n1                                                NaN               \n2                                                NaN               \n3                                                NaN               \n4                                                NaN               \n\n   GDP_per_capita  \n0             NaN  \n1             NaN  \n2             NaN  \n3             NaN  \n4             NaN  \nAfter:  Index(['Economy', 'Code', 'Year', 'Schizophrenia (%)', 'Bipolar disorder (%)',\n       'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)',\n       'Income group', 'average_learning_Adjusted_of_school', 'Continent',\n       'GDP(2022)',\n       'not_at_all_comfortable_speaking_anxiety_or_depression_percent',\n       'GDP_per_capita'],\n      dtype='object')\n(5488, 14)\n\n\n\n\n\n\n\n\n\n\nCode\ndf_all_mental_health_symtoms.to_csv(\"./Data_cleaned/all_mental_health_symptoms.csv\")\n# save the dataframe to csv file\ndf_symtoms.to_csv(\"./Data_cleaned/symptoms.csv\")\n\n\n\n\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load data from csv and Excel files\nfile_paths &lt;- list.files(path = \"./Data_uncleaned\", pattern = \"\\\\.csv$|\\\\.xlsx$\", full.names = TRUE)\nprint(file_paths)\n\n# Load individual data frames\ndf_GDP &lt;- read_csv(\"./Data_uncleaned/GDP.csv\")\ndf_income_group &lt;- read_excel(\"./Data_uncleaned/Income_group.xlsx\")\ndf_Countries_Continents &lt;- read_csv(\"./Data_uncleaned/Countries_Continents.csv\")\ndf_education &lt;- read_csv(\"./Data_uncleaned/education.csv\")\ndf_discomfort_speaking_anxiety_depression_2020 &lt;- read_csv(\"./Data_uncleaned/discomfort_speaking_anxiety_depression_2020.csv\")\ndf_eating_disorder &lt;- read_csv(\"./Data_uncleaned/eating_disorders_prevalence_males_vs_females.csv\")\ndf_first_anxiety_or_depression &lt;- read_csv(\"./Data_uncleaned/age_when_first_had_anxiety_depression.csv\")\ndf_mental_health &lt;- read_excel(\"./Data_uncleaned/mental_health.xlsx\")\ndf_gdp_per_captita &lt;- read_csv('./Data_uncleaned/GDP_per_captita.csv')\ndf_all_mental_health_symtoms &lt;- read_csv(\"./Data_uncleaned/all_mental_health_symptoms.csv\")\ndf_symtoms &lt;- read_csv(\"./Data_uncleaned/symptoms.csv\")\n\n\n\nFor simplicity, I only show the R cleaning of 2 dataframes here.\n\n\nprint(\"Before: \")\nprint(head(df_eating_disorder))\n\ndf_eating_disorder &lt;- df_eating_disorder %&gt;%\n  select(-Continent, -Population) %&gt;%\n  filter(Year %in% 1990:2021) %&gt;%\n  rename(Economy = Entity) %&gt;%\n  drop_na(Eating_disorders_Male)\n\ndf_eating_disorder$All_gender &lt;- rowMeans(df_eating_disorder[, c(\"Eating_disorders_Male\", \"Eating_disorders_Female\")], na.rm = TRUE)\n\nprint(\"After: \")\nprint(head(df_eating_disorder))\n\n\n\nprint(\"Before: \")\nprint(head(df_discomfort_speaking_anxiety_depression_2020))\n\ndf_discomfort_speaking_anxiety_depression_2020 &lt;- df_discomfort_speaking_anxiety_depression_2020 %&gt;%\n  rename(Economy = Entity)\n\nprint(\"After: \")\nprint(head(df_discomfort_speaking_anxiety_depression_2020))\n\n# write_csv(df_discomfort_speaking_anxiety_depression_2020, \"./Data_Cleaned/discomfort_speaking_anxiety_depression_2020_r.csv\")"
  },
  {
    "objectID": "Cleaning.html#r-cleaning-process",
    "href": "Cleaning.html#r-cleaning-process",
    "title": "Data Cleaning",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load data from csv and Excel files\nfile_paths &lt;- list.files(path = \"./Data_uncleaned\", pattern = \"\\\\.csv$|\\\\.xlsx$\", full.names = TRUE)\nprint(file_paths)\n\n# Load individual data frames\ndf_GDP &lt;- read_csv(\"./Data_uncleaned/GDP.csv\")\ndf_income_group &lt;- read_excel(\"./Data_uncleaned/Income_group.xlsx\")\ndf_Countries_Continents &lt;- read_csv(\"./Data_uncleaned/Countries_Continents.csv\")\ndf_education &lt;- read_csv(\"./Data_uncleaned/education.csv\")\ndf_discomfort_speaking_anxiety_depression_2020 &lt;- read_csv(\"./Data_uncleaned/discomfort_speaking_anxiety_depression_2020.csv\")\ndf_eating_disorder &lt;- read_csv(\"./Data_uncleaned/eating_disorders_prevalence_males_vs_females.csv\")\ndf_first_anxiety_or_depression &lt;- read_csv(\"./Data_uncleaned/age_when_first_had_anxiety_depression.csv\")\ndf_mental_health &lt;- read_excel(\"./Data_uncleaned/mental_health.xlsx\")\ndf_gdp_per_captita &lt;- read_csv('./Data_uncleaned/GDP_per_captita.csv')\ndf_all_mental_health_symtoms &lt;- read_csv(\"./Data_uncleaned/all_mental_health_symptoms.csv\")\ndf_symtoms &lt;- read_csv(\"./Data_uncleaned/symptoms.csv\")\n\n\n\nFor simplicity, I only show the R cleaning of 2 dataframes here.\n\n\nprint(\"Before: \")\nprint(head(df_eating_disorder))\n\ndf_eating_disorder &lt;- df_eating_disorder %&gt;%\n  select(-Continent, -Population) %&gt;%\n  filter(Year %in% 1990:2021) %&gt;%\n  rename(Economy = Entity) %&gt;%\n  drop_na(Eating_disorders_Male)\n\ndf_eating_disorder$All_gender &lt;- rowMeans(df_eating_disorder[, c(\"Eating_disorders_Male\", \"Eating_disorders_Female\")], na.rm = TRUE)\n\nprint(\"After: \")\nprint(head(df_eating_disorder))\n\n\n\nprint(\"Before: \")\nprint(head(df_discomfort_speaking_anxiety_depression_2020))\n\ndf_discomfort_speaking_anxiety_depression_2020 &lt;- df_discomfort_speaking_anxiety_depression_2020 %&gt;%\n  rename(Economy = Entity)\n\nprint(\"After: \")\nprint(head(df_discomfort_speaking_anxiety_depression_2020))\n\n# write_csv(df_discomfort_speaking_anxiety_depression_2020, \"./Data_Cleaned/discomfort_speaking_anxiety_depression_2020_r.csv\")"
  },
  {
    "objectID": "HW-02/Code/cleaning.html",
    "href": "HW-02/Code/cleaning.html",
    "title": "Load data",
    "section": "",
    "text": "#https://pdas.samhsa.gov/saes/substate#\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\n#cd HW-02\n#load data from csv file\nall_data_frames=glob.glob(\"../Data/*.csv\")+glob.glob(\"../Data/*.xlsx\")\nfor fname_index, fname in enumerate(all_data_frames):\n    print(fname_index, fname)\n    \ndf_GDP = pd.read_csv(\"../Data/GDP.csv\") \ndf_income_group = pd.read_excel(\"../Data/Income_group.xlsx\")\ndf_Countries_Continents = pd.read_csv(\"../Data/Countries_Continents.csv\")\ndf_education = pd.read_csv(\"../Data/education.csv\")\ndf_discomfort_speaking_anxiety_depression_2020 = pd.read_csv(\"../Data/discomfort_speaking_anxiety_depression_2020.csv\")\ndf_eating_disorder = pd.read_csv(\"../Data/eating_disorders_prevalence_males_vs_females.csv\")\ndf_first_anxiety_or_depression = pd.read_csv(\"../Data/age_when_first_had_anxiety_depression.csv\")\n\n\n0 ../Data/education.csv\n1 ../Data/Countries_Continents.csv\n2 ../Data/age_when_first_had_anxiety_depression.csv\n3 ../Data/all_mental_health_symptoms.csv\n4 ../Data/GDP_per_captita.csv\n5 ../Data/discomfort_speaking_anxiety_depression_2020.csv\n6 ../Data/eating_disorders_prevalence_males_vs_females.csv\n7 ../Data/GDP.csv\n8 ../Data/Income_group.xlsx\n9 ../Data/mental_health.xlsx\n\n\n\nclean data\n\nEating_disorders\n\n#A.clean the data named eating_disorders\n# show all the columns of the dataframe\nprint('Before : ',df_eating_disorder.head())\n# print(df_eating_disorder.head())\n# drop the columns Continent and Population because they are meaningless to the analysis\ndf_eating_disorder = df_eating_disorder.drop(columns=['Continent','Population'])\n# print(df_eating_disorder.shape)\n# only keep the rows of the dataframe when the column 'Year' is between 1990-2021\n# eating_disorder_df = eating_disorder_df.loc[eating_disorder_df['Year'] == range(1990,2022)]\ndf_eating_disorder = df_eating_disorder[df_eating_disorder['Year'].isin(range(1990,2022))]\nrename_map={\n    'Entity':'Economy',\n    'Year':'Year',\n}\ndf_eating_disorder.rename(columns=rename_map, inplace=True)\n#drop the rows when the column named Eating_disorders_Male is null in the dataframe df_eating_disorder\ndf_eating_disorder= df_eating_disorder.dropna(subset=['Eating_disorders_Male'])\n\n#caculate the mean of the column Male and Female of the dataframe and save it to a new column named All_gender\ndf_eating_disorder['All_gender'] = df_eating_disorder[['Eating_disorders_Male', 'Eating_disorders_Female']].mean(axis=1)\nprint('After: ',df_eating_disorder.head())\n#save the dataframe to csv file\ndf_eating_disorder.to_csv(\"../Cleaned_Data/eating_disorder_male_female.csv\", index=False)\n\nBefore :          Entity      Code  Year  Eating_disorders_Male  \\\n0     Abkhazia  OWID_ABK  2015                    NaN   \n1  Afghanistan       AFG  1990               0.088487   \n2  Afghanistan       AFG  1991               0.086048   \n3  Afghanistan       AFG  1992               0.083625   \n4  Afghanistan       AFG  1993               0.081628   \n\n   Eating_disorders_Female  Population Continent  \n0                      NaN         NaN      Asia  \n1                 0.161867  10694804.0       NaN  \n2                 0.156910  10745168.0       NaN  \n3                 0.152412  12057436.0       NaN  \n4                 0.147938  14003764.0       NaN  \nAfter:         Economy Code  Year  Eating_disorders_Male  Eating_disorders_Female  \\\n1  Afghanistan  AFG  1990               0.088487                 0.161867   \n2  Afghanistan  AFG  1991               0.086048                 0.156910   \n3  Afghanistan  AFG  1992               0.083625                 0.152412   \n4  Afghanistan  AFG  1993               0.081628                 0.147938   \n5  Afghanistan  AFG  1994               0.079439                 0.143980   \n\n   All_gender  \n1    0.125177  \n2    0.121479  \n3    0.118018  \n4    0.114783  \n5    0.111710  \n\n\n\n\nDiscomfort_speaking_anxiety_depression_2020\n\n#B.clean the data named discomfort_speaking_anxiety_depression_2020\nprint('Before : ',df_discomfort_speaking_anxiety_depression_2020.head())\ndf_discomfort_speaking_anxiety_depression_2020 = pd.read_csv(\"../Data/discomfort_speaking_anxiety_depression_2020.csv\")\n#rename the column 'Entity' to 'Country' and 'Share - Question: mh5 - Someone local comfortable speaking about anxiety/depression with someone they know - Answer: Not at all comfortable - Gender: all - Age_group: all' to 'Not_Comfortable'\ndf_discomfort_speaking_anxiety_depression_2020.rename(columns={'Entity':'Economy'}, inplace=True)\nprint('After ', df_discomfort_speaking_anxiety_depression_2020.head())\ndf_discomfort_speaking_anxiety_depression_2020.to_csv(\"../Data/discomfort_speaking_anxiety_depression_2020.csv\", index=False)\n\nBefore :       Economy Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \nAfter       Economy Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \n\n\n\n\nGDP\n\n#C.clean the data named GDP\n#add column names to the dataframe df_GDP\nprint('Before: ',df_GDP.head())\ndf_GDP.columns=['Code', 'Ranking','unnamed:2','Economy','GDP(2022)','unnamed:5']\ndf_GDP=df_GDP.drop(columns=['unnamed:2','unnamed:5'])\n#check the data type of the column GDP\n# print(df_GDP.dtypes)\n#convert the column GDP to numeric type\ndf_GDP['GDP(2022)']=df_GDP['GDP(2022)'].str.replace(',','')\n#convert the column GDP from object type to numeric type\ndf_GDP['GDP(2022)']=pd.to_numeric(df_GDP['GDP(2022)'],errors='coerce')\nprint('After: ',df_GDP.head())\ndf_GDP.to_csv(\"../Cleaned_Data/GDP.csv\", index=False)\n\nBefore:     USA    1  Unnamed: 2   United States   25,462,700   Unnamed: 5\n0  CHN  2.0         NaN           China   17,963,171          NaN\n1  JPN  3.0         NaN           Japan    4,231,141          NaN\n2  DEU  4.0         NaN         Germany    4,072,192          NaN\n3  IND  5.0         NaN           India    3,385,090          NaN\n4  GBR  6.0         NaN  United Kingdom    3,070,668          NaN\nAfter:    Code  Ranking         Economy   GDP(2022)\n0  CHN      2.0           China  17963171.0\n1  JPN      3.0           Japan   4231141.0\n2  DEU      4.0         Germany   4072192.0\n3  IND      5.0           India   3385090.0\n4  GBR      6.0  United Kingdom   3070668.0\n\n\n\n\nCountries_Continents\n\n#D.clean the data named Countries_Continents\n#rename the column Country to Economy\nprint('Before: ',df_Countries_Continents.head())\ndf_Countries_Continents.rename(columns={'Country':'Economy'},inplace=True)\nprint('After: ',df_Countries_Continents.head())\ndf_Countries_Continents.to_csv(\"../Cleaned_Data/Countries_Continents.csv\", index=False)\n\nBefore:    Continent   Country\n0    Africa   Algeria\n1    Africa    Angola\n2    Africa     Benin\n3    Africa  Botswana\n4    Africa   Burkina\nAfter:    Continent   Economy\n0    Africa   Algeria\n1    Africa    Angola\n2    Africa     Benin\n3    Africa  Botswana\n4    Africa   Burkina\n\n\n\n\nEducation\n\n#E.clean the data named education\nprint('Before: ',df_education.head())\n#create a new column named 'average_learning_Adjusted_of_school' to calculate the average of the columns 'Average years of schooling' of each Entity\ndf_education['average_learning_Adjusted_of_school'] = df_education.groupby('Entity')['Learning-Adjusted Years of School'].transform('mean')\n# This method keeps the first occurrence of each entity and drops the subsequent duplicates.\ndf_education = df_education.drop_duplicates(subset='Entity', keep='first')\n# rename the column entity to Economy\ndf_education.rename(columns={'Entity':'Economy'},inplace=True)\nprint('After: ',df_education.head())\ndf_education.to_csv(\"../Cleaned_Data/education.csv\", index=False)\n\nBefore:          Entity Code  Year  Learning-Adjusted Years of School\n0  Afghanistan  AFG  2017                           4.870000\n1  Afghanistan  AFG  2018                           4.949788\n2  Afghanistan  AFG  2020                           5.052838\n3       Africa  NaN  2017                           4.931378\n4       Africa  NaN  2018                           5.123904\nAfter:          Economy Code  Year  Learning-Adjusted Years of School  \\\n0   Afghanistan  AFG  2017                           4.870000   \n3        Africa  NaN  2017                           4.931378   \n6       Albania  ALB  2010                           7.382478   \n10      Algeria  DZA  2010                           7.202200   \n14       Angola  AGO  2017                           4.110000   \n\n    average_learning_Adjusted_of_school  \n0                              4.957542  \n3                              5.058634  \n6                              8.526723  \n10                             7.045445  \n14                             4.193318  \n\n\n\n\nAge_when_first_anxiety_depression\n\n#G.clean the data named age_when_first_anxiety_depression.csv\nprint('Before: ',df_first_anxiety_or_depression.head())\ndf_first_anxiety_or_depression=df_first_anxiety_or_depression.drop(columns=['Code'])\n# print(df_first_anxiety_or_depression.columns)\n#melt the dataframe\ndf_first_anxiety_or_depression=pd.melt(df_first_anxiety_or_depression, id_vars=['Entity'], value_vars=['Ages &lt;13','Ages_13_19','Ages_20_29','Ages_30_39','Ages ≥40','Dont_know/Refused'], var_name='Age', value_name='Percentage')\nprint('After: ',df_first_anxiety_or_depression.head())\ndf_first_anxiety_or_depression.to_csv(\"../Cleaned_Data/age_when_first_anxiety_or_depression.csv\", index=False)\n\n# # Sample dataframe\n# df = pd.DataFrame({\n#     'A': ['a1', 'a2', 'a3'],\n#     'B': [1, 2, 3],\n#     'C': [4, 5, 6],\n#     'D': [7, 8, 9]\n# })\n\n# print(\"Original DataFrame:\")\n# print(df)\n\n# # Use melt to transform the dataframe\n# melted_df = pd.melt(df, id_vars=['A'], value_vars=['B', 'C', 'D'], var_name='Variable', value_name='Value')\n\n# print(\"\\nMelted DataFrame:\")\n# print(melted_df)\n\nBefore:                            Entity Code  Year  Ages &lt;13  Ages_13_19  Ages_20_29  \\\n0                         Africa  NaN  2020  1.271836   14.229228   22.264650   \n1                           Asia  NaN  2020  7.795371    9.628078   23.403633   \n2                         Europe  NaN  2020  9.083381   22.210781   22.034777   \n3          High-income countries  NaN  2020  2.473921   24.484507   10.832585   \n4  Lower-middle-income countries  NaN  2020  8.800553    8.485406   26.288944   \n\n   Ages_30_39   Ages ≥40  Dont_know/Refused  \n0   16.803090   2.706476          42.724724  \n1   12.663021   9.983976          36.525925  \n2    7.628320  24.531280          14.511458  \n3   16.975899  16.115680          29.117413  \n4    8.428943   7.348721          40.647434  \nAfter:                            Entity       Age  Percentage\n0                         Africa  Ages &lt;13    1.271836\n1                           Asia  Ages &lt;13    7.795371\n2                         Europe  Ages &lt;13    9.083381\n3          High-income countries  Ages &lt;13    2.473921\n4  Lower-middle-income countries  Ages &lt;13    8.800553\n\n\n\n\nGDP_per_capita\n\n#H.cleaning GDP_per_capita.csv\ndf_gdp_per_captita=pd.read_csv('../Data/GDP_per_captita.csv')\nprint('Before: ',df_gdp_per_captita.head())\n#make a new dataframe with the column from 1990 to 2017\ndf_gdp_per_captita_subset=df_gdp_per_captita[['GDP per capita, current prices(U.S. dollars per capita)','1990','1991','1992','1993','1994','1995','1996','1997','1998','1999', \n'2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',  \n'2010','2011','2012','2013','2014','2015','2016','2017']]\ndf_gdp_per_captita_subset=df_gdp_per_captita_subset.rename(columns={'GDP per capita, current prices(U.S. dollars per capita)':'Economy'})\n#melt the dataframe\ndf_gdp_per_captita_subset=pd.melt(df_gdp_per_captita_subset, id_vars=['Economy'], value_vars=['1990','1991','1992','1993','1994','1995','1996','1997','1998','1999',    \n'2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',\n'2010','2011','2012','2013','2014','2015','2016','2017'], var_name='Year', value_name='GDP_per_capita')\nprint('After: ',df_gdp_per_captita_subset.head())\n# print the data type of the column GDP_per_capita\n# print(df_gdp_per_captita_subset.dtypes)\n#convert the column GDP_per_capita and Year from object type to numeric type\ndf_gdp_per_captita_subset['GDP_per_capita']=pd.to_numeric(df_gdp_per_captita_subset['GDP_per_capita'],errors='coerce')\ndf_gdp_per_captita_subset['Year']=pd.to_numeric(df_gdp_per_captita_subset['Year'],errors='coerce')\n# df_gdp_per_captita_subset.to_csv('../Cleaned_Data/GDP_percaptita.csv',index=False)\n\nBefore:    GDP per capita, current prices(U.S. dollars per capita)      1980      1981  \\\n0                                        Afghanistan        no data   no data   \n1                                            Albania        728.359   817.734   \n2                                            Algeria       2268.607  2305.505   \n3                                            Andorra        no data   no data   \n4                                             Angola        802.627   731.427   \n\n       1982      1983      1984      1985      1986      1987      1988  ...  \\\n0   no data   no data   no data   no data   no data   no data   no data  ...   \n1   824.542   815.529   788.332   788.801   855.724   832.245   805.046  ...   \n2  2254.328  2316.679  2432.717  2753.697  2698.915  2705.111  2143.742  ...   \n3   no data   no data   no data   no data   no data   no data   no data  ...   \n4   712.576   723.654   747.325   817.068   743.735   828.958   875.526  ...   \n\n        2019       2020       2021       2022       2023       2024  \\\n0    586.204    611.268    443.385    no data    no data    no data   \n1   5345.058   5278.986   6259.762   6657.637    8057.49   8877.337   \n2   3953.402   3321.601   3659.709    4306.82   4874.706    5130.36   \n3  40688.491  36973.845  41806.876  41084.874  44107.317  45642.103   \n4   2612.246   1709.283   2169.648   3438.147   2550.001   2452.737   \n\n        2025       2026       2027       2028  \n0    no data    no data    no data    no data  \n1   9280.572   9818.569   10470.18  11187.728  \n2   5243.056   5335.671   5355.284    5365.19  \n3  46215.307  46546.578  46713.341  46920.949  \n4   2483.027    2507.49   2570.826   2622.394  \n\n[5 rows x 50 columns]\nAfter:         Economy  Year GDP_per_capita\n0  Afghanistan  1990        no data\n1      Albania  1990        675.736\n2      Algeria  1990       2473.512\n3      Andorra  1990        no data\n4       Angola  1990       1062.788\n\n\n\n\nMental_health\n\n#F.clean the data named mental_health.xlsx and merge the dataframes\n\ndf_mental_health = pd.read_excel('../Data/mental_health.xlsx')\n# show all the columns of the dataframe\n# mental_health_df.columns\nprint(df_mental_health.head())\n# # drop the columns of the dataframes that do not belong to mental health disorders\ndf_mental_health = df_mental_health.drop(columns=['Drug use disorders (%)', 'Alcohol use disorders (%)'])\n\n#rename the column Entity to Economy\ndf_mental_health.rename(columns={'Entity':'Economy'},inplace=True)\n\n#drop the code column when the column Code is null\ndf_mental_health=df_mental_health.dropna(subset=['Code'])\n\n#merge the dataframe df_mental_health to df_income_group\ndf_mental_health_merge= df_mental_health.merge(df_income_group[['Code','Income group']], on='Code', how='left')\n\n#merge the dataframe df_education to df_mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_education[['Code','average_learning_Adjusted_of_school']], on='Code', how='left')\n\n# merge the two dataframes countries_continents and mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_Countries_Continents[['Economy','Continent']], on='Economy', how='left')\nprint(df_mental_health_merge.head())\n\n#merge the dataframe df_GDP to df_mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_GDP[['Code','GDP(2022)']], on='Code', how='left')\nprint(df_discomfort_speaking_anxiety_depression_2020.head())\n\n#merge the dataframe df_discomfort_speaking_anxiety_depression_2020 to df_mental_health_merge\ndf_mental_health_merge=df_mental_health_merge.merge(df_discomfort_speaking_anxiety_depression_2020[['Code','not_at_all_comfortable_speaking_anxiety_or_depression_percent']], on='Code', how='left')\nprint(df_mental_health_merge.head())\n\n# print the shape of the dataframe df_mental_health_merge\nprint(df_mental_health_merge.shape)\n#merge the dafaqframe df_gdp_per_captita_subset with df_mental_health_merge based on the column Economy and Year\ndf_mental_health_merge=df_mental_health_merge.merge(df_gdp_per_captita_subset[['Economy','Year','GDP_per_capita']], on=['Economy','Year'], how='left')\nprint(df_mental_health_merge.head())\n\n# print the shape of the dataframe df_mental_health_merge\nprint(df_mental_health_merge.shape)\n#save the dataframe to csv file\ndf_mental_health_merge.to_csv(\"../Cleaned_Data/mental_health.csv\", index=False)\n\n\n        Entity Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Drug use disorders (%)  \\\n0              0.101855               4.828830                1.677082   \n1              0.099313               4.829740                1.684746   \n2              0.096692               4.831108                1.694334   \n3              0.094336               4.830864                1.705320   \n4              0.092439               4.829423                1.716069   \n\n   Depression (%)  Alcohol use disorders (%)  \n0        4.071831                   0.672404  \n1        4.079531                   0.671768  \n2        4.088358                   0.670644  \n3        4.096190                   0.669738  \n4        4.099582                   0.669260  \n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%) Income group  \\\n0              0.101855               4.828830        4.071831   Low income   \n1              0.099313               4.829740        4.079531   Low income   \n2              0.096692               4.831108        4.088358   Low income   \n3              0.094336               4.830864        4.096190   Low income   \n4              0.092439               4.829423        4.099582   Low income   \n\n   average_learning_Adjusted_of_school Continent  \n0                             4.957542      Asia  \n1                             4.957542      Asia  \n2                             4.957542      Asia  \n3                             4.957542      Asia  \n4                             4.957542      Asia  \n     Economy Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%) Income group  \\\n0              0.101855               4.828830        4.071831   Low income   \n1              0.099313               4.829740        4.079531   Low income   \n2              0.096692               4.831108        4.088358   Low income   \n3              0.094336               4.830864        4.096190   Low income   \n4              0.092439               4.829423        4.099582   Low income   \n\n   average_learning_Adjusted_of_school Continent  GDP(2022)  \\\n0                             4.957542      Asia    14583.0   \n1                             4.957542      Asia    14583.0   \n2                             4.957542      Asia    14583.0   \n3                             4.957542      Asia    14583.0   \n4                             4.957542      Asia    14583.0   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                                NaN              \n1                                                NaN              \n2                                                NaN              \n3                                                NaN              \n4                                                NaN              \n(5488, 13)\n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%) Income group  \\\n0              0.101855               4.828830        4.071831   Low income   \n1              0.099313               4.829740        4.079531   Low income   \n2              0.096692               4.831108        4.088358   Low income   \n3              0.094336               4.830864        4.096190   Low income   \n4              0.092439               4.829423        4.099582   Low income   \n\n   average_learning_Adjusted_of_school Continent  GDP(2022)  \\\n0                             4.957542      Asia    14583.0   \n1                             4.957542      Asia    14583.0   \n2                             4.957542      Asia    14583.0   \n3                             4.957542      Asia    14583.0   \n4                             4.957542      Asia    14583.0   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \\\n0                                                NaN               \n1                                                NaN               \n2                                                NaN               \n3                                                NaN               \n4                                                NaN               \n\n   GDP_per_capita  \n0             NaN  \n1             NaN  \n2             NaN  \n3             NaN  \n4             NaN  \n(5488, 14)"
  },
  {
    "objectID": "HW-03/Code/EDA.html",
    "href": "HW-03/Code/EDA.html",
    "title": "Data Understanding",
    "section": "",
    "text": "import pandas as pd\n#load the data sets\ndf_age_first_depression=pd.read_csv('../Data/age_when_first_anxiety_or_depression.csv')\ndf_eating_disorder=pd.read_csv('../Data/eating_disorder_male_female.csv')\ndf_mental_health=pd.read_csv('../Data/mental_health.csv')\ndf_gdp_mental_health_2017=pd.read_csv('../Data/GDP_percaptita_mental_health_2017.csv')\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicators like income group, average learning adjusted years of school, continent, GDP for 2022, and a column related to comfort speaking about anxiety or depression, which has many missing values (NaN).\nEconomy: The name of the economy or country. Code: The country code.\nYear: The year of the data record.\nSchizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage.\nEating disorders (%): The prevalence of eating disorders as a percentage.\nAnxiety disorders (%): The prevalence of anxiety disorders as a percentage.\nDepression (%): The prevalence of depression as a percentage.\nIncome group: The income group classification of the economy.\naverage_learning_Adjusted_of_school: Some metric related to schooling, perhaps average years of schooling adjusted for learning.\nContinent: The continent where the economy is located.\nGDP(2022): The GDP of the economy for the year 2022.\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent: The percentage of people not at all comfortable speaking about anxiety or depression\nRecords: 5,488 Variables: 13 (1 integer, 8 floats, 4 objects) Features: Includes country data, year, percentages for various mental health conditions, income group, average learning, continent, GDP, and comfort speaking about anxiety/depression.\nThis dataset includes the prevalence of eating disorders among males and females, as well as a combined figure for all genders, across different countries and years.\nCountry: The name of the country.\nCountry Code: The corresponding country code.\nYear: The year of the observation.\nEating_disorders_Male: The prevalence of eating disorders among males.\nEating_disorders_Female: The prevalence of eating disorders among females.\nAll_gender: The prevalence of eating disorders across all genders.\nRecords: 6,420 Variables: 6 (1 integer, 3 floats, 2 objects) Features: Includes country data, year, and eating disorder prevalence separated by male, female, and all genders.\nThe dataset contains information on the age at which individuals first experienced anxiety or depression, categorized by different entities (which seem to represent regions or income categories).\nEntity: The region or income category.\nAge: The age category when anxiety or depression was first experienced.\nPercentage: The percentage of individuals in that entity and age category.\nRecords: 42 Variables: 3 (2 objects, 1 float) Features: Includes entity (could be country or other types of entities), age category, and percentage of individuals with first anxiety or depression experience.\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicator like GDP per captita.\nEconomy: The name of the economy or country. Year: The GDP_per_captita of the year 2017 of the economy or country. Schizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage. Eating disorders (%): The prevalence of eating disorders as a percentage. Anxiety disorders (%): The prevalence of anxiety disorders as a percentage. Depression (%): The prevalence of depression as a percentage.\nRecords: 166 Variables, 7 features ( 2 objects, 5 float) features:\ndatasets={\n    'age_first_depression':df_age_first_depression,\n    'eating_disorder':df_eating_disorder,\n    'mental_health':df_mental_health,\n    'gdp_mental_health_2017':df_gdp_mental_health_2017\n}\n\nfor name,df in datasets.items():\n    print(f\"{name} Dataset - First 5 Rows:\")\n    display(df.head())\n    print(f\"\\n{name} Dataset - Info:\")\n    display(df.info())\n    print(f\"{name} Dataset -Shape:\")\n    display(df.shape)\n    print('--------------------------------------------------------------------')\n\nage_first_depression Dataset - First 5 Rows:\n\nage_first_depression Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Entity      42 non-null     object \n 1   Age         42 non-null     object \n 2   Percentage  42 non-null     float64\ndtypes: float64(1), object(2)\nmemory usage: 1.1+ KB\nage_first_depression Dataset -Shape:\n--------------------------------------------------------------------\neating_disorder Dataset - First 5 Rows:\n\neating_disorder Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6420 entries, 0 to 6419\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Economy                  6420 non-null   object \n 1   Code                     6150 non-null   object \n 2   Year                     6420 non-null   int64  \n 3   Eating_disorders_Male    6420 non-null   float64\n 4   Eating_disorders_Female  6420 non-null   float64\n 5   All_gender               6420 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 301.1+ KB\neating_disorder Dataset -Shape:\n--------------------------------------------------------------------\nmental_health Dataset - First 5 Rows:\n\nmental_health Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5488 entries, 0 to 5487\nData columns (total 13 columns):\n #   Column                                                         Non-Null Count  Dtype  \n---  ------                                                         --------------  -----  \n 0   Economy                                                        5488 non-null   object \n 1   Code                                                           5488 non-null   object \n 2   Year                                                           5488 non-null   int64  \n 3   Schizophrenia (%)                                              5488 non-null   float64\n 4   Bipolar disorder (%)                                           5488 non-null   float64\n 5   Eating disorders (%)                                           5488 non-null   float64\n 6   Anxiety disorders (%)                                          5488 non-null   float64\n 7   Depression (%)                                                 5488 non-null   float64\n 8   Income group                                                   5432 non-null   object \n 9   average_learning_Adjusted_of_school                            4676 non-null   float64\n 10  Continent                                                      4900 non-null   object \n 11  GDP(2022)                                                      5264 non-null   float64\n 12  not_at_all_comfortable_speaking_anxiety_or_depression_percent  3108 non-null   float64\ndtypes: float64(8), int64(1), object(4)\nmemory usage: 557.5+ KB\nmental_health Dataset -Shape:\n--------------------------------------------------------------------\ngdp_mental_health_2017 Dataset - First 5 Rows:\n\ngdp_mental_health_2017 Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 166 entries, 0 to 165\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Economy                166 non-null    object \n 1   2017                   166 non-null    object \n 2   Schizophrenia (%)      166 non-null    float64\n 3   Bipolar disorder (%)   166 non-null    float64\n 4   Eating disorders (%)   166 non-null    float64\n 5   Anxiety disorders (%)  166 non-null    float64\n 6   Depression (%)         166 non-null    float64\ndtypes: float64(5), object(2)\nmemory usage: 9.2+ KB\ngdp_mental_health_2017 Dataset -Shape:\n--------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nEntity\nAge\nPercentage\n\n\n\n\n0\nAfrica\nAges &lt;13\n1.271836\n\n\n1\nAsia\nAges &lt;13\n7.795371\n\n\n2\nEurope\nAges &lt;13\n9.083381\n\n\n3\nHigh-income countries\nAges &lt;13\n2.473921\n\n\n4\nLower-middle-income countries\nAges &lt;13\n8.800553\n\n\n\n\n\n\n\nNone\n\n\n(42, 3)\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nEating_disorders_Male\nEating_disorders_Female\nAll_gender\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.088487\n0.161867\n0.125177\n\n\n1\nAfghanistan\nAFG\n1991\n0.086048\n0.156910\n0.121479\n\n\n2\nAfghanistan\nAFG\n1992\n0.083625\n0.152412\n0.118018\n\n\n3\nAfghanistan\nAFG\n1993\n0.081628\n0.147938\n0.114783\n\n\n4\nAfghanistan\nAFG\n1994\n0.079439\n0.143980\n0.111710\n\n\n\n\n\n\n\nNone\n\n\n(6420, 6)\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.160560\n0.697779\n0.101855\n4.828830\n4.071831\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n1\nAfghanistan\nAFG\n1991\n0.160312\n0.697961\n0.099313\n4.829740\n4.079531\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n2\nAfghanistan\nAFG\n1992\n0.160135\n0.698107\n0.096692\n4.831108\n4.088358\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n3\nAfghanistan\nAFG\n1993\n0.160037\n0.698257\n0.094336\n4.830864\n4.096190\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n4\nAfghanistan\nAFG\n1994\n0.160022\n0.698469\n0.092439\n4.829423\n4.099582\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n\n\n\n\n\nNone\n\n\n(5488, 13)\n\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n\n\n\n\n\n\n\nNone\n\n\n(166, 7)"
  },
  {
    "objectID": "HW-03/Code/EDA.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "href": "HW-03/Code/EDA.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "title": "Data Understanding",
    "section": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation",
    "text": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation\nCalculate and report basic summary statistics for the datasets.\nThis will include mean, median, mode, standard deviation, and variance for numerical variables. For categorical variables, we’ll provide frequency distributions.​\n\ndescription_stats={\n    \"age_first_depression_or_anxiety\":df_age_first_depression.describe(include='all'),\n    \"eating_disorder\":df_eating_disorder.describe(include='all'),\n    'gdp_mental_health_2017':df_gdp_mental_health_2017.describe(include='all'),\n    \"mental_health\":df_mental_health.describe(include='all'),\n}\ndescription_stats\n\n{'age_first_depression_or_anxiety':         Entity       Age  Percentage\n count       42        42   42.000000\n unique       7         6         NaN\n top     Africa  Ages &lt;13         NaN\n freq         6         7         NaN\n mean       NaN       NaN   16.666667\n std        NaN       NaN   10.060695\n min        NaN       NaN    1.271836\n 25%        NaN       NaN    8.871260\n 50%        NaN       NaN   14.426575\n 75%        NaN       NaN   22.251861\n max        NaN       NaN   42.724724,\n 'eating_disorder':             Economy  Code         Year  Eating_disorders_Male  \\\n count          6420  6150  6420.000000            6420.000000   \n unique          214   205          NaN                    NaN   \n top     Afghanistan   AFG          NaN                    NaN   \n freq             30    30          NaN                    NaN   \n mean            NaN   NaN  2004.500000               0.119775   \n std             NaN   NaN     8.656116               0.068943   \n min             NaN   NaN  1990.000000               0.033360   \n 25%             NaN   NaN  1997.000000               0.071057   \n 50%             NaN   NaN  2004.500000               0.098256   \n 75%             NaN   NaN  2012.000000               0.148705   \n max             NaN   NaN  2019.000000               0.672270   \n \n         Eating_disorders_Female   All_gender  \n count               6420.000000  6420.000000  \n unique                      NaN          NaN  \n top                         NaN          NaN  \n freq                        NaN          NaN  \n mean                   0.273787     0.196781  \n std                    0.214920     0.140066  \n min                    0.056762     0.045083  \n 25%                    0.121105     0.096171  \n 50%                    0.187430     0.144395  \n 75%                    0.352395     0.252282  \n max                    1.395754     1.034012  ,\n 'gdp_mental_health_2017':             Economy     2017  Schizophrenia (%)  Bipolar disorder (%)  \\\n count           166      166         166.000000            166.000000   \n unique          166      166                NaN                   NaN   \n top     Afghanistan  635.789                NaN                   NaN   \n freq              1        1                NaN                   NaN   \n mean            NaN      NaN           0.210030              0.736341   \n std             NaN      NaN           0.040753              0.157692   \n min             NaN      NaN           0.149087              0.411127   \n 25%             NaN      NaN           0.182119              0.626204   \n 50%             NaN      NaN           0.201004              0.708670   \n 75%             NaN      NaN           0.234878              0.845505   \n max             NaN      NaN           0.363326              1.206088   \n \n         Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \n count             166.000000             166.000000      166.000000  \n unique                   NaN                    NaN             NaN  \n top                      NaN                    NaN             NaN  \n freq                     NaN                    NaN             NaN  \n mean                0.260652               4.024311        3.443300  \n std                 0.167330               1.176412        0.621952  \n min                 0.079896               2.066871        2.196154  \n 25%                 0.136695               3.187924        2.958046  \n 50%                 0.204115               3.593617        3.463656  \n 75%                 0.307116               4.712918        3.828232  \n max                 0.943081               8.539931        5.636661  ,\n 'mental_health':             Economy  Code         Year  Schizophrenia (%)  \\\n count          5488  5488  5488.000000        5488.000000   \n unique          196   196          NaN                NaN   \n top     Afghanistan   AFG          NaN                NaN   \n freq             28    28          NaN                NaN   \n mean            NaN   NaN  2003.500000           0.208183   \n std             NaN   NaN     8.078483           0.041998   \n min             NaN   NaN  1990.000000           0.146902   \n 25%             NaN   NaN  1996.750000           0.179452   \n 50%             NaN   NaN  2003.500000           0.198509   \n 75%             NaN   NaN  2010.250000           0.230554   \n max             NaN   NaN  2017.000000           0.375110   \n \n         Bipolar disorder (%)  Eating disorders (%)  Anxiety disorders (%)  \\\n count            5488.000000           5488.000000            5488.000000   \n unique                   NaN                   NaN                    NaN   \n top                      NaN                   NaN                    NaN   \n freq                     NaN                   NaN                    NaN   \n mean                0.716884              0.234023               3.946979   \n std                 0.164246              0.154147               1.134810   \n min                 0.314535              0.073908               2.023393   \n 25%                 0.615732              0.121760               3.178912   \n 50%                 0.693954              0.180378               3.515140   \n 75%                 0.830217              0.278681               4.659540   \n max                 1.206597              0.943991               8.967330   \n \n         Depression (%) Income group  average_learning_Adjusted_of_school  \\\n count      5488.000000         5432                          4676.000000   \n unique             NaN            4                                  NaN   \n top                NaN  High income                                  NaN   \n freq               NaN         1764                                  NaN   \n mean          3.474504          NaN                             7.760170   \n std           0.671741          NaN                             2.528810   \n min           2.139903          NaN                             2.251002   \n 25%           2.955355          NaN                             5.684793   \n 50%           3.461421          NaN                             7.855914   \n 75%           3.877343          NaN                            10.048229   \n max           6.602754          NaN                            12.775495   \n \n        Continent     GDP(2022)  \\\n count       4900  5.264000e+03   \n unique         6           NaN   \n top       Africa           NaN   \n freq        1428           NaN   \n mean         NaN  3.929245e+05   \n std          NaN  1.451071e+06   \n min          NaN  2.230000e+02   \n 25%          NaN  1.240875e+04   \n 50%          NaN  4.239550e+04   \n 75%          NaN  2.550042e+05   \n max          NaN  1.796317e+07   \n \n         not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n count                                         3108.000000              \n unique                                                NaN              \n top                                                   NaN              \n freq                                                  NaN              \n mean                                            30.009805              \n std                                             11.940808              \n min                                              5.833535              \n 25%                                             21.170624              \n 50%                                             30.920343              \n 75%                                             37.729200              \n max                                             58.778120              }\n\n\n\nMental Health Dataset\nCorrelation Heatmap: To see how different types of mental health disorders are correlated with each other.\nTime Series Plot: To observe the trend of a specific disorder over time for a particular economy or aggregated globally.\nBar Chart: To compare the prevalence of different disorders in a specific year across multiple economies.\nBox Plot: To visualize the distribution of prevalence rates for a particular disorder across different income groups or continents.\nScatter Plot: To examine the relationship between GDP and the prevalence of a particular disorder or the discomfort in speaking about mental health issues.\nHistograms for mental disorders\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the aesthetics for the plots\nsns.set(style=\"whitegrid\")\n\n# Define a function to create histograms for given columns\ndef plot_histograms(data, columns, bins=20, figsize=(15, 5)):\n    fig, axes = plt.subplots(1, len(columns), figsize=figsize)\n    for ax, col in zip(axes, columns):\n        sns.histplot(data[col].dropna(), bins=bins, ax=ax, kde=True)  # Drop NaN for plotting\n        ax.set_title(f'Distribution of {col}')\n    plt.tight_layout()\n    return fig\n\n# Select columns to plot for Mental Health Data (excluding 'Year' and non-numerical columns)\nmental_health_columns = ['Schizophrenia (%)', 'Bipolar disorder (%)', \n                         'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)']\n\n# Plot histograms for the selected columns\nhistograms_mental_health = plot_histograms(df_mental_health, mental_health_columns)\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_income_group=df_mental_health.groupby('Income group').mean()\ndf_income_group=df_income_group.drop(columns=['Year','GDP(2022)','not_at_all_comfortable_speaking_anxiety_or_depression_percent'])\nprint(type(df_income_group))\nprint(df_income_group)\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 6))\ndf_income_group.plot(kind='bar', ax=ax)\n\n# Customization\nax.set_title('Mean Percentage of Mental Health Disorders by Income Group')\nax.set_ylabel('Mean Percentage')\nax.set_xlabel('Income Group')\nplt.xticks(rotation=45)\nplt.legend(title='Mental Health Disorders')\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n                     Schizophrenia (%)  Bipolar disorder (%)  \\\nIncome group                                                   \nHigh income                   0.243514              0.822296   \nLow income                    0.164570              0.626751   \nLower middle income           0.192946              0.631165   \nUpper middle income           0.202089              0.724714   \n\n                     Eating disorders (%)  Anxiety disorders (%)  \\\nIncome group                                                       \nHigh income                      0.399097               4.707860   \nLow income                       0.102740               3.500061   \nLower middle income              0.134964               3.493755   \nUpper middle income              0.202037               3.739220   \n\n                     Depression (%)  average_learning_Adjusted_of_school  \nIncome group                                                              \nHigh income                3.575551                            10.382413  \nLow income                 3.804767                             4.031739  \nLower middle income        3.495127                             6.440634  \nUpper middle income        3.172075                             7.940104  \n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1122149726.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_income_group=df_mental_health.groupby('Income group').mean()\n\n\n\n\n\n\nCorralation Analysis\nCorrelation Heatmap\n‘Income group’ and ‘Continent’ to the list of columns for the correlation matrix, which are categorical variables. The correlation matrix typically requires numerical variables. To include these categorical variables, we would need to convert them into a numerical format using techniques such as one-hot encoding.\n\nimport numpy as np\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    # 'Income group',\n    # 'Continent',\n\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\n\n\n\n\n\nThe updated heatmap now includes the one-hot encoded categorical variables ‘Income group’ and ‘Continent’. Due to the number of categories, the annotations were turned off to keep the heatmap readable. Each square still represents the correlation between two variables, with warmer colors indicating a stronger positive correlation and cooler colors a stronger negative correlation.\nTime Series Plot\nGlobal trend of ‘Depression (%)’ over time.\n\n# Time Series Plot for 'Depression (%)' trend globally over time\n\n# Calculating the global average of 'Depression (%)' for each year\nglobal_depression_trend = df_mental_health.groupby('Year')['Depression (%)'].mean().reset_index()\n\n# Plotting the global trend of 'Depression (%)' over time\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=global_depression_trend, x='Year', y='Depression (%)')\nplt.title('Global Trend of Depression Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Depression (%)')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\nThe Time Series Plot above shows the global trend of depression over time, depicting the average percentage of depression prevalence across all economies for each year.\n#### Data Grouping and Segmentation Bar Chart\ncompare the prevalence of ‘Anxiety disorders (%)’ and ‘Eating disorders (%)’ in the most recent year available across the top 10 economies by GDP. To do this, we first need to identify the top 10 economies by GDP and then gather the data for the specified disorders.\n\n# Bar Chart: Prevalence of 'Anxiety disorders (%)' and 'Eating disorders (%)' ,'Bipolar disorder (%),in the most recent year across top 10 economies by GDP\n\n# Identifying the most recent year available in the dataset\nmost_recent_year = df_mental_health['Year'].max()\n\n# Identifying the top 10 economies by GDP in the most recent year\ntop_economies = df_mental_health[df_mental_health['Year'] == most_recent_year].sort_values(by='GDP(2022)', ascending=False).head(10)\n\n# Preparing the data for the bar chart\nbar_chart_data = top_economies[['Economy', 'Anxiety disorders (%)', 'Eating disorders (%)','Bipolar disorder (%)']].set_index('Economy')\n\n# Plotting the bar chart\nbar_chart_data.plot(kind='bar', figsize=(14, 7))\nplt.title(f'Prevalence of Anxiety and Eating Disorders in {most_recent_year} Across Top 10 Economies by GDP')\nplt.xlabel('Economy')\nplt.ylabel('Prevalence (%)')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y')\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe Bar Chart displays the prevalence of anxiety and eating disorders in the most recent year available across the top 10 economies as measured by GDP. Each economy is represented by a bar, with separate segments for anxiety disorders and eating disorders, allowing for a direct comparison between these conditions within each economy.\nwe can draw the conclusion that the Eating disorders and Anxiety disorders, Bipolar disorder (%) are not related to GDP\nThe Box Plot\nBox Plot: Distribution of ‘Bipolar disorder (%)’ prevalence rates across different income groups\n\n\n# Filter the data for 'Bipolar disorder (%)' for the most recent year\nbipolar_data_recent_year = df_mental_health[df_mental_health['Year'] == most_recent_year]\n\n# Plotting the box plot\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=bipolar_data_recent_year, x='Income group', y='Bipolar disorder (%)')\nplt.title(f'Distribution of Bipolar Disorder Prevalence Across Income Groups in {most_recent_year}')\nplt.xlabel('Income Group')\nplt.ylabel('Bipolar Disorder Prevalence (%)')\nplt.grid(axis='y')\n\n# Show the box plot\nplt.show()\n\n\n\n\nThe Box Plot above shows the distribution of bipolar disorder prevalence rates across different income groups for the most recent year available. The plot provides insights into the median prevalence rates, the interquartile ranges, and any potential outliers within each income group category.\nWe can draw the conclusion that the Bipolar Disorder is realted to Income group.\nScatter Plot:\nExamine the relationship between GDP and the prevalence of depression for the most recent year across all economies.​\n\n# Scatter Plot: Relationship between 'GDP(2022)' and 'Depression (%)' for the most recent year\n\n# Plotting the scatter plot\nplt.figure(figsize=(10, 7))\nsns.scatterplot(data=bipolar_data_recent_year, x='GDP(2022)', y='Depression (%)')\nplt.title(f'Relationship Between GDP and Depression Prevalence in {most_recent_year}')\nplt.xlabel('GDP (2022)')\nplt.ylabel('Depression Prevalence (%)')\nplt.grid(True)\n\n# Show the scatter plot\nplt.show()\n\n\n\n\n\nwe can draw the conclusion that Depression Prevalence (%) are not related to GDP\nThe scatter plot\n\n# Let's create a function that generates scatter plots for any given list of disorders.\n# This function will adjust the number of subplots based on the number of disorders.\n\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\neducation_col = 'average_learning_Adjusted_of_school'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\n# Now let's use the function to create scatter plots\ncreate_scatter_plots(df_mental_health, education_col, mental_health_disorders, 2)\n\n\n\n\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone.\n\n\n\nEating disorder Dataset\nTime Series Line Plot: Showing the trend of eating disorder prevalence over years for a specific country or averaged globally.\nBar Chart: Comparing the prevalence of eating disorders between males and females across different countries.\nBox Plot: Displaying the distribution of eating disorder prevalence for all countries across different years to see the variability and outliers.\nHeatmap: Visualizing the prevalence of eating disorders across countries and years in a color-coded format.\nScatter Plot: Comparing the male vs. female prevalence of eating disorders to see the correlation between genders.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the average prevalence of eating disorders for each year across all countries\naverage_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n# Time Series Line Plot for the global trend\nplt.figure(figsize=(14, 7))\n\n# Plotting the trends for each gender and all genders combined\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Male', label='Male')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Female', label='Female')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='All_gender', label='All Genders')\n\nplt.title('Global Average Prevalence of Eating Disorders Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Prevalence')\nplt.legend()\nplt.grid(True)\n\n# Show the line plot\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/758625183.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  average_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n\n\n\n\nThe line plot above shows the global average prevalence of eating disorders over time for males, females, and all genders combined. The trends can be compared to see how the prevalence has changed over the years.\nBar chart compare the prevalence of eating disorders between males and females across different countries. For clarity, we will take a subset of countries to avoid cluttering the chart.\n\n# Randomly select 10 countries for comparison\nrandom_countries = df_eating_disorder['Economy'].drop_duplicates().sample(10, random_state=1)\n\n# Filter the data for the selected countries and the latest year available\nlatest_year = df_eating_disorder['Year'].max()\ncomparison_data = df_eating_disorder[(df_eating_disorder['Economy'].isin(random_countries)) & (df_eating_disorder['Year'] == latest_year)]\n\n# Bar Chart: Comparing the prevalence of eating disorders between males and females across different countries\nplt.figure(figsize=(14, 7))\n\n# Plot for males\nsns.barplot(x='Economy', y='Eating_disorders_Male', data=comparison_data, color='lightblue', label='Male')\n\n# Plot for females on top of males to create a grouped bar chart\nsns.barplot(x='Economy', y='Eating_disorders_Female', data=comparison_data, color='pink', label='Female')\n\nplt.title(f'Comparison of Eating Disorders Prevalence by Gender in {latest_year}')\nplt.xlabel('Economy')\nplt.ylabel('Prevalence of Eating Disorders')\nplt.xticks(rotation=45, ha='right')\nplt.legend()\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe bar chart above compares the prevalence of eating disorders between males (in light blue) and females (in pink) for a randomly selected subset of 10 countries in the latest year available in the dataset.\nData Grouping and Segmentation For the third visualization, We can group the data by country and by year to analyze trends over time and segment by gender to compare the prevalence rates.\nIdentifying Outliers We will look for countries with prevalence rates that are significantly higher or lower than the global average for each year and by gender.\n\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nMale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.11%. The range of prevalence rates is quite broad, with the lowest around 0.03% and the highest at about 0.67%. The interquartile range (middle 50% of the data) spans from approximately 0.08% to 0.16%, indicating that half of the reported rates fall within this range.\nFemale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.21%, which is notably higher than that of males. The prevalence rates for females also show a broad range, from about 0.06% to 1.40%. The interquartile range for females is wider than for males, ranging from about 0.14% to 0.40%, reflecting greater variability in the rates reported for females.\nOutlier Identification:\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\n\n# For the heatmap focused on China, we will select all years of data for China only.\nchina_data = df_eating_disorder[df_eating_disorder['Country'] == 'China'].pivot('Year', 'Country Code', 'All_gender')\n\n# Heatmap: Visualizing the prevalence of eating disorders across years in China\nplt.figure(figsize=(10, 8))\nsns.heatmap(china_data, cmap=\"YlOrRd\", linewidths=.5, annot=True, fmt=\".2f\")\n\nplt.title('Heatmap of Eating Disorders Prevalence Across Years in China')\nplt.xlabel('Country Code')\nplt.ylabel('Year')\n\n# Show the heatmap\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1600456578.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  china_data = df_eating_disorder[df_eating_disorder['Country'] == 'China'].pivot('Year', 'Country Code', 'All_gender')\n\n\n\n\n\nheatmap\nThe heatmap above visualizes the prevalence of eating disorders across different years in China. Each cell in the heatmap corresponds to a specific year and shows the prevalence rate, with the color intensity reflecting the magnitude of that rate.​\ncreate a scatter plot to compare the male vs. female prevalence of eating disorders to see the correlation between genders. We’ll use data from the most recent year for all countries to see the relationship.​\n\n# Scatter Plot: Comparing male vs. female prevalence of eating disorders\n# Filter the dataset for the latest year\nscatter_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\nplt.figure(figsize=(10, 7))\n\n# Plotting the scatter plot for male vs. female prevalence\nsns.scatterplot(data=scatter_data, x='Eating_disorders_Male', y='Eating_disorders_Female', hue='Country', legend=False)\n\nplt.title(f'Correlation Between Male and Female Prevalence of Eating Disorders in {latest_year}')\nplt.xlabel('Male Prevalence')\nplt.ylabel('Female Prevalence')\n\n# Plot a 45 degree line to show y=x for reference\nmax_val = max(scatter_data['Eating_disorders_Male'].max(), scatter_data['Eating_disorders_Female'].max())\nplt.plot([0, max_val], [0, max_val], '--', color='gray')\n\n# Show the scatter plot\nplt.show()\n\n\n\n\nThe scatter plot above compares the prevalence of eating disorders between males (on the x-axis) and females (on the y-axis) for the most recent year available in the dataset. Each point represents a country, and the gray dashed line indicates where the prevalence would be equal for both genders. Points above the line show countries where the prevalence is higher in females compared to males, which seems to be the case for all countries shown.\n\n\nAge First Depression or Anxiety Data\n\n# Select columns to plot for Age First Depression or Anxiety Data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the style of seaborn\nsns.set_style(\"whitegrid\")\n\n# Bar Chart: Showing the percentage of individuals for each age category within each entity\nplt.figure(figsize=(10, 8))\nbar_chart = sns.barplot(x='Percentage', y='Entity', hue='Age', data=df_age_first_depression, ci=None)\nplt.title('Percentage of Individuals by Entity and Age Category for First Anxiety or Depression')\nplt.xlabel('Percentage')\nplt.ylabel('Entity')\nplt.legend(title='Age Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/626523418.py:10: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  bar_chart = sns.barplot(x='Percentage', y='Entity', hue='Age', data=df_age_first_depression, ci=None)\n\n\n\n\n\nCreate a pie chart showing the distribution of age categories for the onset of anxiety or depression for the “World” entity, I’ll first filter the dataset for the ‘World’ entity, and then plot the data.\n\n# Filter the data for the 'World' entity\nworld_data = df_age_first_depression[df_age_first_depression['Entity'] == 'World']\n\n# Pie Chart: Showing the distribution of age categories for the World entity\nplt.figure(figsize=(8, 8))\nplt.pie(world_data['Percentage'], labels=world_data['Age'], autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Age Categories for First Anxiety or Depression in the World')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n# Show the pie chart\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSince the data does not seem to have a time component or an inherent sequence, we’ll skip the line chart and instead create a heatmap to visualize the relationship between entities and age groups.\n\n# Create a pivot table for the heatmap\nheatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n# Heatmap: Visualize the percentage of individuals by entity and age category\nplt.figure(figsize=(12, 10))\nheatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Heatmap of Percentage by Entity and Age Category for First Anxiety or Depression')\nplt.xlabel('Age Category')\nplt.ylabel('Entity')\n\n# Show the heatmap\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/601293865.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  heatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n\n\n\n\nThe heatmap above visualizes the percentages by entity and age category for the first experience of anxiety or depression. The color intensity represents the magnitude of the percentages, with cooler colors indicating lower percentages and warmer colors indicating higher percentages.\nFinally create a stacked bar chart to represent the cumulative percentages of age categories for each entity.\ngrouping the data by Entity and visualizing the distribution of the Percentage across different Age brackets. This will help us understand if there are significant differences in the age of onset for anxiety or depression across different parts of the world and different income levels.​\n\n# Stacked Bar Chart: Create a crosstab to prepare data for the stacked plot\nstacked_data = pd.crosstab(index=df_age_first_depression['Entity'], columns=df_age_first_depression['Age'], values=df_age_first_depression['Percentage'], aggfunc='sum', normalize='index')\n\n# Plotting the stacked bar chart\nstacked_data.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='viridis')\nplt.title('Stacked Bar Chart of Age Categories for First Anxiety or Depression by Entity')\nplt.xlabel('Entity')\nplt.ylabel('Percentage')\nplt.legend(title='Age Category', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xticks(rotation=45, ha='right')\n\n# Show the stacked bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe bar chart and table display the distribution of the age of onset for anxiety or depression across various entities, which include continents, income categories, and the world overall. From the visualization and the data, we can observe the following:\nVariability in Age of Onset: There is considerable variability in the age of onset for anxiety or depression among different entities. For example, in Europe, a higher percentage reports the onset during the age brackets of “Ages 20-29” and “Ages ≥40”, whereas in Africa, the onset is more reported in the younger age bracket “Ages 13-19” and “Ages 20-29”.\nHigh-Income Countries: In high-income countries, the age bracket “Ages 13-19” has the highest reported onset, followed by “Ages ≥40” and “Ages 20-29”.\nLower-Middle-Income Countries: Here, the age bracket “Ages 20-29” has the highest reported onset, suggesting that the onset of anxiety or depression may occur later compared to high-income countries.\nDon’t Know/Refused: A significant portion of respondents across all entities did not provide a specific age bracket or refused to answer, with Africa having the highest percentage in this category. This could reflect cultural differences in discussing mental health or data collection challenges.\n\n\nGDP_mental_health_2017_Dataset\nHistograms for mental health indicators to see their distribution across the dataset.\n\n# Histograms for mental health indicators\nplt.figure(figsize=(15, 10))\n\n# List of mental health indicators to plot\nmental_health_indicators = ['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)', \n                            'Anxiety disorders (%)']\n\n# Plotting each histogram\nfor i, indicator in enumerate(mental_health_indicators, 1):\n    plt.subplot(2, 2, i)\n    sns.histplot(df_gdp_mental_health_2017[indicator], kde=False, bins=30)\n    plt.title(f'Distribution of {indicator}')\n    plt.xlabel(indicator)\n    plt.ylabel('Frequency')\n\n# Adjusting layout\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe histograms provide a distribution of each mental health indicator across the countries in the dataset for the year 2017\nscatter plots\nvisualize the potential correlation between GDP per capita and each mental health indicator.\n\n# Renaming the '2017' column to 'GDP_per_capita' for clarity\ndf_gdp_mental_health_2017.rename(columns={'2017': 'GDP_per_capita'}, inplace=True)\n\n# Ensuring 'GDP_per_capita' is a numeric column\ndf_gdp_mental_health_2017['GDP_per_capita'] = pd.to_numeric(df_gdp_mental_health_2017['GDP_per_capita'], errors='coerce')\n\n# List of mental health indicators to plot\nmental_health_indicators = ['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)', 'Anxiety disorders (%)']\n\n# Scatter plots of GDP per capita vs each mental health indicator\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plotting scatter plots\nfor i, indicator in enumerate(mental_health_indicators):\n    sns.scatterplot(ax=axes[i//2, i%2], \n                    data=df_gdp_mental_health_2017, \n                    x='GDP_per_capita', \n                    y=indicator)\n    axes[i//2, i%2].set_title(f'GDP per Capita vs {indicator}')\n    axes[i//2, i%2].set_xlabel('GDP per Capita')\n    axes[i//2, i%2].set_ylabel(indicator)\n\n# Adjusting layout\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatter plots have been generated, showing the relationship between GDP per capita and each mental health indicator.\nSchizophrenia (%) vs GDP per Capita: There does not appear to be a clear trend indicating that the prevalence of schizophrenia is related to GDP per capita. The distribution of points seems scattered without a distinct pattern.\nBipolar Disorder (%) vs GDP per Capita: Similar to schizophrenia, bipolar disorder prevalence does not show a clear correlation with GDP per capita based on the scatter plot.\nEating Disorders (%) vs GDP per Capita: For eating disorders, there might be a slight positive trend, suggesting that higher GDP per capita could be associated with a higher reported prevalence of eating disorders. This could be due to better reporting and diagnosis in higher-income countries, but further statistical analysis would be needed to confirm any correlation.\nAnxiety Disorders (%) vs GDP per Capita: The scatter plot does not reveal a strong correlation between the prevalence of anxiety disorders and GDP per capita. However, there is some spread in the data that might warrant a closer look with more sophisticated statistical tools.\nGeneral Observations: The prevalence rates for the mental health conditions do not show strong visual evidence of a correlation with GDP per capita. This might suggest that mental health issues are widespread and not necessarily directly related to the economic status of a country.\nIt is important to note that these are preliminary insights based solely on visual analysis. To draw more concrete conclusions, you would need to perform a quantitative analysis, such as calculating the correlation coefficients or conducting regression analysis. Additionally, it’s important to consider other factors that could influence mental health statistics, such as healthcare access, cultural attitudes towards mental health, and the quality of data reporting in different countries.\nHeatmap of the correlation matrix to understand how different indicators are related to each other. This will include GDP per capita and the mental health indicators.​\n\n# Correlation matrix heatmap\nplt.figure(figsize=(10, 8))\n\n# Calculate correlation matrix\ncorr_matrix = df_gdp_mental_health_2017.corr()\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n\n# Title\nplt.title('Correlation Matrix Heatmap')\n\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1859813442.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_matrix = df_gdp_mental_health_2017.corr()\n\n\n\n\n\nThe heatmap displays the correlation coefficients between the GDP per capita and mental health indicators. Values close to 1 or -1 indicate a strong positive or negative correlation, respectively, while values close to 0 indicate a weak or no correlation."
  },
  {
    "objectID": "HW-03/Code/EDA.html#hypothesis-generation",
    "href": "HW-03/Code/EDA.html#hypothesis-generation",
    "title": "Data Understanding",
    "section": "Hypothesis Generation",
    "text": "Hypothesis Generation\n\nAge First Depression or Anxiety Data\n\nThe cultural, economic, and social environments in high-income countries might contribute to an earlier age of onset for anxiety or depression.\nIn regions with lower-middle-income levels, the onset of anxiety or depression might occur later, potentially due to different life stressors or lower levels of mental health awareness and reporting.\n\n\n\nEating disorder Dataset\n\nThe prevalence of eating disorders is higher in countries with higher GDP per capita, which could reflect better diagnostic capabilities or different societal pressures.\nThere may be significant differences in the prevalence of eating disorders between genders across different countries, potentially reflecting cultural, social, or biological factors.\n\n\n\nGDP_mental_health_2017_Dataset\n\nHigher GDP per capita might be associated with higher reported rates of mental health disorders, potentially due to better health services and reporting mechanisms.\nThe prevalence of different mental health disorders will vary across economies, possibly due to genetic, environmental, cultural, or socioeconomic factors.\n\n\n\nMental Health Dataset\n\nEducational outcomes may be associated with the prevalence or reporting of certain mental health conditions, such as schizophrenia and eating disorders.\nThe lack of correlation between GDP and comfort in discussing mental health could imply that economic development does not directly influence cultural attitudes toward mental health openness.\n\n3.The correlations between different mental health conditions suggest the possibility of common underlying risk factors or increased comorbidity."
  },
  {
    "objectID": "HW-03/Code/EDA.html#identiy-outliers",
    "href": "HW-03/Code/EDA.html#identiy-outliers",
    "title": "Data Understanding",
    "section": "Identiy Outliers",
    "text": "Identiy Outliers\n\nAge First Depression or Anxiety Data\nThe data does not have individual country-level entries, so it’s not possible to identify outliers in the traditional sense. However, the “Don’t Know/Refused” category stands out and could be considered an anomaly in the reporting of mental health data.\n\n\nEating disorder Dataset\nWe will look for countries with prevalence rates that are significantly higher or lower than the global average for each year and by gender.\n\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\n\n\nGDP_mental_health_2017_Dataset\n\n\nMental Health Dataset"
  },
  {
    "objectID": "HW-03/Code/EDA.html#report-and-discuss-methods-and-findings",
    "href": "HW-03/Code/EDA.html#report-and-discuss-methods-and-findings",
    "title": "Data Understanding",
    "section": "Report and discuss methods and findings",
    "text": "Report and discuss methods and findings\n\nAge First Depression or Anxiety Data\nThe bar chart and table display the distribution of the age of onset for anxiety or depression across various entities, which include continents, income categories, and the world overall. From the visualization and the data, we can observe the following:\nVariability in Age of Onset: There is considerable variability in the age of onset for anxiety or depression among different entities. For example, in Europe, a higher percentage reports the onset during the age brackets of “Ages 20-29” and “Ages ≥40”, whereas in Africa, the onset is more reported in the younger age bracket “Ages 13-19” and “Ages 20-29”.\nHigh-Income Countries: In high-income countries, the age bracket “Ages 13-19” has the highest reported onset, followed by “Ages ≥40” and “Ages 20-29”.\nLower-Middle-Income Countries: Here, the age bracket “Ages 20-29” has the highest reported onset, suggesting that the onset of anxiety or depression may occur later compared to high-income countries.\nDon’t Know/Refused: A significant portion of respondents across all entities did not provide a specific age bracket or refused to answer, with Africa having the highest percentage in this category. This could reflect cultural differences in discussing mental health or data collection challenges.\n\n\nEating disorder Dataset\nThe prevalence of eating disorders is significantly higher in females than in males, which aligns with the general understanding of these conditions.\nThe variability in prevalence rates among females is greater, indicating that eating disorders in females may be more influenced by a variety of factors, possibly including cultural aspects, societal pressures, or biological predispositions.\nThe global average prevalence of eating disorders over time for males, females, and all genders combined is growing up.\n\n\nGDP_mental_health_2017_Dataset\nEating Disorders (%) vs GDP per Capita: For eating disorders, there might be a slight positive trend, suggesting that higher GDP per capita could be associated with a higher reported prevalence of eating disorders.\n\n\nMental Health Dataset\nwe can draw the conclusion that the Eating disorders and Anxiety disorders, Bipolar disorder (%) are not related to GDP.\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone."
  },
  {
    "objectID": "HW-03/Code/EDA.html#tools-and-software",
    "href": "HW-03/Code/EDA.html#tools-and-software",
    "title": "Data Understanding",
    "section": "Tools and Software",
    "text": "Tools and Software\n\nPandas: For data manipulation and analysis.\nMatplotlib: For creating the bar chart visualization.\nSeaborn: Although not used in the last plot, it is an excellent tool for creating heatmaps and other complex visualizations."
  },
  {
    "objectID": "HW-04/Code/clean.html",
    "href": "HW-04/Code/clean.html",
    "title": "Bella Shi's Website",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf_mental_health = pd.read_csv('../Data/mental_health.csv')\n#select rows when Year=2017\ndf_mental_health_2017 = df_mental_health[df_mental_health['Year']==2017]\n# print(df_mental_health_2017.head())\n\ndf_gdp_per_capita = pd.read_csv('../Data/GDP_percaptita_mental_health_2017.csv')\nprint(df_gdp_per_capita.head())\n\n# Merge the two DataFrames on the Economy column\nmerged_df = df_gdp_per_capita.merge(df_mental_health_2017[['Economy', 'average_learning_Adjusted_of_school', 'Income group']], on='Economy', how='left')\n\n# # Display the merged DataFrame\n# print(merged_df.head())\n\n# Save the DataFrame to a CSV file\nmerged_df.to_csv('../Data/merged_df.csv', index=False)\n\n       Economy       2017  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan    635.789           0.166158              0.708089   \n1      Albania   4525.887           0.201025              0.704480   \n2      Algeria   4014.707           0.197913              0.818687   \n3      Andorra  40017.741           0.263512              0.963331   \n4       Angola     4039.3           0.172794              0.623904   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \n0              0.107142               4.882481        4.136347  \n1              0.174046               3.385245        2.208414  \n2              0.213612               5.065876        3.661094  \n3              0.644559               5.305375        3.729532  \n4              0.173643               3.296906        4.160484"
  },
  {
    "objectID": "HW-04/HW-4.html",
    "href": "HW-04/HW-4.html",
    "title": "Homework-4",
    "section": "",
    "text": "During module-4, we will focus on partition clustering (k-means), hierarchical clustering (with dendrograms), and density clustering (DBSCAN), as well as dimensionality reduction. There are other clustering options as well and you can, and should, always do more than the baseline requirements!\nWe will also explore different distance (or similarity) measures, visualizations, methods for choosing the number of clusters (such as elbow and silhouette methods), and how to interpret the results of clustering analysis.\nBefore getting started, recall the following fundamental concepts:\n\nSupervised learning:\n\nIn supervised learning we try to learn some KNOWN mapping from an input feature space \\(\\mathbf X\\) to an output target space \\(Y\\) (i.e. regression or classification).\n\nUn-supervised learning:\n\nTypically when doing unsupervised learning (i.e. clustering or dimensionality reduction) you DO NOT know if any relevant mappings (relationships) in the data.\nIn this case, there is NO KNOWN “target” space \\(Y\\) containing the “ground truth” labels (targets).\nInstead, you have some features space \\(\\mathbf X\\) and you are trying to systematically figure out if the data in that space forms groups (clusters).\nThis “un-labeled” nature of the data-set is what makes it “unsupervised”.\nIt is possible to already have some categorical labels in your data before clustering. In this case, the clustering algorithm may or may-not find clusters corresponding to the existing labels.\nIf it finds new labels, this suggests that there may be unknown groups in the data which are NOT associated with the existing labels.\n\n\n\n\nSoftware: For this assignment you MUST use Python. However, you can, and should, also repeat the exercise in R if you want.\n\n\nThe project’s objective is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex, multimodal data while preserving essential information and enhancing data visualization.\nThis project will allow you to apply your knowledge of PCA and t-SNE to a real-world scenario and gain valuable experience in dimensionality reduction and data visualization. It’s an opportunity to showcase your skills and creativity in tackling complex data analysis challenges.\nInstructions:\n\nDimensionality Reduction with PCA:\n\nApply PCA to your record dataset.\nDetermine the optimal number of principal components to retain.\nVisualize the reduced-dimensional data using PCA.\nAnalyze and interpret the results.\n\nDimensionality Reduction with t-SNE:\n\nImplement t-SNE on the same dataset.\nExplore different perplexity values and their impact.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare t-SNE results with PCA results.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in terms of preserving data structure and information.\nCompare the visualization capabilities of PCA and t-SNE.\nDiscuss the trade-offs and scenarios where one technique may outperform the other.\n\n\nComponents:\n\nProject Proposal:\n\nA brief proposal outlining your project’s objectives, dataset selection, and the tools or libraries you plan to use (e.g., Python, scikit-learn).\n\nCode Implementation:\n\nPython code for implementing PCA and t-SNE on the selected dataset.\nCode should include parameter tuning for t-SNE (perplexity) and visualization of the results.\n\nProject Report:\n\nA comprehensive project report detailing the steps taken, results obtained, and your analysis.\nInclude visualizations, comparisons, and insights gained from the dimensionality reduction techniques.\n\n\n\n\n\nIn this homework you will choose appropriate portions of one or more of your datasets and apply k-means, DBSCAN, and Hierarchical clustering.\nYou can do this for either your record data OR text data, you can choose which data-set you prefer to work with. You can also do both.\nYou will also need to document the process on your clustering tab on your GU-domains website.\nIn the clustering tab please have the following sub-sections\n\nIntroduction:\n\nProvided brief summary (1 to 2 paragraphs) about your feature data \\(X\\), and what you are trying to achieve with your clustering analysis.\n\nTheory:\n\nWrite a brief technical write up about how EACH clustering method works (KMEAN, DBSAN, hierarchical clustering). Also include details on model selection methods that you use (elbow, silhouette, etc).\nDon’t go “too deep”, around 2 to 4 paragraphs, per method, is fine. Write it in a way that a boss with a non-technical background would understand. Describe the method from a “big picture” point of view, how it works and what it is supposed to do.\n\nDo this “in your own words”. DO NOT copy this section from any other source. If you do so you will receive a zero on the assignment and will be referred to the department for further disciplinary action.\n\nMethods:\n\nIn this section, describe your coding workflow;\n\n\nIf you are not using Quarto, then provide links to the relevant code in this section.\n\n\nIf you are using Quarto (recommended), then include your code in-line using the “code folding” option, so that users can toggle the code on/off as needed. The notebook should also be hosted on Github. You can always do the entire thing in .ipynb then render it to HTML using Quarto at the end.\n\n\nPlease use the following workflow\n\nData selection: If you have not done so already, create either a numeric record feature dataset \\(\\mathbf X\\) AND/OR a text feature dataset \\(\\mathbf X\\) from your existing data.\n\nRemove the labels (targets) \\(Y\\) as needed so that it is suitable for clustering.\nIf you have labels, you can use them at the end, to check if the clustering predictions coincided with the existing labels in the data-set. In general this may or may-not be true.\nBut these labels should not be used as part of the clustering analysis.\n\nFeature selection: (optional)\n\nIf you want, you can perform filter based feature selection on your data-set \\(\\mathbf X\\) as a pre-processing step before clustering\nYou can also use optimal feature sets obtained during previous assignments\n\nHyper-parameter tuning\n\nFor each of the three clustering algorithms, perform any relevant parameter tuning in an attempt to achieve the optimal clustering results\ne.g. For k-means, Use Elbow and Silhouette methods to illustrate the ideal number of clusters. Visualize your results.\nAlso, when relevant, explore different choices of distance metric for the algorithm. Which distance metric seems to works best in which cases and why?\n\nFinal results\n\nOnce you have everything “dialed in”, re-do the analysis one last time with the optimal parameter choice to get your “final results”.\n\n\n\nResults:\n\nUsing your “final results”, discuss, illustrate, and compare the results of your various clustering analysis methods.\nWhich method seemed to work the best and why, which was easier to use or preferable, etc.\nCan you make connections between the optimal cluster predictions, after parameter tuning, with any of the labels in the data set. Do they coincide? Why or why not?\nDid the clustering results provide any new insights into your data?\nExplore the results, and create as many meaningful visualizations as you need. Be creative, and experiment with different image aesthetics.\nEnsure all visualizations are professional, ascetically pleasing, labeled, captioned, use color, are clear, and support your discussion and goals.\n\nConclusions:\n\nIn this section, the goal is to summarize & wrap-up the report. It explains what was found, in a way that would make sense to a general readership.\nThis area is non-technical. Technical descriptions of what you did should be in the methods or results sections, not conclusions.\nThe Conclusions should focus on key and important findings and how these findings affect real-life and real people.\n\nReferences:\n\nReference all non-original content.\nIdeally (but optionally) use .bibtex combined with Quarto to provide in-line internal citations\nSee the following link for an example:\n\nhttps://drive.google.com/open?id=12tYQnDuHS4ZxSTXgwsR4pRLhBYAMy14q&authuser=jh2343%40georgetown.edu&usp=drive_fs"
  },
  {
    "objectID": "HW-04/HW-4.html#assignment",
    "href": "HW-04/HW-4.html#assignment",
    "title": "Homework-4",
    "section": "",
    "text": "Software: For this assignment you MUST use Python. However, you can, and should, also repeat the exercise in R if you want.\n\n\nThe project’s objective is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex, multimodal data while preserving essential information and enhancing data visualization.\nThis project will allow you to apply your knowledge of PCA and t-SNE to a real-world scenario and gain valuable experience in dimensionality reduction and data visualization. It’s an opportunity to showcase your skills and creativity in tackling complex data analysis challenges.\nInstructions:\n\nDimensionality Reduction with PCA:\n\nApply PCA to your record dataset.\nDetermine the optimal number of principal components to retain.\nVisualize the reduced-dimensional data using PCA.\nAnalyze and interpret the results.\n\nDimensionality Reduction with t-SNE:\n\nImplement t-SNE on the same dataset.\nExplore different perplexity values and their impact.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare t-SNE results with PCA results.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in terms of preserving data structure and information.\nCompare the visualization capabilities of PCA and t-SNE.\nDiscuss the trade-offs and scenarios where one technique may outperform the other.\n\n\nComponents:\n\nProject Proposal:\n\nA brief proposal outlining your project’s objectives, dataset selection, and the tools or libraries you plan to use (e.g., Python, scikit-learn).\n\nCode Implementation:\n\nPython code for implementing PCA and t-SNE on the selected dataset.\nCode should include parameter tuning for t-SNE (perplexity) and visualization of the results.\n\nProject Report:\n\nA comprehensive project report detailing the steps taken, results obtained, and your analysis.\nInclude visualizations, comparisons, and insights gained from the dimensionality reduction techniques.\n\n\n\n\n\nIn this homework you will choose appropriate portions of one or more of your datasets and apply k-means, DBSCAN, and Hierarchical clustering.\nYou can do this for either your record data OR text data, you can choose which data-set you prefer to work with. You can also do both.\nYou will also need to document the process on your clustering tab on your GU-domains website.\nIn the clustering tab please have the following sub-sections\n\nIntroduction:\n\nProvided brief summary (1 to 2 paragraphs) about your feature data \\(X\\), and what you are trying to achieve with your clustering analysis.\n\nTheory:\n\nWrite a brief technical write up about how EACH clustering method works (KMEAN, DBSAN, hierarchical clustering). Also include details on model selection methods that you use (elbow, silhouette, etc).\nDon’t go “too deep”, around 2 to 4 paragraphs, per method, is fine. Write it in a way that a boss with a non-technical background would understand. Describe the method from a “big picture” point of view, how it works and what it is supposed to do.\n\nDo this “in your own words”. DO NOT copy this section from any other source. If you do so you will receive a zero on the assignment and will be referred to the department for further disciplinary action.\n\nMethods:\n\nIn this section, describe your coding workflow;\n\n\nIf you are not using Quarto, then provide links to the relevant code in this section.\n\n\nIf you are using Quarto (recommended), then include your code in-line using the “code folding” option, so that users can toggle the code on/off as needed. The notebook should also be hosted on Github. You can always do the entire thing in .ipynb then render it to HTML using Quarto at the end.\n\n\nPlease use the following workflow\n\nData selection: If you have not done so already, create either a numeric record feature dataset \\(\\mathbf X\\) AND/OR a text feature dataset \\(\\mathbf X\\) from your existing data.\n\nRemove the labels (targets) \\(Y\\) as needed so that it is suitable for clustering.\nIf you have labels, you can use them at the end, to check if the clustering predictions coincided with the existing labels in the data-set. In general this may or may-not be true.\nBut these labels should not be used as part of the clustering analysis.\n\nFeature selection: (optional)\n\nIf you want, you can perform filter based feature selection on your data-set \\(\\mathbf X\\) as a pre-processing step before clustering\nYou can also use optimal feature sets obtained during previous assignments\n\nHyper-parameter tuning\n\nFor each of the three clustering algorithms, perform any relevant parameter tuning in an attempt to achieve the optimal clustering results\ne.g. For k-means, Use Elbow and Silhouette methods to illustrate the ideal number of clusters. Visualize your results.\nAlso, when relevant, explore different choices of distance metric for the algorithm. Which distance metric seems to works best in which cases and why?\n\nFinal results\n\nOnce you have everything “dialed in”, re-do the analysis one last time with the optimal parameter choice to get your “final results”.\n\n\n\nResults:\n\nUsing your “final results”, discuss, illustrate, and compare the results of your various clustering analysis methods.\nWhich method seemed to work the best and why, which was easier to use or preferable, etc.\nCan you make connections between the optimal cluster predictions, after parameter tuning, with any of the labels in the data set. Do they coincide? Why or why not?\nDid the clustering results provide any new insights into your data?\nExplore the results, and create as many meaningful visualizations as you need. Be creative, and experiment with different image aesthetics.\nEnsure all visualizations are professional, ascetically pleasing, labeled, captioned, use color, are clear, and support your discussion and goals.\n\nConclusions:\n\nIn this section, the goal is to summarize & wrap-up the report. It explains what was found, in a way that would make sense to a general readership.\nThis area is non-technical. Technical descriptions of what you did should be in the methods or results sections, not conclusions.\nThe Conclusions should focus on key and important findings and how these findings affect real-life and real people.\n\nReferences:\n\nReference all non-original content.\nIdeally (but optionally) use .bibtex combined with Quarto to provide in-line internal citations\nSee the following link for an example:\n\nhttps://drive.google.com/open?id=12tYQnDuHS4ZxSTXgwsR4pRLhBYAMy14q&authuser=jh2343%40georgetown.edu&usp=drive_fs"
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html",
    "href": "HW-03/Code/ Naïve Bayes.html",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a probabilistic machine learning model that’s based on Bayes’ Theorem. It’s called “naive” because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets."
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html#hw-3.2.1-prepare-your-data-for-naïve-bayes",
    "href": "HW-03/Code/ Naïve Bayes.html#hw-3.2.1-prepare-your-data-for-naïve-bayes",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "HW-3.2.1: Prepare your Data for Naïve Bayes",
    "text": "HW-3.2.1: Prepare your Data for Naïve Bayes\n1.Handle missing values\n2.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models.\n3.Split the data into training and test sets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\ndf_mental_health=pd.read_csv('../Data/mental_health.csv')\n\nCheck missing values in the dataset\n\nmissing_data=df_mental_health.isnull().sum()\nprint(missing_data)\n\nEconomy                                                             0\nCode                                                                0\nYear                                                                0\nSchizophrenia (%)                                                   0\nBipolar disorder (%)                                                0\nEating disorders (%)                                                0\nAnxiety disorders (%)                                               0\nDepression (%)                                                      0\nIncome group                                                       56\naverage_learning_Adjusted_of_school                               812\nContinent                                                         588\nGDP(2022)                                                         224\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent    2380\ndtype: int64\n\n\n\n1.Handle missing values\na.Dropping columns with more than 1000 null values\n(We don’t Remove Rows with Missing Values because although this is the simplest approach, but it can lead to a significant reduction in data size, which might not be ideal if the missing data is extensive.)\nso after this step, we dropped the column not_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\nthreshold = 1000  # Set the threshold for the minimum number of non-NA values required\ndf_mental_health = df_mental_health.dropna(axis=1, thresh=(len(df_mental_health) - threshold))\n# Display the columns remaining after the operation\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\nNr0=len(df_mental_health.index)\nNc0=len(df_mental_health.columns)\nprint(\"Nrows = \",Nr0,\"\\nNcol=\",Nc0,\"\\nMatrix entries = \", Nr0*Nc0)\n# missing_data1 = encoded_data1.isnull().sum()\n# print(missing_data1)\n\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school    812\nContinent                              588\nGDP(2022)                              224\ndtype: int64\nNrows =  5488 \nNcol= 12 \nMatrix entries =  65856\n\n\nb.Impute Missing Values:\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is a common strategy for numerical data.\nForward or Backward Fill: For time-series data, you can fill missing values with the next or previous values.\nPredictive Imputation: Use a machine learning algorithm to predict the missing values based on other data in the dataset.\nMean/Median Imputation: If the data is normally distributed, you might use the mean. If the distribution is skewed,the median might be more appropriate.\n\nimport seaborn as sns\n#plot the GDP(2022) in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['GDP(2022)'])\n#plot the average_learning_Adjusted_of_school in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_54217/2106952041.py:4: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['GDP(2022)'])\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_54217/2106952041.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n\n\n&lt;Axes: xlabel='average_learning_Adjusted_of_school', ylabel='Density'&gt;\n\n\n\n\n\n\n\n\nso here we impute missing values using the mean for GDP(2022) and the median for average_learning_Adjusted_of_school (assuming the distributions are appropriate for these measures):\n\n# Impute missing values with the mean for GDP(2022)\ndf_mental_health['GDP(2022)'].fillna(df_mental_health['GDP(2022)'].mean(), inplace=True)\n\n# Impute missing values with the median for average_learning_Adjusted_of_school\ndf_mental_health['average_learning_Adjusted_of_school'].fillna(df_mental_health['average_learning_Adjusted_of_school'].median(), inplace=True)\n\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school      0\nContinent                              588\nGDP(2022)                                0\ndtype: int64\n\n\n\nDrop Rows (‘Income group’) with Missing Values:\n\nThe ‘Income group’ and ‘Continent’ columns still have missing values. Before we proceed with the Naive Bayes classification, we need to address these missing values. Since ‘Income group’ will be our target variable for classification, we cannot impute it as we did with the continuous variables. Instead, we should remove the rows with missing ‘Income group’ labels.\nAs for the ‘Continent’ column, it might not be necessary for our classification model if we’re using economic and health indicators as features.(because we can just dont choose the feature in the feature selection). However, if we decide that continent information might be useful, we could either impute the missing values based on the ‘Economy’ column or drop the column if it’s not required.\n\n# Dropping rows where 'Income group' is missing since it's our target variable\ndf_mental_health = df_mental_health.dropna(subset=['Income group'])\n\n# # Dropping the 'Continent' column\n# data_cleaned = df_mental_health.drop('Continent', axis=1)\n\n# Verifying that there are no more missing values\nmissing_values_final = df_mental_health.isnull().sum()\nmissing_values_final, df_mental_health.shape\n\n(Economy                                  0\n Code                                     0\n Year                                     0\n Schizophrenia (%)                        0\n Bipolar disorder (%)                     0\n Eating disorders (%)                     0\n Anxiety disorders (%)                    0\n Depression (%)                           0\n Income group                             0\n average_learning_Adjusted_of_school      0\n Continent                              560\n GDP(2022)                                0\n dtype: int64,\n (5432, 12))\n\n\nThe dataset is now clean and ready for classification. We can proceed with the next step.\n\n# Save the cleaned dataframe to a new CSV file\ndf_mental_health.to_csv('./mental_health_cleaned_not_encoded.csv', index=False)\n\n3.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models and Split the data into training and test sets\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the cleaned dataset\nfile_path = './mental_health_cleaned_not_encoded.csv'\ndata = pd.read_csv(file_path)\n\n# Encoding the 'Income group' column to have numerical labels\nlabel_encoder = LabelEncoder()\ndata['Income group'] = label_encoder.fit_transform(data['Income group'])\n\n# Selecting features and target variable\nX = data.drop(['Economy', 'Code', 'Income group', 'Year', 'Continent'], axis=1)\ny = data['Income group']\n\n# Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Checking the shape of the datasets\n(X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)\n\n(((3802, 7), (815, 7), (815, 7)), ((3802,), (815,), (815,)))"
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html#hw-3.2.2-feature-selection",
    "href": "HW-03/Code/ Naïve Bayes.html#hw-3.2.2-feature-selection",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "HW-3.2.2: Feature selection",
    "text": "HW-3.2.2: Feature selection\nDecide which features are relevant for our classification task(Naive Bayes model) and drop the rest.\nThe ‘Income group’ is a categorical variable that could be used as a label for classification, so we predict the ‘Income group’ based on health and economic indicators.\nmethod: 1. EDA to find the correlation between the features and the target variable (corralation matrix)\n\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    # 'Income group',\n    # 'Continent',\n\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\nprint(encoded_data.head())\nencoded_data.to_csv('./mental_health_encoded.csv', index=False)\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\nprint(encoded_data.head())\nprint(type(encoded_data))\nprint(new_correlation_matrix)\n\n\n\n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \\\n0              0.101855               4.828830        4.071831   \n1              0.099313               4.829740        4.079531   \n2              0.096692               4.831108        4.088358   \n3              0.094336               4.830864        4.096190   \n4              0.092439               4.829423        4.099582   \n\n   average_learning_Adjusted_of_school  GDP(2022)  Income group_High income  \\\n0                             4.957542    14583.0                         0   \n1                             4.957542    14583.0                         0   \n2                             4.957542    14583.0                         0   \n3                             4.957542    14583.0                         0   \n4                             4.957542    14583.0                         0   \n\n   Income group_Low income  Income group_Lower middle income  \\\n0                        1                                 0   \n1                        1                                 0   \n2                        1                                 0   \n3                        1                                 0   \n4                        1                                 0   \n\n   Income group_Upper middle income  Continent_Africa  Continent_Asia  \\\n0                                 0                 0               1   \n1                                 0                 0               1   \n2                                 0                 0               1   \n3                                 0                 0               1   \n4                                 0                 0               1   \n\n   Continent_Europe  Continent_North America  Continent_Oceania  \\\n0                 0                        0                  0   \n1                 0                        0                  0   \n2                 0                        0                  0   \n3                 0                        0                  0   \n4                 0                        0                  0   \n\n   Continent_South America  \n0                        0  \n1                        0  \n2                        0  \n3                        0  \n4                        0  \n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \\\n0              0.101855               4.828830        4.071831   \n1              0.099313               4.829740        4.079531   \n2              0.096692               4.831108        4.088358   \n3              0.094336               4.830864        4.096190   \n4              0.092439               4.829423        4.099582   \n\n   average_learning_Adjusted_of_school  GDP(2022)  Income group_High income  \\\n0                             4.957542    14583.0                         0   \n1                             4.957542    14583.0                         0   \n2                             4.957542    14583.0                         0   \n3                             4.957542    14583.0                         0   \n4                             4.957542    14583.0                         0   \n\n   Income group_Low income  Income group_Lower middle income  \\\n0                        1                                 0   \n1                        1                                 0   \n2                        1                                 0   \n3                        1                                 0   \n4                        1                                 0   \n\n   Income group_Upper middle income  Continent_Africa  Continent_Asia  \\\n0                                 0                 0               1   \n1                                 0                 0               1   \n2                                 0                 0               1   \n3                                 0                 0               1   \n4                                 0                 0               1   \n\n   Continent_Europe  Continent_North America  Continent_Oceania  \\\n0                 0                        0                  0   \n1                 0                        0                  0   \n2                 0                        0                  0   \n3                 0                        0                  0   \n4                 0                        0                  0   \n\n   Continent_South America  \n0                        0  \n1                        0  \n2                        0  \n3                        0  \n4                        0  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\n                                     Schizophrenia (%)  Bipolar disorder (%)  \\\nSchizophrenia (%)                             1.000000              0.240575   \nBipolar disorder (%)                          0.240575              1.000000   \nEating disorders (%)                          0.688793              0.704006   \nAnxiety disorders (%)                         0.472522              0.658057   \nDepression (%)                                0.125692              0.114153   \nGDP(2022)                                     0.369815             -0.059046   \naverage_learning_Adjusted_of_school           0.636316              0.429033   \nIncome group_High income                      0.585802              0.442182   \nIncome group_Low income                      -0.405724             -0.216036   \nIncome group_Lower middle income             -0.221720             -0.324419   \nIncome group_Upper middle income             -0.083432              0.027109   \nContinent_Africa                             -0.524780             -0.269195   \nContinent_Asia                                0.138752             -0.133996   \nContinent_Europe                              0.279973              0.342498   \nContinent_North America                      -0.039523              0.275602   \nContinent_Oceania                             0.209432             -0.190432   \nContinent_South America                      -0.072790              0.226848   \n\n                                     Eating disorders (%)  \\\nSchizophrenia (%)                                0.688793   \nBipolar disorder (%)                             0.704006   \nEating disorders (%)                             1.000000   \nAnxiety disorders (%)                            0.679320   \nDepression (%)                                   0.195924   \nGDP(2022)                                        0.131819   \naverage_learning_Adjusted_of_school              0.648532   \nIncome group_High income                         0.739056   \nIncome group_Low income                         -0.333568   \nIncome group_Lower middle income                -0.397374   \nIncome group_Upper middle income                -0.123462   \nContinent_Africa                                -0.393812   \nContinent_Asia                                  -0.122490   \nContinent_Europe                                 0.425994   \nContinent_North America                          0.074215   \nContinent_Oceania                                0.000394   \nContinent_South America                          0.085132   \n\n                                     Anxiety disorders (%)  Depression (%)  \\\nSchizophrenia (%)                                 0.472522        0.125692   \nBipolar disorder (%)                              0.658057        0.114153   \nEating disorders (%)                              0.679320        0.195924   \nAnxiety disorders (%)                             1.000000        0.338573   \nDepression (%)                                    0.338573        1.000000   \nGDP(2022)                                         0.085053        0.052842   \naverage_learning_Adjusted_of_school               0.353867       -0.013123   \nIncome group_High income                          0.459848        0.100652   \nIncome group_Low income                          -0.156747        0.191008   \nIncome group_Lower middle income                 -0.250900        0.015965   \nIncome group_Upper middle income                 -0.112419       -0.271149   \nContinent_Africa                                 -0.264615        0.271530   \nContinent_Asia                                   -0.049442       -0.024714   \nContinent_Europe                                  0.187011        0.001411   \nContinent_North America                           0.029437       -0.282403   \nContinent_Oceania                                 0.030380       -0.030521   \nContinent_South America                           0.229374       -0.056769   \n\n                                     GDP(2022)  \\\nSchizophrenia (%)                     0.369815   \nBipolar disorder (%)                 -0.059046   \nEating disorders (%)                  0.131819   \nAnxiety disorders (%)                 0.085053   \nDepression (%)                        0.052842   \nGDP(2022)                             1.000000   \naverage_learning_Adjusted_of_school   0.223242   \nIncome group_High income              0.083878   \nIncome group_Low income              -0.086646   \nIncome group_Lower middle income     -0.105537   \nIncome group_Upper middle income      0.085280   \nContinent_Africa                     -0.134962   \nContinent_Asia                        0.179471   \nContinent_Europe                      0.042354   \nContinent_North America              -0.042112   \nContinent_Oceania                    -0.032099   \nContinent_South America              -0.009233   \n\n                                     average_learning_Adjusted_of_school  \\\nSchizophrenia (%)                                               0.636316   \nBipolar disorder (%)                                            0.429033   \nEating disorders (%)                                            0.648532   \nAnxiety disorders (%)                                           0.353867   \nDepression (%)                                                 -0.013123   \nGDP(2022)                                                       0.223242   \naverage_learning_Adjusted_of_school                             1.000000   \nIncome group_High income                                        0.640737   \nIncome group_Low income                                        -0.504080   \nIncome group_Lower middle income                               -0.325065   \nIncome group_Upper middle income                                0.039467   \nContinent_Africa                                               -0.570805   \nContinent_Asia                                                  0.061933   \nContinent_Europe                                                0.560514   \nContinent_North America                                        -0.001007   \nContinent_Oceania                                              -0.055448   \nContinent_South America                                         0.031498   \n\n                                     Income group_High income  \\\nSchizophrenia (%)                                    0.585802   \nBipolar disorder (%)                                 0.442182   \nEating disorders (%)                                 0.739056   \nAnxiety disorders (%)                                0.459848   \nDepression (%)                                       0.100652   \nGDP(2022)                                            0.083878   \naverage_learning_Adjusted_of_school                  0.640737   \nIncome group_High income                             1.000000   \nIncome group_Low income                             -0.272814   \nIncome group_Lower middle income                    -0.430693   \nIncome group_Upper middle income                    -0.414144   \nContinent_Africa                                    -0.389138   \nContinent_Asia                                      -0.073196   \nContinent_Europe                                     0.437275   \nContinent_North America                             -0.029037   \nContinent_Oceania                                   -0.062101   \nContinent_South America                             -0.027233   \n\n                                     Income group_Low income  \\\nSchizophrenia (%)                                  -0.405724   \nBipolar disorder (%)                               -0.216036   \nEating disorders (%)                               -0.333568   \nAnxiety disorders (%)                              -0.156747   \nDepression (%)                                      0.191008   \nGDP(2022)                                          -0.086646   \naverage_learning_Adjusted_of_school                -0.504080   \nIncome group_High income                           -0.272814   \nIncome group_Low income                             1.000000   \nIncome group_Lower middle income                   -0.244323   \nIncome group_Upper middle income                   -0.234936   \nContinent_Africa                                    0.452508   \nContinent_Asia                                     -0.084071   \nContinent_Europe                                   -0.206793   \nContinent_North America                            -0.137062   \nContinent_Oceania                                  -0.091711   \nContinent_South America                            -0.096450   \n\n                                     Income group_Lower middle income  \\\nSchizophrenia (%)                                           -0.221720   \nBipolar disorder (%)                                        -0.324419   \nEating disorders (%)                                        -0.397374   \nAnxiety disorders (%)                                       -0.250900   \nDepression (%)                                               0.015965   \nGDP(2022)                                                   -0.105537   \naverage_learning_Adjusted_of_school                         -0.325065   \nIncome group_High income                                    -0.430693   \nIncome group_Low income                                     -0.244323   \nIncome group_Lower middle income                             1.000000   \nIncome group_Upper middle income                            -0.370894   \nContinent_Africa                                             0.230024   \nContinent_Asia                                               0.176326   \nContinent_Europe                                            -0.298539   \nContinent_North America                                     -0.105328   \nContinent_Oceania                                            0.115292   \nContinent_South America                                     -0.102536   \n\n                                     Income group_Upper middle income  \\\nSchizophrenia (%)                                           -0.083432   \nBipolar disorder (%)                                         0.027109   \nEating disorders (%)                                        -0.123462   \nAnxiety disorders (%)                                       -0.112419   \nDepression (%)                                              -0.271149   \nGDP(2022)                                                    0.085280   \naverage_learning_Adjusted_of_school                          0.039467   \nIncome group_High income                                    -0.414144   \nIncome group_Low income                                     -0.234936   \nIncome group_Lower middle income                            -0.370894   \nIncome group_Upper middle income                             1.000000   \nContinent_Africa                                            -0.170437   \nContinent_Asia                                              -0.036598   \nContinent_Europe                                            -0.001172   \nContinent_North America                                      0.244202   \nContinent_Oceania                                            0.019655   \nContinent_South America                                      0.208010   \n\n                                     Continent_Africa  Continent_Asia  \\\nSchizophrenia (%)                           -0.524780        0.138752   \nBipolar disorder (%)                        -0.269195       -0.133996   \nEating disorders (%)                        -0.393812       -0.122490   \nAnxiety disorders (%)                       -0.264615       -0.049442   \nDepression (%)                               0.271530       -0.024714   \nGDP(2022)                                   -0.134962        0.179471   \naverage_learning_Adjusted_of_school         -0.570805        0.061933   \nIncome group_High income                    -0.389138       -0.073196   \nIncome group_Low income                      0.452508       -0.084071   \nIncome group_Lower middle income             0.230024        0.176326   \nIncome group_Upper middle income            -0.170437       -0.036598   \nContinent_Africa                             1.000000       -0.299560   \nContinent_Asia                              -0.299560        1.000000   \nContinent_Europe                            -0.313921       -0.263675   \nContinent_North America                     -0.208067       -0.174764   \nContinent_Oceania                           -0.139222       -0.116939   \nContinent_South America                     -0.146416       -0.122981   \n\n                                     Continent_Europe  \\\nSchizophrenia (%)                            0.279973   \nBipolar disorder (%)                         0.342498   \nEating disorders (%)                         0.425994   \nAnxiety disorders (%)                        0.187011   \nDepression (%)                               0.001411   \nGDP(2022)                                    0.042354   \naverage_learning_Adjusted_of_school          0.560514   \nIncome group_High income                     0.437275   \nIncome group_Low income                     -0.206793   \nIncome group_Lower middle income            -0.298539   \nIncome group_Upper middle income            -0.001172   \nContinent_Africa                            -0.313921   \nContinent_Asia                              -0.263675   \nContinent_Europe                             1.000000   \nContinent_North America                     -0.183143   \nContinent_Oceania                           -0.122545   \nContinent_South America                     -0.128876   \n\n                                     Continent_North America  \\\nSchizophrenia (%)                                  -0.039523   \nBipolar disorder (%)                                0.275602   \nEating disorders (%)                                0.074215   \nAnxiety disorders (%)                               0.029437   \nDepression (%)                                     -0.282403   \nGDP(2022)                                          -0.042112   \naverage_learning_Adjusted_of_school                -0.001007   \nIncome group_High income                           -0.029037   \nIncome group_Low income                            -0.137062   \nIncome group_Lower middle income                   -0.105328   \nIncome group_Upper middle income                    0.244202   \nContinent_Africa                                   -0.208067   \nContinent_Asia                                     -0.174764   \nContinent_Europe                                   -0.183143   \nContinent_North America                             1.000000   \nContinent_Oceania                                  -0.081223   \nContinent_South America                            -0.085420   \n\n                                     Continent_Oceania  \\\nSchizophrenia (%)                             0.209432   \nBipolar disorder (%)                         -0.190432   \nEating disorders (%)                          0.000394   \nAnxiety disorders (%)                         0.030380   \nDepression (%)                               -0.030521   \nGDP(2022)                                    -0.032099   \naverage_learning_Adjusted_of_school          -0.055448   \nIncome group_High income                     -0.062101   \nIncome group_Low income                      -0.091711   \nIncome group_Lower middle income              0.115292   \nIncome group_Upper middle income              0.019655   \nContinent_Africa                             -0.139222   \nContinent_Asia                               -0.116939   \nContinent_Europe                             -0.122545   \nContinent_North America                      -0.081223   \nContinent_Oceania                             1.000000   \nContinent_South America                      -0.057156   \n\n                                     Continent_South America  \nSchizophrenia (%)                                  -0.072790  \nBipolar disorder (%)                                0.226848  \nEating disorders (%)                                0.085132  \nAnxiety disorders (%)                               0.229374  \nDepression (%)                                     -0.056769  \nGDP(2022)                                          -0.009233  \naverage_learning_Adjusted_of_school                 0.031498  \nIncome group_High income                           -0.027233  \nIncome group_Low income                            -0.096450  \nIncome group_Lower middle income                   -0.102536  \nIncome group_Upper middle income                    0.208010  \nContinent_Africa                                   -0.146416  \nContinent_Asia                                     -0.122981  \nContinent_Europe                                   -0.128876  \nContinent_North America                            -0.085420  \nContinent_Oceania                                  -0.057156  \nContinent_South America                             1.000000  \n\n\n\n\n\nStrong Correlation: Values above 0.7 or below -0.7 suggest a strong correlation between the variables. Moderate Correlation: Values between 0.3 and 0.7 (or -0.3 and -0.7) suggest a moderate correlation. Weak Correlation: Values between 0 and 0.3 (or 0 and -0.3) suggest a weak correlation.\n\nPearson Correlation\n\nThe Pearson correlation coefficients you’ve provided show the linear relationship between each feature and the ‘Income group’ target variable. A coefficient close to 1 or -1 indicates a strong positive or negative linear relationship, respectively. A coefficient close to 0 suggests no linear relationship.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming data is already loaded and appropriate for Pearson Correlation\n# For Pearson Correlation, we would ideally have only numeric features and labels\n\n# Compute the Pearson Correlation matrix\ncorrelation_matrix = data.corr()\n# print(correlation_matrix)\n\n# Selecting only correlations with the target variable (assuming target is numeric)\ntarget_correlation = correlation_matrix['Income group'].sort_values(ascending=False)\n\n# Review the correlations and decide on a threshold for selecting features\nprint(target_correlation)\n\n# For multicollinearity, we could remove features that have a high correlation with other features\nthreshold = 0.8  # This is an example threshold, it may need to be adjusted\nhigh_correlation_pairs = []\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i+1, len(correlation_matrix.columns)):\n        if abs(correlation_matrix.iloc[i, j]) &gt; threshold:\n            high_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n\n# Now, you can decide which features to drop based on their correlation with the target\n# and their inter-correlations with other features\n\nIncome group                           1.000000e+00\nYear                                   1.892221e-14\nGDP(2022)                             -9.632665e-03\nDepression (%)                        -2.334807e-01\nBipolar disorder (%)                  -2.752850e-01\naverage_learning_Adjusted_of_school   -3.443070e-01\nAnxiety disorders (%)                 -3.575221e-01\nSchizophrenia (%)                     -3.745985e-01\nEating disorders (%)                  -5.301981e-01\nName: Income group, dtype: float64\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_54217/2072872300.py:8: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()\n\n\nEating disorders (%): With a correlation coefficient of approximately -0.52, this feature has the strongest negative linear relationship with the ‘Income group’ and is likely to be the most informative for predicting income group.\nAnxiety disorders (%), Schizophrenia (%), average_learning_Adjusted_of_school, Bipolar disorder (%), and Depression (%): All of these features have moderate negative correlations with the ‘Income group’, meaning as these percentages increase, the likelihood of being in a higher income group decreases.\nGDP(2022): This feature has a very weak negative correlation with the ‘Income group’, suggesting it might not be very useful for predicting the income group on its own.\nYear: The correlation is effectively zero, indicating no linear relationship with the ‘Income group’.\n3.Chisquare test\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n# Import the necessary libraries\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Initialize the SelectKBest with chi-squared score function\n# Using 'k='all'' to select all features for now, can change to a specific number based on the chi-squared scores\nkbest = SelectKBest(score_func=chi2, k='all')\n\n# Fit the SelectKBest to the training data and transform it\nX_train_kbest = kbest.fit_transform(X_train, y_train)\n\n# Transform the validation and test data\nX_val_kbest = kbest.transform(X_val)\nX_test_kbest = kbest.transform(X_test)\n\n# Get the scores for each feature\nfeature_scores = kbest.scores_\n\n# Create a DataFrame to view the scores and columns\nfeature_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Score': feature_scores})\n\n# Print the feature scores\nprint(feature_scores_df.sort_values(by='Score', ascending=False))\n\n                               Feature         Score\n6                            GDP(2022)  4.832444e+08\n5  average_learning_Adjusted_of_school  1.590596e+03\n3                Anxiety disorders (%)  2.765138e+02\n2                 Eating disorders (%)  2.371816e+02\n4                       Depression (%)  5.058270e+01\n1                 Bipolar disorder (%)  3.730221e+01\n0                    Schizophrenia (%)  1.404252e+01\n\n\nIn this example, features excluding ‘Year’ and ‘GDP(2022)’ are used for classification. The ‘Year’ column is dropped because it’s not relevant for classification, and ‘GDP(2022)’ is dropped because it has a very weak correlation with the ‘Income group’ and is unlikely to be useful for classification.\n\nHW-3.2.3: Naïve Bayes (NB) with Labeled Record Data\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Selecting features and target variable\n# print(X.columns)\nX = X.drop(['GDP(2022)', 'average_learning_Adjusted_of_school'], axis=1)\n\n# Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# # Checking the shape of the datasets\n(X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)\n\n\n# Train the Naïve Bayes model\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Validate the model\ny_val_pred = nb_classifier.predict(X_val)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nval_classification_report = classification_report(y_val, y_val_pred)\nval_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n\n# Visualize the confusion matrix\nsns.heatmap(val_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Validation Set')\nplt.show()\n\n# Evaluate the model using the validation set\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(f\"Classification Report for Validation Set:\\n{val_classification_report}\")\n\n\n\n\nValidation Accuracy: 0.5901840490797546\nClassification Report for Validation Set:\n              precision    recall  f1-score   support\n\n           0       0.84      0.58      0.69       272\n           1       0.60      0.83      0.70        94\n           2       0.57      0.58      0.58       227\n           3       0.42      0.50      0.46       222\n\n    accuracy                           0.59       815\n   macro avg       0.61      0.63      0.61       815\nweighted avg       0.62      0.59      0.60       815\n\n\n\n\n# Test the model\ny_test_pred = nb_classifier.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_classification_report = classification_report(y_test, y_test_pred)\ntest_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n\n# Visualize the confusion matrix for the test set\nsns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n# Evaluate the model using the test set\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Classification Report for Test Set:\\n{test_classification_report}\")\n\n\n\n\nTest Accuracy: 0.6024539877300613\nClassification Report for Test Set:\n              precision    recall  f1-score   support\n\n           0       0.84      0.63      0.72       281\n           1       0.62      0.84      0.71       102\n           2       0.54      0.59      0.56       219\n           3       0.44      0.46      0.45       213\n\n    accuracy                           0.60       815\n   macro avg       0.61      0.63      0.61       815\nweighted avg       0.63      0.60      0.61       815"
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html#hw-3.2.4-naïve-bayes-nb-with-labeled-text-data",
    "href": "HW-03/Code/ Naïve Bayes.html#hw-3.2.4-naïve-bayes-nb-with-labeled-text-data",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data",
    "text": "HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data\nLabel the data, either manually or through automated methods, based on your classification goals. Preprocess the text data by cleaning and tokenizing the text, normalizing it, and removing stop words. Extract features using methods like TF-IDF or count vectorization. Train your Naive Bayes model, typically a Multinomial Naive Bayes for text data. Evaluate and refine your model, testing its accuracy and other metrics on a separate test set.\nload the data\n\n# Load the data from the uploaded CSV file\ndf_mental_health = pd.read_csv(\"../Data/all_mental_health_symptoms_nb.csv\")\ndf_mental_health.head()  # Display the first few rows to understand the dataset structure\n\n\n\n\n\n\n\n\nSymptoms\nLabel\n\n\n\n\n0\nFeelings of sadness, tearfulness, emptiness or...\nDepression\n\n\n1\nAngry outbursts, irritability or frustration, ...\nDepression\n\n\n2\nLoss of interest or pleasure in most or all no...\nDepression\n\n\n3\nSleep disturbances, including insomnia or slee...\nDepression\n\n\n4\nTiredness and lack of energy, so even small ta...\nDepression\n\n\n\n\n\n\n\nThe dataset is structured with two columns: ‘Symptoms’ and ‘Label’. Preprocess the text data in the ‘Symptoms’ column. Encode the ‘Label’ column to numerical values, as Naive Bayes requires numerical input. Split the data into training and testing sets. Train a Naive Bayes classifier on the training set. Evaluate the model on the testing set.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Preprocessing the text data\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\nX = tfidf_vectorizer.fit_transform(df_mental_health['Symptoms']).toarray()\n\n# Encoding the 'Label' column\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df_mental_health['Label'])\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Training the Naive Bayes classifier\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluating the model calculating the testing accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f'testing accuracy:',accuracy)\n\n# Predicting the training set results to calculate the training accuracy\ny_train_pred = nb_classifier.predict(X_train)\n\n# Calculating the training accuracy\ntraining_accuracy = accuracy_score(y_train, y_train_pred)\ntraining_accuracy\nprint(f'training accuracy:',training_accuracy)\n\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\nprint(report)\n\n\n\ntesting accuracy: 0.3142857142857143\ntraining accuracy: 0.9873417721518988\n                  precision    recall  f1-score   support\n\n         Anxiety       0.27      0.80      0.40         5\nBipolar Disorder       0.50      0.25      0.33         8\n      Depression       0.20      0.12      0.15         8\n Eating Disorder       0.33      0.14      0.20         7\n   Schizophrenia       0.38      0.43      0.40         7\n\n        accuracy                           0.31        35\n       macro avg       0.33      0.35      0.30        35\n    weighted avg       0.34      0.31      0.29        35\n\n\n\nThe accuracy of the Naive Bayes model on the mental health symptoms dataset is not high: 0.56. This is likely due to the following factors: Overlap of Symptoms: Many mental health conditions have overlapping symptoms, which can confuse the model. For example, symptoms like “anxiety” or “sleep disturbances” can occur in multiple disorders.\nPreprocess the “Symptoms” text data with vectorization.\nEncode the “Label” column into numerical values.\nSplit the data into a training set and a test set.\nTrain a Naive Bayes model. Evaluate the model on the test data.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n# Attempt to read the CSV file while skipping problematic lines\ntry:\n    new_data = pd.read_csv('../Data/mental_sports_nb.csv',on_bad_lines='skip')\n    # Display the first few rows of the dataframe and the number of rows skipped\n    new_first_rows = new_data.head()\n    new_num_rows = new_data.shape[0]\n    new_exception_message = None\nexcept Exception as e:\n    new_first_rows, new_num_rows, new_exception_message = None, None, str(e)\n\nnew_first_rows, new_num_rows, new_exception_message\n\n# Initialize the vectorizer and label encoder\nvectorizer = CountVectorizer()\nlabel_encoder = LabelEncoder()\n\n# Vectorize the 'Symptoms' text\nX = vectorizer.fit_transform(new_data['Symptoms'])\n\n# Encode the 'Label' column\ny = label_encoder.fit_transform(new_data['Label'])\n\n# Split the data into training and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\nprint(accuracy)\nprint(classification_rep)\n\n0.9565217391304348\n               precision    recall  f1-score   support\n\nMental Health       0.92      1.00      0.96        12\n       Sports       1.00      0.91      0.95        11\n\n     accuracy                           0.96        23\n    macro avg       0.96      0.95      0.96        23\n weighted avg       0.96      0.96      0.96        23"
  },
  {
    "objectID": "HW-02/Code/test.html",
    "href": "HW-02/Code/test.html",
    "title": "Bella Shi's Website",
    "section": "",
    "text": "import pandas as pd\nphysical_health_symptoms = [\n    \"I often experience shortness of breath.\",\n    \"I have been feeling persistent fatigue lately.\",\n    \"I am experiencing unexplained weight loss.\",\n    \"I have a consistent cough that won't go away.\",\n    \"I am dealing with frequent headaches.\",\n    \"I have noticed a change in my appetite.\",\n    \"I've been having trouble sleeping recently.\",\n    \"I am experiencing muscle weakness.\",\n    \"I have chronic back pain.\",\n    \"I feel sharp chest pains occasionally.\",\n    \"I have been suffering from prolonged dizziness.\",\n    \"I get unusually tired after physical activity.\",\n    \"I've noticed swelling in my ankles and feet.\",\n    \"I have episodes of rapid heartbeat.\",\n    \"I am experiencing joint pain and stiffness.\",\n    \"I have persistent abdominal pain.\",\n    \"I suffer from severe menstrual cramps.\",\n    \"I have been having digestive issues frequently.\",\n    \"I feel numbness in my hands and feet.\",\n    \"I am dealing with constant throat irritation.\",\n    \"I have unusually dry skin.\",\n    \"I've noticed some vision changes.\",\n    \"I am experiencing frequent urination.\",\n    \"I have a high fever that won't subside.\",\n    \"I am dealing with uncontrolled bleeding.\",\n    \"I have sudden severe headaches.\",\n    \"I get easily bruised without any apparent reason.\",\n    \"I have been having chest congestion.\",\n    \"I am experiencing a loss of balance.\",\n    \"I have difficulty swallowing.\",\n    \"I've been suffering from constant nausea.\",\n    \"I have a persistent rash.\",\n    \"I experience severe allergic reactions.\",\n    \"I have irregular bowel movements.\",\n    \"I am dealing with excessive sweating.\",\n    \"I have a chronic sore throat.\",\n    \"I get severe cramps in my legs.\",\n    \"I have been experiencing unusual hair loss.\",\n    \"I get muscle spasms frequently.\",\n    \"I suffer from chronic dehydration.\",\n    \"I experience tingling sensations in my limbs.\",\n    \"I have unexplained bruises appearing.\",\n    \"I suffer from frequent earaches.\",\n    \"I have difficulty concentrating due to physical discomfort.\",\n    \"I experience unexplained changes in my weight.\",\n    \"I have a persistent feeling of being unwell.\",\n    \"I suffer from extreme fatigue after eating.\",\n    \"I have trouble breathing when lying down.\",\n    \"I get sharp pains in my abdomen.\",\n    \"I have been experiencing increased thirst.\",\n    \"I suffer from regular night sweats.\",\n    \"I have bouts of rapid and unexplained mood swings.\",\n    \"I get sharp pains in my joints.\",\n    \"I have trouble maintaining my balance.\",\n    \"I experience blurred vision.\",\n    \"I have persistent itching without a rash.\",\n    \"I suffer from swelling in my lymph nodes.\",\n    \"I experience a loss of muscle coordination.\",\n    \"I have severe pain in my lower back.\",\n    \"I suffer from extreme cold sensitivity.\",\n    \"I have unexplained gastrointestinal discomfort.\",\n    \"I experience frequent heartburn.\",\n    \"I have a constant salty taste in my mouth.\",\n    \"I suffer from severe migraines.\",\n    \"I experience chest tightness.\",\n    \"I have difficulty breathing during exercise.\",\n    \"I get numbness in my face.\",\n    \"I have been experiencing a persistent cough.\",\n    \"I suffer from sudden vision loss in one eye.\",\n    \"I have a loss of smell and taste.\",\n    \"I experience ringing in my ears.\",\n    \"I have swollen, red, or painful joints.\",\n    \"I get frequent infections.\",\n    \"I experience severe abdominal cramping.\",\n    \"I have trouble with frequent urination at night.\",\n    \"I suffer from excessive thirst.\",\n    \"I get dizzy spells when I stand up.\",\n    \"I have chronic indigestion.\",\n    \"I experience heat intolerance.\",\n    \"I have a prolonged sore that doesn't heal.\",\n    \"I suffer from unexplained muscle aches.\",\n    \"I experience chronic constipation.\",\n    \"I have frequent bouts of diarrhea.\",\n    \"I get unexplained nosebleeds.\",\n    \"I suffer from palpitations.\",\n    \"I have bouts of severe sweating at night.\",\n    \"I experience sudden swelling of hands and feet.\",\n    \"I have persistent acne that doesn’t respond to treatment.\",\n    \"I suffer from extreme light sensitivity.\",\n    \"I have recurring urinary tract infections.\",\n    \"I experience short, sharp pains in my chest.\",\n    \"I have a tingling sensation in my fingertips.\",\n    \"I suffer from excessive gas and bloating.\",\n    \"I experience frequent flare-ups of eczema.\",\n    \"I have chronic sinus pressure.\",\n    \"I suffer from recurrent bronchitis.\",\n    \"I have severe pains in my neck.\",\n    \"I experience swelling and pain in my knee.\",\n    \"I have a chronic metallic taste in my mouth.\",\n    \"I suffer from restless leg syndrome at night.\"\n]\nmental_health_symptoms = [\n    \"I frequently feel anxious without any specific reason.\",\n    \"I have been experiencing overwhelming sadness.\",\n    \"I feel a persistent sense of hopelessness.\",\n    \"I have lost interest in activities I used to enjoy.\",\n    \"I am dealing with significant mood swings.\",\n    \"I find it hard to concentrate on everyday tasks.\",\n    \"I am experiencing severe emotional highs and lows.\",\n    \"I feel a constant sense of worthlessness.\",\n    \"I have been having trouble sleeping or sleeping too much.\",\n    \"I feel persistently tired and have low energy.\",\n    \"I am experiencing unexplained irritability.\",\n    \"I am withdrawing from social interactions.\",\n    \"I have a significant change in my eating habits.\",\n    \"I am dealing with constant restlessness.\",\n    \"I feel intense and uncontrolled anger.\",\n    \"I have thoughts of self-harm or suicide.\",\n    \"I am experiencing panic attacks.\",\n    \"I have obsessive thoughts or behaviors.\",\n    \"I am struggling with compulsive behaviors.\",\n    \"I have been feeling detached from reality.\",\n    \"I am experiencing hallucinations.\",\n    \"I have a persistent sense of guilt.\",\n    \"I am having difficulty dealing with stress.\",\n    \"I feel disconnected from my own emotions.\",\n    \"I am experiencing paranoia or distrust of others.\",\n    \"I have a loss of motivation for most activities.\",\n    \"I am feeling unusually confused.\",\n    \"I am dealing with severe anxiety in social situations.\",\n    \"I am struggling with feelings of overwhelm.\",\n    \"I am experiencing persistent nightmares.\",\n    \"I feel an excessive fear or worry about specific things.\",\n    \"I am struggling with flashbacks of traumatic events.\",\n    \"I am having trouble forming or maintaining relationships.\",\n    \"I am experiencing severe physical reactions to stress.\",\n    \"I am having difficulty controlling my thoughts.\",\n    \"I have a strong feeling of dread about the future.\",\n    \"I am experiencing prolonged periods of sadness.\",\n    \"I am dealing with unexplained physical ailments.\",\n    \"I feel a lack of interest in personal hygiene.\",\n    \"I am experiencing significant weight gain or loss.\",\n    \"I feel a sense of detachment from loved ones.\",\n    \"I am dealing with an inability to cope with daily problems.\",\n    \"I have a decreased ability to feel pleasure.\",\n    \"I am experiencing frequent tearfulness.\",\n    \"I have a strong sense of guilt or worthlessness.\",\n    \"I am finding it hard to make decisions.\",\n    \"I am experiencing an inability to feel empathy.\",\n    \"I am dealing with persistent nervousness.\",\n    \"I have a pervasive fear of being judged by others.\",\n    \"I am experiencing a loss of sexual desire.\",\n    \"I am feeling trapped or in an unbearable situation.\",\n    \"I am dealing with excessive rumination on past events.\",\n    \"I feel a general sense of malaise or being unwell.\",\n    \"I am experiencing sudden changes in behavior.\",\n    \"I have a lack of desire to interact with others.\",\n    \"I am feeling a persistent emptiness.\",\n    \"I am experiencing unexplained aches and pains.\",\n    \"I have a strong sense of unreality or being detached from oneself.\",\n    \"I am dealing with a decreased ability to cope with minor problems.\",\n    \"I am experiencing intense fear or phobia.\",\n    \"I feel a chronic sense of unease or nervous anticipation.\",\n    \"I am dealing with an extreme sensitivity to rejection.\",\n    \"I am experiencing frequent outbursts of anger.\",\n    \"I have a pervasive pattern of unstable relationships.\",\n    \"I am feeling a chronic sense of alienation from others.\",\n    \"I am experiencing difficulty with memory or concentration.\",\n    \"I have a decreased sense of self-worth.\",\n    \"I am dealing with a persistent feeling of being overwhelmed.\",\n    \"I am experiencing strong feelings of envy or jealousy.\",\n    \"I feel an inability to be alone.\",\n    \"I am feeling an intense and irrational fear of everyday situations.\",\n    \"I am experiencing difficulty understanding and expressing emotions.\",\n    \"I am dealing with a constant preoccupation with certain ideas.\",\n    \"I have a lack of flexibility in thinking or behavior.\",\n    \"I am experiencing extreme restlessness or agitation.\",\n    \"I am feeling a deep fear of abandonment.\",\n    \"I have a tendency to self-isolate or withdraw from others.\",\n    \"I am experiencing a prolonged sense of grief or mourning.\",\n    \"I feel an excessive need for admiration or attention.\",\n    \"I am dealing with a severe reaction to criticism.\",\n    \"I am experiencing a persistent feeling of being out of control.\",\n    \"I have a distorted or negative body image.\",\n    \"I am feeling a chronic sense of boredom or emptiness.\",\n    \"I am experiencing an exaggerated sense of self-importance.\",\n    \"I feel an intense and inappropriate display of emotions.\",\n    \"I am dealing with a persistent distrust of others.\",\n]\n\n# Label for each symptom\nlabels = [\"Physical health\" for _ in range(100)]\n\nlabels2 = [\"Mental health\" for _ in range(86)]\n\n# Creating the DataFrame\nsymptoms_df = pd.DataFrame({\n    \"Symptoms\": physical_health_symptoms,\n    \"Label\": labels\n})\n\nsymptoms_df1 = pd.DataFrame({\n    \"Symptoms\": mental_health_symptoms,\n    \"Label\": labels2\n})\n\n# Check the lengths of the lists\nprint(len(mental_health_symptoms))\nprint(len(labels2))\n\n# If they are not the same, correct the lengths\n# Assuming 'labels2' is supposed to be the same length as 'mental_health_symptoms'\nlabels2 = [\"Mental Health\"] * len(mental_health_symptoms)\n\n# # Now create the DataFrame\n# symptoms_df1 = pd.DataFrame({\n#     \"Symptoms\": mental_health_symptoms,\n#     \"Label\": labels2\n# })\n\n# Saving to a CSV file\nsymptoms_df.to_csv(\"./physical_health_symptoms.csv\", index=False)\nsymptoms_df1.to_csv(\"./mental_health_symptoms.csv\", index=False)\n# combine two csv files\ndf1 = pd.read_csv(\"./physical_health_symptoms.csv\")\ndf2 = pd.read_csv(\"./mental_health_symptoms.csv\")\ndf = pd.concat([df1, df2])\ndf.to_csv(\"./symptoms.csv\", index=False)\n\n86\n86"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "https://www.verywellmind.com/definition-of-mental-illness-4587855 https://www.betterhealth.vic.gov.au/health/conditionsandtreatments/anxiety-disorders https://www.betterhealth.vic.gov.au/health/servicesandsupport/types-of-mental-health-issues-and-illnesses"
  },
  {
    "objectID": "Naive Bayes.html",
    "href": "Naive Bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a probabilistic machine learning model that’s based on Bayes’ Theorem. It’s called “naive” because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets.\n\n\nAt its core, Naive Bayes relies on Bayes’ Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For classification, Bayes’ Theorem is stated as:\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\\]\nHere,P(B|A) is the probability of hypothesis A given data B. P(B|A) is the probability of the data under the hypothesis, P(A) is the probability of the hypothesis before seeing the data, and P(B) is the probability of the data under any hypothesis.\n\n\n\nThe objective in Naive Bayes is to determine the probability of a label given a set of features, and then predict the label with the highest probability. In other words, we calculate the posterior probability of each class given the input features and predict the class for which the posterior probability is the highest.\n\n\n\nNaive Bayes aims to model the conditional probability of the class labels given the features in order to make predictions. It’s particularly advantageous in situations where the dimensionality of the input space is high, as it avoids the curse of dimensionality to a certain extent due to its feature independence assumption\n\n\n\nThere are several variants of the Naive Bayes classifier, each appropriate for a different type of dataset:\n\nGaussian Naive Bayes:\n\nUsed when features have a continuous distribution and an assumption of normal distribution (Gaussian) is reasonable. Appropriate for many real-world problems, like predicting whether a given email is spam or not.\n\nMultinomial Naive Bayes:\n\nUsed for discrete data where features represent counts or frequency counts of events. Often used in text classification, where the features are related to word counts or frequencies within the documents.\n\nBernoulli Naive Bayes:\n\nUsed when features are binary (i.e., they take only two values like true and false). Suitable for making predictions from binary features, like if a word occurs in a document or not, which is a common scenario in text classification problems."
  },
  {
    "objectID": "Naive Bayes.html#introduction-to-naive-bayes",
    "href": "Naive Bayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a probabilistic machine learning model that’s based on Bayes’ Theorem. It’s called “naive” because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets.\n\n\nAt its core, Naive Bayes relies on Bayes’ Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For classification, Bayes’ Theorem is stated as:\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\\]\nHere,P(B|A) is the probability of hypothesis A given data B. P(B|A) is the probability of the data under the hypothesis, P(A) is the probability of the hypothesis before seeing the data, and P(B) is the probability of the data under any hypothesis.\n\n\n\nThe objective in Naive Bayes is to determine the probability of a label given a set of features, and then predict the label with the highest probability. In other words, we calculate the posterior probability of each class given the input features and predict the class for which the posterior probability is the highest.\n\n\n\nNaive Bayes aims to model the conditional probability of the class labels given the features in order to make predictions. It’s particularly advantageous in situations where the dimensionality of the input space is high, as it avoids the curse of dimensionality to a certain extent due to its feature independence assumption\n\n\n\nThere are several variants of the Naive Bayes classifier, each appropriate for a different type of dataset:\n\nGaussian Naive Bayes:\n\nUsed when features have a continuous distribution and an assumption of normal distribution (Gaussian) is reasonable. Appropriate for many real-world problems, like predicting whether a given email is spam or not.\n\nMultinomial Naive Bayes:\n\nUsed for discrete data where features represent counts or frequency counts of events. Often used in text classification, where the features are related to word counts or frequencies within the documents.\n\nBernoulli Naive Bayes:\n\nUsed when features are binary (i.e., they take only two values like true and false). Suitable for making predictions from binary features, like if a word occurs in a document or not, which is a common scenario in text classification problems."
  },
  {
    "objectID": "Naive Bayes.html#prepare-data-for-naïve-bayes",
    "href": "Naive Bayes.html#prepare-data-for-naïve-bayes",
    "title": "Naive Bayes",
    "section": "Prepare Data for Naïve Bayes",
    "text": "Prepare Data for Naïve Bayes\n1.Handle missing values\n2.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models.\n3.Split the data into training and test sets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\ndf_mental_health=pd.read_csv('./Data_cleaned/mental_health.csv')\n\nCheck missing values in the dataset\n\nmissing_data=df_mental_health.isnull().sum()\nprint(missing_data)\nprint(df_mental_health.shape)\n\nEconomy                                                             0\nCode                                                                0\nYear                                                                0\nSchizophrenia (%)                                                   0\nBipolar disorder (%)                                                0\nEating disorders (%)                                                0\nAnxiety disorders (%)                                               0\nDepression (%)                                                      0\nIncome group                                                       56\naverage_learning_Adjusted_of_school                               812\nContinent                                                         588\nGDP(2022)                                                         224\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent    2380\nGDP_per_capita                                                   1020\ndtype: int64\n(5488, 14)\n\n\n\n1.Handle missing values\na.Dropping columns with more than 1000 null values\n(We don’t Remove Rows with Missing Values because although this is the simplest approach, but it can lead to a significant reduction in data size, which might not be ideal if the missing data is extensive.)\nso after this step, we dropped the column not_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\nthreshold = 1030  # Set the threshold for the minimum number of non-NA values required\ndf_mental_health = df_mental_health.dropna(axis=1, thresh=(len(df_mental_health) - threshold))\n# Display the columns remaining after the operation\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\nNr0=len(df_mental_health.index)\nNc0=len(df_mental_health.columns)\nprint(\"Nrows = \",Nr0,\"\\nNcol=\",Nc0,\"\\nMatrix entries = \", Nr0*Nc0)\n# missing_data1 = encoded_data1.isnull().sum()\n# print(missing_data1)\n\nEconomy                                   0\nCode                                      0\nYear                                      0\nSchizophrenia (%)                         0\nBipolar disorder (%)                      0\nEating disorders (%)                      0\nAnxiety disorders (%)                     0\nDepression (%)                            0\nIncome group                             56\naverage_learning_Adjusted_of_school     812\nContinent                               588\nGDP(2022)                               224\nGDP_per_capita                         1020\ndtype: int64\nNrows =  5488 \nNcol= 13 \nMatrix entries =  71344\n\n\nb.Impute Missing Values:\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is a common strategy for numerical data.\nForward or Backward Fill: For time-series data, we can fill missing values with the next or previous values.\nPredictive Imputation: Use a machine learning algorithm to predict the missing values based on other data in the dataset.\nMean/Median Imputation: If the data is normally distributed, you might use the mean. If the distribution is skewed,the median might be more appropriate.\n\nimport seaborn as sns\n#1.plot the GDP(2022) in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['GDP(2022)'])\n#2.plot the average_learning_Adjusted_of_school in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n#3. plot the gdp_per_capita in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['GDP_per_capita'])\n#3.although income group and df_mental_health also have a lot of missing values\n#  but we can not plot  because it is a categorical variable\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_57004/2298538592.py:4: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['GDP(2022)'])\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_57004/2298538592.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_57004/2298538592.py:10: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['GDP_per_capita'])\n\n\n&lt;Axes: xlabel='GDP_per_capita', ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\n\n\nso here we impute missing values using the mean for GDP(2022) and the median for average_learning_Adjusted_of_school (assuming the distributions are appropriate for these measures):\n\n\n# Impute missing values with the mean for GDP(2022)\ndf_mental_health['GDP(2022)'].fillna(df_mental_health['GDP(2022)'].mean(), inplace=True)\n\n# Impute missing values with the median for average_learning_Adjusted_of_school\ndf_mental_health['average_learning_Adjusted_of_school'].fillna(df_mental_health['average_learning_Adjusted_of_school'].median(), inplace=True)\n\n# Impute missing values with the median for GDP_per_capita\ndf_mental_health['GDP_per_capita'].fillna(df_mental_health['GDP_per_capita'].median(), inplace=True)\nmissing_data = df_mental_health.isnull().sum()\n\n# # impute missing values with mode for Continent\n# df_mental_health['Continent'].fillna(df_mental_health['Continent'].mode()[0], inplace=True)\nprint(missing_data)\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school      0\nContinent                              588\nGDP(2022)                                0\nGDP_per_capita                           0\ndtype: int64\n\n\n\nDrop rows with missing values in columns (‘Income group’,‘Continent’)\n\n\n# Droping rows where 'Income group' annd 'continent' is missing since it's our target variable\ndf_mental_health = df_mental_health.dropna(subset=['Income group'])\ndf_mental_health=df_mental_health.dropna(subset=['Continent'])\n\n# Verifying that there are no more missing values\nmissing_values_final = df_mental_health.isnull().sum()\nmissing_values_final, df_mental_health.shape\n\n(Economy                                0\n Code                                   0\n Year                                   0\n Schizophrenia (%)                      0\n Bipolar disorder (%)                   0\n Eating disorders (%)                   0\n Anxiety disorders (%)                  0\n Depression (%)                         0\n Income group                           0\n average_learning_Adjusted_of_school    0\n Continent                              0\n GDP(2022)                              0\n GDP_per_capita                         0\n dtype: int64,\n (4872, 13))\n\n\n\n# Save the cleaned dataframe to a new CSV file\ndf_mental_health.to_csv('./Data_cleaned/mental_health_DR.csv', index=False)\n\nThe dataset is now clean and ready for classification. We can proceed with the next step."
  },
  {
    "objectID": "Naive Bayes.html#feature-selection",
    "href": "Naive Bayes.html#feature-selection",
    "title": "Naive Bayes",
    "section": "Feature selection",
    "text": "Feature selection\nDecide which features are relevant for our classification task(Naive Bayes model) and drop the rest.\nIn this section, I am going to use the following features (educational, economial and geological factors) and use the mental health disorder (Use eating disorder here) as a target varible for classfication. But since our target varible it is a numerical varible so we are going to a function to convert it to categorical variable that could be used for classification in Naive Bayes.\n\nMethod:\n\nEDA to find the correlation between the features and the target variable\n\n\nCorrelation matrix\nPearson Correlation\n\n\nSelect the features that have the highest correlation with the target variable\nUse the method to convert the target variable from numerical to categorical variable\n\n\n1. EDA\n\nCorralation matrix\n\n\nprint(df_mental_health.columns)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# # Load the cleaned dataset\n# file_path = './Data/mental_health.csv'\n# data = pd.read_csv(file_path)\n\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    'GDP_per_capita',\n    # 'Income group',\n    # 'Continent',\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\n# encoded_data.to_csv('./mental_health_encoded.csv', index=False)\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\n# print(encoded_data.head())\nprint(type(encoded_data))\n# print(new_correlation_matrix)\n\n\n\nIndex(['Economy', 'Code', 'Year', 'Schizophrenia (%)', 'Bipolar disorder (%)',\n       'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)',\n       'Income group', 'average_learning_Adjusted_of_school', 'Continent',\n       'GDP(2022)', 'GDP_per_capita'],\n      dtype='object')\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\nStrong Correlation: Values above 0.7 or below -0.7 suggest a strong correlation between the variables. Moderate Correlation: Values between 0.3 and 0.7 (or -0.3 and -0.7) suggest a moderate correlation. Weak Correlation: Values between 0 and 0.3 (or 0 and -0.3) suggest a weak correlation.\n\nPearson Correlation\n\nThe Pearson correlation coefficients you’ve provided show the linear relationship between each feature and the ‘Income group’ target variable. A coefficient close to 1 or -1 indicates a strong positive or negative linear relationship, respectively. A coefficient close to 0 suggests no linear relationship.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# For Pearson Correlation, we would ideally have only numeric features and labels\n# so we will encode the categorical features (Income group and Continent) using LabelEncoder\n# Encoding the 'Income group' column to have numerical labels\nlabel_encoder = LabelEncoder()\ndf_mental_health['Income group'] = label_encoder.fit_transform(df_mental_health['Income group'])\n# # Encoding the 'Continent' column to have numerical labels\ndf_mental_health['Continent'] = label_encoder.fit_transform(df_mental_health['Continent'])\n\n# Compute the Pearson Correlation matrix\ncorrelation_matrix = df_mental_health.corr()\n# print(correlation_matrix)\n# Selecting only correlations with the target variable (assuming target is numeric)\ntarget_correlation = correlation_matrix['Eating disorders (%)'].sort_values(ascending=False)\n\n# Review the correlations and decide on a threshold for selecting features\nprint(target_correlation)\n\nEating disorders (%)                   1.000000\nGDP_per_capita                         0.769337\nBipolar disorder (%)                   0.749827\nSchizophrenia (%)                      0.699402\nAnxiety disorders (%)                  0.668839\naverage_learning_Adjusted_of_school    0.663485\nContinent                              0.358841\nDepression (%)                         0.180075\nGDP(2022)                              0.136258\nYear                                   0.086225\nIncome group                          -0.528488\nName: Eating disorders (%), dtype: float64\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_57004/2418411318.py:14: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = df_mental_health.corr()\n\n\n\n\n2. Select the features\nNow, we can decide which features to drop based on their correlation with the target and their inter-correlations with other features\nSelect: So we see from the results that GDP_per_capita has a corralaition 0.769337, income group has a corraltion that is -0.528488, average_learning_Adjusted_of_school has a corraltion is 0.663485, and Continent has a corrralation is 0.35884. So we are going to select these features for our classification model.\nDrop: The rest of the features have a weak correlation with the target variable: GDP(2022) with a correlation coefficient of approximately 0.14. And since we are using the mental health features have strong inter-correlations from the heatmap, so we are going to drop the mental health features except for eating disorders as well .\n\ncolumns_to_drop = ['Economy', 'Code', 'Year', 'Schizophrenia (%)', 'Bipolar disorder (%)',\n                   'Anxiety disorders (%)', 'Depression (%)', 'GDP(2022)']\ndf_mental_health = df_mental_health.drop(columns=columns_to_drop, axis=1)\ndf_mental_health.head()\n\n\n\n\n\n\n\n\nEating disorders (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP_per_capita\n\n\n\n\n0\n0.101855\n1\n4.957542\n1\n3309.844\n\n\n1\n0.099313\n1\n4.957542\n1\n3309.844\n\n\n2\n0.096692\n1\n4.957542\n1\n3309.844\n\n\n3\n0.094336\n1\n4.957542\n1\n3309.844\n\n\n4\n0.092439\n1\n4.957542\n1\n3309.844\n\n\n\n\n\n\n\n\n\n3. Convert the target variable to categorical variable\nI create a function to convert continuous variables into two categories (‘High’, ‘Low’)\n\nimport pandas as pd\n\n# Function to convert continuous variables into two categories ('High', 'Low')\ndef binarize_data(df, columns):\n    \"\"\"\n    Binarizes the specified columns of the dataframe based on the median.\n    \"\"\"\n    # List of columns to trinarize\n    columns = ['Eating disorders (%)']\n    binarized_df_mental_health = df_mental_health.copy()\n    for col in columns:\n        # Define the median\n        median = df[col].median()\n        # Binarize the column\n        binarized_df_mental_health[col] = pd.cut(df[col], bins=[-float('inf'), median, float('inf')],\n                                   labels=['Low', 'High'])\n    return binarized_df_mental_health\n\n# Binarize the dataframe\nbinarized_df_mental_health = binarize_data(df_mental_health, columns=['Eating disorders (%)'])\nbinarized_df_mental_health.head()\nbinarized_df_mental_health.to_csv('./Data_cleaned/mental_health_binarized.csv', index=False)\n\n\n\n\nNaïve Bayes (NB) with Labeled Record Data\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n# print(df_mental_health.info())\n# #save df_mental_health to csv\n# df_mental_health.to_csv('./Data/mental_health_DT.csv', index=False) # this is the cleaned data of the mental health and income group\n# print(df_mental_health.head())\n\n# Selecting features and target variable\n# print(X.columns)\nX = binarized_df_mental_health.drop(['Eating disorders (%)'], axis=1)\ny = binarized_df_mental_health['Eating disorders (%)']\n# print(X.columns)\n\n# Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Checking the shape of the datasets\n(X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)\n# Train the Naïve Bayes model\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Validate the model\ny_val_pred = nb_classifier.predict(X_val)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nval_classification_report = classification_report(y_val, y_val_pred)\nval_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n\n\n# Visualize the confusion matrix\nsns.heatmap(val_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Validation Set')\nplt.show()\n\n\n\n\n\n# Evaluate the model using the validation set\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(f\"Classification Report for Validation Set:\\n{val_classification_report}\")\n\nValidation Accuracy: 0.8385772913816689\nClassification Report for Validation Set:\n              precision    recall  f1-score   support\n\n        High       0.95      0.72      0.82       365\n         Low       0.77      0.96      0.86       366\n\n    accuracy                           0.84       731\n   macro avg       0.86      0.84      0.84       731\nweighted avg       0.86      0.84      0.84       731\n\n\n\n\n# Test the model\ny_test_pred = nb_classifier.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_classification_report = classification_report(y_test, y_test_pred)\ntest_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n\n# Visualize the confusion matrix for the test set\nsns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n# Evaluate the model using the test set\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Classification Report for Test Set:\\n{test_classification_report}\")\n\n\n\n\nTest Accuracy: 0.8604651162790697\nClassification Report for Test Set:\n              precision    recall  f1-score   support\n\n        High       0.95      0.75      0.84       355\n         Low       0.80      0.97      0.88       376\n\n    accuracy                           0.86       731\n   macro avg       0.88      0.86      0.86       731\nweighted avg       0.88      0.86      0.86       731\n\n\n\n\n\nNaïve Bayes (NB) with Labeled Text Data\nPredict if it descrbes a mental health problem based on the symptoms Preprocess the “Symptoms” text data with vectorization.\n\nThe dataset is structured with two columns: ‘Symptoms’ and ‘Label’. load the dataset\nPreprocess the text data in the ‘Symptoms’ column. This includes cleaning and tokenizing the text, normalizing it, and removing stop words.\nEncode the ‘Label’ column to numerical values, as Naive Bayes requires numerical input.\nSplit the data into training and testing sets.\nTrain a Naive Bayes classifier on the training set.\nEvaluate the model on the testing set.\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n# Attempt to read the CSV file while skipping problematic lines\ntry:\n    new_data = pd.read_csv('./Data_cleaned/symptoms.csv',on_bad_lines='skip')\n    # Display the first few rows of the dataframe and the number of rows skipped\n    new_first_rows = new_data.head()\n    new_num_rows = new_data.shape[0]\n    new_exception_message = None\nexcept Exception as e:\n    new_first_rows, new_num_rows, new_exception_message = None, None, str(e)\n\nnew_first_rows, new_num_rows, new_exception_message\n\n# Initialize the vectorizer and label encoder\nvectorizer = CountVectorizer()\nlabel_encoder = LabelEncoder()\n\n# Vectorize the 'Symptoms' text\nX = vectorizer.fit_transform(new_data['Symptoms'])\n\n# Encode the 'Label' column\ny = label_encoder.fit_transform(new_data['Label'])\n\n# Split the data into training and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\nprint(accuracy)\nprint(classification_rep)\n\ntest_confusion_matrix = confusion_matrix(y_test, y_pred)\n# Visualize the confusion matrix for the test set\nsns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n0.8157894736842105\n                 precision    recall  f1-score   support\n\n  Mental health       0.86      0.82      0.84        22\nPhysical health       0.76      0.81      0.79        16\n\n       accuracy                           0.82        38\n      macro avg       0.81      0.82      0.81        38\n   weighted avg       0.82      0.82      0.82        38"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About me",
    "section": "About me",
    "text": "About me\nBella Shi is a currently a student studying in the graduate school of Art and Sciences in Georgetown university. She majored in software engineering and minored in English in undergraduate. In her lesuire time, She likes swimming, going to the gym, hiking as well as other outdoor activities."
  },
  {
    "objectID": "about.html#academic-interests",
    "href": "about.html#academic-interests",
    "title": "About me",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nData Science and Analytics\nComputer Science"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\n\n2018-2022: Taiyuan University Of Technology\n2023-Present: Georgetown University"
  },
  {
    "objectID": "Gathering.html",
    "href": "Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Mental health Depression disorder Data. What are mental health disorder types?\nWhat percentage of people in each country suffer these disorder types from year 1990 to 2017?\ndownload link\n\n\n\n\n\n\n\nShare of population with eating disorders. This is estimated as the total number of cases with eating disorders relative to the population of a country.\nThis indicator only includes anorexia nervosa and bulimia nervosa (i.e. it does not include binge eating disorder and other specified feeding or eating disorder (OSFED)).\ndownload link\n\n\n\n\n\n\n\nRespondents were asked how comfortable a local person would feel speaking about anxiety or depression with someone they know. This shows the share that gave each response.\ndownload link\n\n\n\n\n\n\n\nThe relationship between countries and their continents.\ndownload link\n\n\n\n\n\n\n\nThe average learning years of schooling refers to the average number of years of schooling that people aged 25 and older, as by this age most individuals have completed their formal education. This metric is significant because it provides insight into the educational level of a population. And in this dataset shows the relationship between the average learning years of schooling and the each Economy.\ndownload link\n\n\n\n\n\n\nGDP is the total value of goods and services produced within a country during one year. GDP per capita is the GDP divided by the population. These two datasets show the relationship between the GDP/ GDP per capita and each Economy\ndownload link"
  },
  {
    "objectID": "Gathering.html#download-data",
    "href": "Gathering.html#download-data",
    "title": "Data Gathering",
    "section": "",
    "text": "Mental health Depression disorder Data. What are mental health disorder types?\nWhat percentage of people in each country suffer these disorder types from year 1990 to 2017?\ndownload link\n\n\n\n\n\n\n\nShare of population with eating disorders. This is estimated as the total number of cases with eating disorders relative to the population of a country.\nThis indicator only includes anorexia nervosa and bulimia nervosa (i.e. it does not include binge eating disorder and other specified feeding or eating disorder (OSFED)).\ndownload link\n\n\n\n\n\n\n\nRespondents were asked how comfortable a local person would feel speaking about anxiety or depression with someone they know. This shows the share that gave each response.\ndownload link\n\n\n\n\n\n\n\nThe relationship between countries and their continents.\ndownload link\n\n\n\n\n\n\n\nThe average learning years of schooling refers to the average number of years of schooling that people aged 25 and older, as by this age most individuals have completed their formal education. This metric is significant because it provides insight into the educational level of a population. And in this dataset shows the relationship between the average learning years of schooling and the each Economy.\ndownload link\n\n\n\n\n\n\nGDP is the total value of goods and services produced within a country during one year. GDP per capita is the GDP divided by the population. These two datasets show the relationship between the GDP/ GDP per capita and each Economy\ndownload link"
  },
  {
    "objectID": "Gathering.html#api",
    "href": "Gathering.html#api",
    "title": "Data Gathering",
    "section": "API",
    "text": "API\n\n1. Income group Data\n\n\nDescription\nThe World Development Indicators (WDI) is the primary World Bank collection of development indicators, compiled from officially-recognized international sources. It presents the most current and accurate global development data available, and includes national, regional and global estimates.\nIn this dataset: each economy is classified as a income level based on the World Bank’s income group classification (see source for more details). The income group classification is based on Gross National Income (GNI) per capita for the previous year (2019, in this case). The income group thresholds are determined using the World Bank Atlas method. The classification is updated each year on July 1st.\n\n\n\nLabel\nValue\n\n\n\n\nFiscal Yeay\nFY21\n\n\nPublication Date\n(July 2020)\n\n\nLow Income Threshold\n$1,035 or less\n\n\nLower Middle Income Range\n$1,036 to $4,045\n\n\nUpper Middle Income Range\n$4,046 to $12,535\n\n\nHigh Income Threshold\n$12,536 or more\n\n\nNumber of economies\n189\n\n\nGNI Year\n2019\n\n\nTerm of Income Classification\nuntil 1 July 2021\n\n\n\nAPI link\n\n\nPython Api Code\n\nimport requests\nimport pandas as pd\nimport json\nimport csv\n# Define the API endpoint URL\nurl = \"https://datacatalogapi.worldbank.org/ddhxext/ResourceFileData?resource_unique_id=DR0090755&version_id=2023-09-27T16:44:25.8023254Z\"\n# Make the GET request\nresponse = requests.get(url)\n# Check the response status\nif response.status_code == 200:\n    data = response.json()  # If the response contains JSON data, it can be accessed using the .json() method\nelse:\n    print(\"Failed to retrieve data. Status code:\", response.status_code)\n# Extract column names from the \"MetaData\" section\ncolumns = [item['ColumnName'] for item in data[\"MetaData\"]]\n# Extract data from the \"Details\" section\ndetails = data[\"Details\"]\nrows = []\nfor chunk in details:\n    row=[]\n    for key,value in chunk.items():\n    # Check if the value is None and replace it with an empty string if needed\n        value = value if value is not None else ''\n        row.append(value)\n    rows.append(row)\n# # Write data to a CSV file\n# with open('./Data_uncleaned/api_python.csv', 'w', newline='') as csvfile:\n#     csv_writer = csv.writer(csvfile)\n#     # Write the header row\n#     csv_writer.writerow(columns)\n#     # Write the data rows\n#     csv_writer.writerows(rows)\n# print(\"CSV file 'api_python.csv' has been created.\")\n\n\n\nR Api Code\nlibrary(httr)\nlibrary(jsonlite)\nurl &lt;- \"https://datacatalogapi.worldbank.org/ddhxext/ResourceFileData?resource_unique_id=DR0090755&version_id=2023-09-27T16:44:25.8023254Z\"\n# Make the GET request\nresponse &lt;- GET(url)\nresponse.text &lt;- content(response, \"text\")\ndata &lt;- fromJSON(response.text)\ndetails &lt;- data$Details\n# To write this data to a CSV file\n# write.csv(details, file = './Data_uncleanedapi_r.csv', row.names = FALSE)"
  },
  {
    "objectID": "reduction.html",
    "href": "reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "First,we after looking at the datasets and the features, we can see that there are some features that are highly correlated with each other. This is a problem because it can lead to overfitting. To solve this problem, we can use dimensionality reduction techniques to reduce the number of features in the dataset. This will help us to avoid overfitting and also reduce the computational cost of the model."
  },
  {
    "objectID": "reduction.html#data-preprocessing",
    "href": "reduction.html#data-preprocessing",
    "title": "Dimensionality Reduction",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\nload the dataset named mental_health_DR.csv (it is the dataset after data cleaning in Naive Bayes).\nHandle missing values and non-numeric entries.\nConvert categorical data to numerical.\nNormalize the data. so the numerical data has a mean of 0 and standard deviation of 1.(This is crucial for PCA and t-SNE because they are sensitive to the scale of the data.)\n\n\n#1.load data\nimport pandas as pd\ndf_reduction=pd.read_csv('./Data_Cleaned/mental_health_DR.csv')\n#2. drop columns\ndf_reduction = df_reduction.drop(['Code','Year','Economy'], axis=1)\ndf_reduction.head()\n\n\n\n\n\n\n\n\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nGDP_per_capita\n\n\n\n\n0\n0.160560\n0.697779\n0.101855\n4.828830\n4.071831\nLow income\n4.957542\nAsia\n14583.0\n3309.844\n\n\n1\n0.160312\n0.697961\n0.099313\n4.829740\n4.079531\nLow income\n4.957542\nAsia\n14583.0\n3309.844\n\n\n2\n0.160135\n0.698107\n0.096692\n4.831108\n4.088358\nLow income\n4.957542\nAsia\n14583.0\n3309.844\n\n\n3\n0.160037\n0.698257\n0.094336\n4.830864\n4.096190\nLow income\n4.957542\nAsia\n14583.0\n3309.844\n\n\n4\n0.160022\n0.698469\n0.092439\n4.829423\n4.099582\nLow income\n4.957542\nAsia\n14583.0\n3309.844\n\n\n\n\n\n\n\n\n# 2.Handling Non-Numeric Values\n# Non-numeric entries such as \"no data\" are replaced with NaN, so they can be imputed later.\n#  Replace non-numeric entries with NaN\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndf_reduction.isnull().sum()\n# 3.Encoding Categorical Data, so Categorical data is encoded numerically.\n# Label encoding assigns each unique value to a different integer.\n# we only encode the Income group column because it is ordinal, we don't encode the the Economy because it is nominal and it's not relevant to the analysis.\nlabel_encoder = LabelEncoder()\ndf_reduction['Income group'] = label_encoder.fit_transform(df_reduction['Income group'])\ndf_reduction['Continent'] = label_encoder.fit_transform(df_reduction['Continent'])\n# if there is missing value in the Income group column, we can use the following code to fill the missing value with the most frequent value\n# df_reduction['Income group'] = df_reduction['Income group'].fillna(df_reduction['Income group'].mode()[0])\n\n# 4.Normalizing the Data (The data is normalized to ensure that all features contribute equally to the analysis.)\n# Normalization standardizes the range of features in your dataset, ensuring that each feature contributes equally to the analysis and is not dominated by those with larger magnitudes.\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_reduction)\n\n# Display the first few rows of the processed data\nscaled_data_df = pd.DataFrame(scaled_data, columns=df_reduction.columns)\nprint(scaled_data_df.head())\n\n   Schizophrenia (%)  Bipolar disorder (%)  Eating disorders (%)  \\\n0          -1.106590             -0.198086             -0.844308   \n1          -1.112626             -0.196941             -0.860632   \n2          -1.116943             -0.196017             -0.877464   \n3          -1.119318             -0.195070             -0.892593   \n4          -1.119686             -0.193730             -0.904771   \n\n   Anxiety disorders (%)  Depression (%)  Income group  \\\n0               0.750876        0.928233     -0.464925   \n1               0.751669        0.940304     -0.464925   \n2               0.752859        0.954143     -0.464925   \n3               0.752646        0.966422     -0.464925   \n4               0.751393        0.971739     -0.464925   \n\n   average_learning_Adjusted_of_school  Continent  GDP(2022)  GDP_per_capita  \n0                            -1.188637   -0.41766  -0.261241       -0.386936  \n1                            -1.188637   -0.41766  -0.261241       -0.386936  \n2                            -1.188637   -0.41766  -0.261241       -0.386936  \n3                            -1.188637   -0.41766  -0.261241       -0.386936  \n4                            -1.188637   -0.41766  -0.261241       -0.386936"
  },
  {
    "objectID": "reduction.html#a.principal-component-analysis-pca",
    "href": "reduction.html#a.principal-component-analysis-pca",
    "title": "Dimensionality Reduction",
    "section": "A.Principal Component Analysis (PCA)",
    "text": "A.Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique that is used to reduce the dimensionality of a large number of interrelated variables while retaining as much as possible of the variation present in the dataset. PCA is an unsupervised learning algorithm, which means that it does not require labels to be trained on. PCA can be used for feature reduction before training a model.\n\nApply PCA.\nDetermine the optimal number of principal components.\nVisualize the PCA results.\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1.Applying PCA\npca = PCA()\nlabels=scaled_data_df['Income group']\nscaled_data_df_1 = scaled_data_df.drop(['Income group'], axis=1)\npca_data = pca.fit_transform(scaled_data_df_1)\n\n# 2.Determining the optimal number of principal components\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\n# 3.Plotting the explained variance\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\nplt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.title('Explained Variance by Different Principal Components')\nplt.show()\n# print('explained_variance is', explained_variance)\n# print('cumulative_variance is', cumulative_variance)\n\n# Using this plot, we can select the number of components that capture a sufficient amount of variance (e.g., 95%)\noptimal_components = sum(cumulative_variance &lt;= 0.95) + 1\nprint('optimal_components is', optimal_components)\n\n\n\n\noptimal_components is 7\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Applying PCA\npca = PCA()\nlabels=scaled_data_df['Income group']\nscaled_data_df_1 = scaled_data_df.drop(['Income group'], axis=1)\n\npca_data = pca.fit_transform(scaled_data_df_1)\n\n# Determining the optimal number of principal components\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\n# Print the individual and cumulative explained variance\nfor i, (ind, cum) in enumerate(zip(explained_variance, cumulative_variance), 1):\n    print(f\"Component {i}: Individual variance = {ind:.4f}, Cumulative variance = {cum:.4f}\")\n\n# Plotting the explained variance\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Elbow Plot for PCA Explained Variance')\nplt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\nplt.axvline(x=np.argmax(cumulative_variance &gt;= 0.95) + 1, color='g', linestyle='--', label='Optimal Components for 95% Variance')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nComponent 1: Individual variance = 0.4647, Cumulative variance = 0.4647\nComponent 2: Individual variance = 0.1506, Cumulative variance = 0.6154\nComponent 3: Individual variance = 0.1384, Cumulative variance = 0.7538\nComponent 4: Individual variance = 0.0810, Cumulative variance = 0.8347\nComponent 5: Individual variance = 0.0522, Cumulative variance = 0.8869\nComponent 6: Individual variance = 0.0422, Cumulative variance = 0.9291\nComponent 7: Individual variance = 0.0344, Cumulative variance = 0.9635\nComponent 8: Individual variance = 0.0251, Cumulative variance = 0.9886\nComponent 9: Individual variance = 0.0114, Cumulative variance = 1.0000\n\n\n\n\n\nThe 2 plots above shows both the individual and cumulative explained variance by each principal component.\nThe first two principal component alone accounts for approximately 61.53% of the variance.\nAbout 96.34% of the variance is explained by the first 7 components.\nVisualize the data in the reduced dimension using PCA Reducing the data to the first two and three principal components\n\nlabels=scaled_data_df['Income group']\nscaled_data_df_1 = scaled_data_df.drop(['Income group'], axis=1)\n\n# Reducing the data to the first three principal components\npca_3 = PCA(n_components=3)\npca_3_data = pca_3.fit_transform(scaled_data_df_1)\n\n# Creating a DataFrame for the PCA results\npca_df = pd.DataFrame(data=pca_3_data, columns=['PC1', 'PC2', 'PC3'])\npca_df['Label'] = labels  # Add the labels to the DataFrame\n\n# Plotting the first two principal components with colors\nplt.figure(figsize=(10, 8))\nplt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Label'], cmap='viridis')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA: First Two Principal Components')\nplt.colorbar()  # Show color scale\nplt.grid(True)\nplt.show()\n    \n# Plotting the first three principal components with colors\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'], c=pca_df['Label'], cmap='viridis')\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')\nax.set_title('PCA: First Three Principal Components')\nfig.colorbar(scatter)  # Show color scale\nplt.show()\n# explained_variance.cumsum()\n\n\n\n\n\n\n\nFirst Two Principal Components: The scatter plot of the first two principal components shows how the data points are spread out. This plot can be useful to identify clusters or patterns, although it’s not immediately clear if distinct clusters are present in this case.\nFirst Three Principal Components: The 3D plot including the first three principal components gives a more detailed view. This additional dimension can sometimes reveal structures that are not visible in two dimensions."
  },
  {
    "objectID": "reduction.html#b.-t-sne-implementation-to-be-done-after-pca",
    "href": "reduction.html#b.-t-sne-implementation-to-be-done-after-pca",
    "title": "Dimensionality Reduction",
    "section": "B. t-SNE Implementation (to be done after PCA):",
    "text": "B. t-SNE Implementation (to be done after PCA):\nPerplexity is a critical parameter in the t-SNE algorithm that helps determine the scale at which the algorithm operates, influencing the balance between highlighting local versus global data structures. is about the expected density around a point rather than the number of clusters in the data. when Perplexity = 10, The algorithm will assume that each point has about 10 close neighbors, when Perplexity = 50, the algorithm will assume that each point has about 50 close neighbors. t-SNE is also an unsupervised technique that’s used for visualization of the feature space. Only pass the feature set to the t-SNE algorithm, The target variable is separate and can be used after t-SNE transformation to color the data points in the visualization.\nAlthough t-SNE doesn’t preserve global structures and distances, when we have labeled data, it can sometimes show whether classes or categories can be easily separated, which might imply that classification tasks could perform well.\n\nApply t-SNE.  Use scikit-learn’s TSNE class. Experiment with different perplexity values (common choices are between 5 and 50). Set the number of components to 2 for visualization purposes.\nExperiment with different perplexity values. Perplexity can be thought of as a measure of how to balance attention between local and global aspects of the data. Low perplexity values lead to more attention being paid to local aspects of the data while high perplexity values result in the global structure of the data being given more weight.\nVisualize the t-SNE results. plotting the two-dimensional t-SNE results.\nCompare with PCA results.\n\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nprint(scaled_data_df.head())\nlabels = scaled_data_df['Income group']\nunique_labels = labels.unique()\n\n# Standardizing the features\nX_std = StandardScaler().fit_transform(scaled_data_df.drop('Income group', axis=1))  # Make sure to exclude the label column\n\n# Applying t-SNE with different perplexity values\nperplexities = [2,5, 10, 20, 30,40,50]\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=3, perplexity=perplexity, random_state=42)\n    X_tsne = tsne.fit_transform(X_std)\n\n    # Visualization\n    for i, unique_label in enumerate(unique_labels):\n        # Select rows where the label equals the current unique label\n        idxs = labels == unique_label\n        plt.scatter(X_tsne[idxs, 0], X_tsne[idxs, 1], label=unique_label)\n    \n    plt.title(f't-SNE with Perplexity = {perplexity}')\n    plt.legend()\n    plt.show()\n\n   Schizophrenia (%)  Bipolar disorder (%)  Eating disorders (%)  \\\n0          -1.106590             -0.198086             -0.844308   \n1          -1.112626             -0.196941             -0.860632   \n2          -1.116943             -0.196017             -0.877464   \n3          -1.119318             -0.195070             -0.892593   \n4          -1.119686             -0.193730             -0.904771   \n\n   Anxiety disorders (%)  Depression (%)  Income group  \\\n0               0.750876        0.928233     -0.464925   \n1               0.751669        0.940304     -0.464925   \n2               0.752859        0.954143     -0.464925   \n3               0.752646        0.966422     -0.464925   \n4               0.751393        0.971739     -0.464925   \n\n   average_learning_Adjusted_of_school  Continent  GDP(2022)  GDP_per_capita  \n0                            -1.188637   -0.41766  -0.261241       -0.386936  \n1                            -1.188637   -0.41766  -0.261241       -0.386936  \n2                            -1.188637   -0.41766  -0.261241       -0.386936  \n3                            -1.188637   -0.41766  -0.261241       -0.386936  \n4                            -1.188637   -0.41766  -0.261241       -0.386936  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn summary, the t-SNE with a perplexity of 10 seems to be a good middle ground for my dataset, balancing local and global structures; offering a reasonable compromise between showing local groupings and fitting into the broader context of the entire dataset."
  },
  {
    "objectID": "reduction.html#evaluation-and-comparison",
    "href": "reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison:",
    "text": "Evaluation and Comparison:\n\nEvaluate the effectiveness of both techniques.\nGlobal vs. Local Structures: PCA tends to show the overall structure, while t-SNE can uncover local groupings and complex structures.\nClarity of Clusters: Determine which method shows clearer clustering. t-SNE might reveal clusters that PCA cannot, especially in complex datasets.\nPCA is effective for understanding the overall variance and structure of the data. It’s computationally efficient and provides a quick overview. t-SNE excels in revealing complex patterns and clusters, particularly useful for datasets where non-linear relationships are significant. However, it’s more computationally intensive and its results can vary with different perplexity values.\nCompare their visualization capabilities.\nPCA Global Structure Visualization: PCA is excellent at visualizing the global structure of the data. It reduces dimensions while preserving as much variance as possible, which is helpful for understanding the overall distribution and direction of the dataset. Linearity: The linear nature of PCA makes it suitable for datasets where relationships among variables are linear. The principal components are linear combinations of the original features, providing a straightforward interpretation. Limitation in Complexity: PCA might not be as effective in revealing complex, non-linear relationships. Clusters or patterns that are not linearly separable might be missed in PCA visualizations. Discuss trade-offs and scenarios for their use.\nt-SNE: Local Structure and Clusters: t-SNE excels in visualizing local structures and clusters, even when these clusters are formed through complex, non-linear relationships. It’s particularly good at separating distinct groups in the data. Non-Linearity: t-SNE can uncover patterns and relationships that are not apparent with linear methods like PCA. This makes t-SNE visualizations more intricate and revealing for certain types of datasets. Sensitivity to Parameters: The output of t-SNE can be highly sensitive to the choice of perplexity and other parameters. This can lead to varying results, making interpretation sometimes subjective.\nTrade-offs and Scenarios for Use \nPCA: Best Use Cases: When you need a quick overview of the data. In cases where linear relationships dominate. For preprocessing in machine learning, especially when model simplicity and computational efficiency are crucial. Trade-offs: PCA might oversimplify complex datasets by missing non-linear patterns. It’s not the best tool for datasets where clusters are formed through non-linear boundaries.\nt-SNE:  Best Use Cases: For detailed exploratory data analysis, especially when you suspect non-linear relationships in your data. In scenarios where distinguishing between different clusters or groups is more important than understanding the variance across the entire dataset. Trade-offs: t-SNE is computationally intensive and not suitable for very large datasets. The results can vary with different runs, requiring careful selection of parameters and interpretation of results. It’s not typically used as a preprocessing step for machine learning models due to its computational complexity and the difficulty in interpreting the transformed features."
  },
  {
    "objectID": "reduction.html#conclusions",
    "href": "reduction.html#conclusions",
    "title": "Dimensionality Reduction",
    "section": "Conclusions",
    "text": "Conclusions\nThe choice between PCA and t-SNE for visualization depends on the dataset characteristics and the specific goals of the analysis. PCA offers a fast, linear approach suitable for an overview and linearly separable data, while t-SNE provides a detailed, non-linear approach ideal for datasets with complex structures and relationships. The decision should align with the analysis objectives, considering the trade-offs in terms of computational efficiency, ease of interpretation, and the ability to uncover underlying patterns in the data."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data",
    "section": "",
    "text": "bella20000605"
  },
  {
    "objectID": "Data.html#github-user-name",
    "href": "Data.html#github-user-name",
    "title": "Data",
    "section": "",
    "text": "bella20000605"
  },
  {
    "objectID": "Data.html#github-project-repo-url",
    "href": "Data.html#github-project-repo-url",
    "title": "Data",
    "section": "Github project Repo URL",
    "text": "Github project Repo URL\nLink to Github"
  },
  {
    "objectID": "ARM.html#publication",
    "href": "ARM.html#publication",
    "title": "ARM and Networking",
    "section": "Publication",
    "text": "Publication\n(Kumbhare and Chobe 2014)"
  }
]