[
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data",
    "section": "",
    "text": "bella20000605"
  },
  {
    "objectID": "Data.html#github-user-name",
    "href": "Data.html#github-user-name",
    "title": "Data",
    "section": "",
    "text": "bella20000605"
  },
  {
    "objectID": "Data.html#github-project-repo-url",
    "href": "Data.html#github-project-repo-url",
    "title": "Data",
    "section": "Github project Repo URL",
    "text": "Github project Repo URL\n[link]download link"
  },
  {
    "objectID": "Gathering.html",
    "href": "Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Mental health Depression disorder Data. What are mental health disorder types?\nHow many people suffer these disorder types?\nHow many people in your country have depression?\nIs depression is higher in men or women?\nDoes depression leads to suicide? What percentage?\nWhich age group has more depression?\ndownload link\n\n\n\n\n\n\n\nShare of population with eating disorders. This is estimated as the total number of cases with eating disorders relative to the population of a country.\nThis indicator only includes anorexia nervosa and bulimia nervosa (i.e. it does not include binge eating disorder and other specified feeding or eating disorder (OSFED)).\ndownload link\n\n\n\n\n\n\n\nRespondents were asked how comfortable a local person would feel speaking about anxiety or depression with someone they know. This shows the share that gave each response.\n download link\n\n\n\n\n\n\n\nThe relationship between countries and their continents. download link"
  },
  {
    "objectID": "Gathering.html#download-data",
    "href": "Gathering.html#download-data",
    "title": "Data Gathering",
    "section": "",
    "text": "Mental health Depression disorder Data. What are mental health disorder types?\nHow many people suffer these disorder types?\nHow many people in your country have depression?\nIs depression is higher in men or women?\nDoes depression leads to suicide? What percentage?\nWhich age group has more depression?\ndownload link\n\n\n\n\n\n\n\nShare of population with eating disorders. This is estimated as the total number of cases with eating disorders relative to the population of a country.\nThis indicator only includes anorexia nervosa and bulimia nervosa (i.e. it does not include binge eating disorder and other specified feeding or eating disorder (OSFED)).\ndownload link\n\n\n\n\n\n\n\nRespondents were asked how comfortable a local person would feel speaking about anxiety or depression with someone they know. This shows the share that gave each response.\n download link\n\n\n\n\n\n\n\nThe relationship between countries and their continents. download link"
  },
  {
    "objectID": "Gathering.html#api",
    "href": "Gathering.html#api",
    "title": "Data Gathering",
    "section": "API",
    "text": "API\n\nDescription\nThe World Development Indicators (WDI) is the primary World Bank collection of development indicators, compiled from officially-recognized international sources. It presents the most current and accurate global development data available, and includes national, regional and global estimates.\n\n\n\nLabel\nValue\n\n\n\n\nFiscal Yeay\nFY21\n\n\nPublication Date\n(July 2020)\n\n\nLow Income Threshold\n$1,035 or less\n\n\nLower Middle Income Range\n$1,036 to $4,045\n\n\nUpper Middle Income Range\n$4,046 to $12,535\n\n\nHigh Income Threshold\n$12,536 or more\n\n\nNumber of economies\n189\n\n\nGNI Year\n2019\n\n\nTerm of Income Classification\nuntil 1 July 2021\n\n\n\nAPI link\n\n\nPython Api Code\nimport requests\nimport pandas as pd\nimport json\nimport csv\n# Define the API endpoint URL\nurl = \"https://datacatalogapi.worldbank.org/ddhxext/ResourceFileData?resource_unique_id=DR0090755&version_id=2023-09-27T16:44:25.8023254Z\"\n# Make the GET request\nresponse = requests.get(url)\n# Check the response status\nif response.status_code == 200:\n    data = response.json()  # If the response contains JSON data, it can be accessed using the .json() method\nelse:\n    print(\"Failed to retrieve data. Status code:\", response.status_code)\n# Extract column names from the \"MetaData\" section\ncolumns = [item['ColumnName'] for item in data[\"MetaData\"]]\n# Extract data from the \"Details\" section\ndetails = data[\"Details\"]\nrows = []\nfor chunk in details:\n    row=[]\n    for key,value in chunk.items():\n    # Check if the value is None and replace it with an empty string if needed\n        value = value if value is not None else ''\n        row.append(value)\n    rows.append(row)\n# Write data to a CSV file\nwith open('./Part2/Code/api_python.csv', 'w', newline='') as csvfile:\n    csv_writer = csv.writer(csvfile)\n    # Write the header row\n    csv_writer.writerow(columns)\n    # Write the data rows\n    csv_writer.writerows(rows)\nprint(\"CSV file 'api_python.csv' has been created.\")\n\n\nR Api Code\n\nlibrary(httr)\nlibrary(jsonlite)\nurl &lt;- \"https://datacatalogapi.worldbank.org/ddhxext/ResourceFileData?resource_unique_id=DR0090755&version_id=2023-09-27T16:44:25.8023254Z\"\n# Make the GET request\nresponse &lt;- GET(url)\nresponse.text &lt;- content(response, \"text\")\ndata &lt;- fromJSON(response.text)\ndetails &lt;- data$Details\n# To write this data to a CSV file\nwrite.csv(details, file = './api_r.csv', row.names = FALSE)"
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About Me",
    "section": "",
    "text": "Bella Shi is a currently a student studying in the graduate school of Art and Sciences in Georgetown university. She majored in software engineering and minored in English in undergraduate.In her lesuire time, She likes swimming, going to the gym, hiking as well as other outdoor activities.\nBella’s Website"
  },
  {
    "objectID": "Naive Bayes.html",
    "href": "Naive Bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a probabilistic machine learning model that’s based on Bayes’ Theorem. It’s called “naive” because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets.\n\n\nAt its core, Naive Bayes relies on Bayes’ Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For classification, Bayes’ Theorem is stated as:\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\\]\nHere,P(B|A) is the probability of hypothesis A given data B. P(B|A) is the probability of the data under the hypothesis, P(A) is the probability of the hypothesis before seeing the data, and P(B) is the probability of the data under any hypothesis.\n\n\n\nThe objective in Naive Bayes is to determine the probability of a label given a set of features, and then predict the label with the highest probability. In other words, we calculate the posterior probability of each class given the input features and predict the class for which the posterior probability is the highest.\n\n\n\nNaive Bayes aims to model the conditional probability of the class labels given the features in order to make predictions. It’s particularly advantageous in situations where the dimensionality of the input space is high, as it avoids the curse of dimensionality to a certain extent due to its feature independence assumption\n\n\n\nThere are several variants of the Naive Bayes classifier, each appropriate for a different type of dataset:\n\nGaussian Naive Bayes:\n\nUsed when features have a continuous distribution and an assumption of normal distribution (Gaussian) is reasonable. Appropriate for many real-world problems, like predicting whether a given email is spam or not.\n\nMultinomial Naive Bayes:\n\nUsed for discrete data where features represent counts or frequency counts of events. Often used in text classification, where the features are related to word counts or frequencies within the documents.\n\nBernoulli Naive Bayes:\n\nUsed when features are binary (i.e., they take only two values like true and false). Suitable for making predictions from binary features, like if a word occurs in a document or not, which is a common scenario in text classification problems."
  },
  {
    "objectID": "Naive Bayes.html#hw-3.2.0-introduction-to-naive-bayes",
    "href": "Naive Bayes.html#hw-3.2.0-introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a probabilistic machine learning model that’s based on Bayes’ Theorem. It’s called “naive” because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets.\n\n\nAt its core, Naive Bayes relies on Bayes’ Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For classification, Bayes’ Theorem is stated as:\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\\]\nHere,P(B|A) is the probability of hypothesis A given data B. P(B|A) is the probability of the data under the hypothesis, P(A) is the probability of the hypothesis before seeing the data, and P(B) is the probability of the data under any hypothesis.\n\n\n\nThe objective in Naive Bayes is to determine the probability of a label given a set of features, and then predict the label with the highest probability. In other words, we calculate the posterior probability of each class given the input features and predict the class for which the posterior probability is the highest.\n\n\n\nNaive Bayes aims to model the conditional probability of the class labels given the features in order to make predictions. It’s particularly advantageous in situations where the dimensionality of the input space is high, as it avoids the curse of dimensionality to a certain extent due to its feature independence assumption\n\n\n\nThere are several variants of the Naive Bayes classifier, each appropriate for a different type of dataset:\n\nGaussian Naive Bayes:\n\nUsed when features have a continuous distribution and an assumption of normal distribution (Gaussian) is reasonable. Appropriate for many real-world problems, like predicting whether a given email is spam or not.\n\nMultinomial Naive Bayes:\n\nUsed for discrete data where features represent counts or frequency counts of events. Often used in text classification, where the features are related to word counts or frequencies within the documents.\n\nBernoulli Naive Bayes:\n\nUsed when features are binary (i.e., they take only two values like true and false). Suitable for making predictions from binary features, like if a word occurs in a document or not, which is a common scenario in text classification problems."
  },
  {
    "objectID": "Naive Bayes.html#hw-3.2.1-prepare-your-data-for-naïve-bayes",
    "href": "Naive Bayes.html#hw-3.2.1-prepare-your-data-for-naïve-bayes",
    "title": "Naive Bayes",
    "section": "HW-3.2.1: Prepare your Data for Naïve Bayes",
    "text": "HW-3.2.1: Prepare your Data for Naïve Bayes\n1.Handle missing values\n2.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models.\n3.Split the data into training and test sets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\ndf_mental_health=pd.read_csv('./Data/mental_health.csv')\n\nCheck missing values in the dataset\n\nmissing_data=df_mental_health.isnull().sum()\nprint(missing_data)\n\nEconomy                                                             0\nCode                                                                0\nYear                                                                0\nSchizophrenia (%)                                                   0\nBipolar disorder (%)                                                0\nEating disorders (%)                                                0\nAnxiety disorders (%)                                               0\nDepression (%)                                                      0\nIncome group                                                       56\naverage_learning_Adjusted_of_school                               812\nContinent                                                         588\nGDP(2022)                                                         224\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent    2380\ndtype: int64\n\n\n\n1.Handle missing values\na.Dropping columns with more than 1000 null values\n(We don’t Remove Rows with Missing Values because although this is the simplest approach, but it can lead to a significant reduction in data size, which might not be ideal if the missing data is extensive.)\nso after this step, we dropped the column not_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\nthreshold = 1000  # Set the threshold for the minimum number of non-NA values required\ndf_mental_health = df_mental_health.dropna(axis=1, thresh=(len(df_mental_health) - threshold))\n# Display the columns remaining after the operation\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\nNr0=len(df_mental_health.index)\nNc0=len(df_mental_health.columns)\nprint(\"Nrows = \",Nr0,\"\\nNcol=\",Nc0,\"\\nMatrix entries = \", Nr0*Nc0)\n# missing_data1 = encoded_data1.isnull().sum()\n# print(missing_data1)\n\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school    812\nContinent                              588\nGDP(2022)                              224\ndtype: int64\nNrows =  5488 \nNcol= 12 \nMatrix entries =  65856\n\n\nb.Impute Missing Values:\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is a common strategy for numerical data.\nForward or Backward Fill: For time-series data, we can fill missing values with the next or previous values.\nPredictive Imputation: Use a machine learning algorithm to predict the missing values based on other data in the dataset.\nMean/Median Imputation: If the data is normally distributed, you might use the mean. If the distribution is skewed,the median might be more appropriate.\n\nimport seaborn as sns\n#1.plot the GDP(2022) in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['GDP(2022)'])\n#2.plot the average_learning_Adjusted_of_school in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n#3.although income group and df_mental_health also have a lot of missing values\n#  but we can not plot  because it is a categorical variable\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_33813/2527354667.py:4: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['GDP(2022)'])\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_33813/2527354667.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n\n\n&lt;Axes: xlabel='average_learning_Adjusted_of_school', ylabel='Density'&gt;\n\n\n\n\n\n\n\n\nso here we impute missing values using the mean for GDP(2022) and the median for average_learning_Adjusted_of_school (assuming the distributions are appropriate for these measures):\n\n# Impute missing values with the mean for GDP(2022)\ndf_mental_health['GDP(2022)'].fillna(df_mental_health['GDP(2022)'].mean(), inplace=True)\n\n# Impute missing values with the median for average_learning_Adjusted_of_school\ndf_mental_health['average_learning_Adjusted_of_school'].fillna(df_mental_health['average_learning_Adjusted_of_school'].median(), inplace=True)\n\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\ndf_mental_health\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school      0\nContinent                              588\nGDP(2022)                                0\ndtype: int64\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.160560\n0.697779\n0.101855\n4.828830\n4.071831\nLow income\n4.957542\nAsia\n14583.0\n\n\n1\nAfghanistan\nAFG\n1991\n0.160312\n0.697961\n0.099313\n4.829740\n4.079531\nLow income\n4.957542\nAsia\n14583.0\n\n\n2\nAfghanistan\nAFG\n1992\n0.160135\n0.698107\n0.096692\n4.831108\n4.088358\nLow income\n4.957542\nAsia\n14583.0\n\n\n3\nAfghanistan\nAFG\n1993\n0.160037\n0.698257\n0.094336\n4.830864\n4.096190\nLow income\n4.957542\nAsia\n14583.0\n\n\n4\nAfghanistan\nAFG\n1994\n0.160022\n0.698469\n0.092439\n4.829423\n4.099582\nLow income\n4.957542\nAsia\n14583.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5483\nZimbabwe\nZWE\n2013\n0.155670\n0.607993\n0.117248\n3.090168\n3.128192\nLower middle income\n6.799749\nAfrica\n20678.0\n\n\n5484\nZimbabwe\nZWE\n2014\n0.155993\n0.608610\n0.118073\n3.093964\n3.140290\nLower middle income\n6.799749\nAfrica\n20678.0\n\n\n5485\nZimbabwe\nZWE\n2015\n0.156465\n0.609363\n0.119470\n3.098687\n3.155710\nLower middle income\n6.799749\nAfrica\n20678.0\n\n\n5486\nZimbabwe\nZWE\n2016\n0.157111\n0.610234\n0.121456\n3.104294\n3.174134\nLower middle income\n6.799749\nAfrica\n20678.0\n\n\n5487\nZimbabwe\nZWE\n2017\n0.157963\n0.611242\n0.124443\n3.110926\n3.192789\nLower middle income\n6.799749\nAfrica\n20678.0\n\n\n\n\n5488 rows × 12 columns\n\n\n\n\nDrop Rows (‘Income group’) with Missing Values:\n\nThe ‘Income group’ and ‘Continent’ columns still have missing values. Before we proceed with the Naive Bayes classification, we need to address these missing values. Since ‘Income group’ will be our target variable for classification, we cannot impute it as we did with the continuous variables. Instead, we should remove the rows with missing ‘Income group’ labels.\nAs for the ‘Continent’ column, it might not be necessary for our classification model if we’re using economic and health indicators as features.(because we can just dont choose the feature in the feature selection). However, if we decide that continent information might be useful, we could either impute the missing values based on the ‘Economy’ column or drop the column if it’s not required.\n\n# Dropping rows where 'Income group' is missing since it's our target variable\ndf_mental_health = df_mental_health.dropna(subset=['Income group'])\n\n# # Dropping the 'Continent' column\ndf_mental_health = df_mental_health.drop('Continent', axis=1)\n\n# Verifying that there are no more missing values\nmissing_values_final = df_mental_health.isnull().sum()\nmissing_values_final, df_mental_health.shape\ndf_mental_health.to_csv('./Data/mental_health_DT.csv', index=False)\n\nThe dataset is now clean and ready for classification. We can proceed with the next step.\n\n# Save the cleaned dataframe to a new CSV file\n# df_mental_health.to_csv('./mental_health_cleaned_not_encoded.csv', index=False)\n\n3.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models and Split the data into training and test sets\n\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n\n# # # Load the cleaned dataset\n# # file_path = './Data/mental_health.csv'\n# # data = pd.read_csv(file_path)\n\n# data= df_mental_health\n# # Encoding the 'Income group' column to have numerical labels\n# label_encoder = LabelEncoder()\n# data['Income group'] = label_encoder.fit_transform(data['Income group'])\n\n# # Selecting features and target variable\n# X = data.drop(['Economy', 'Code', 'Income group', 'Year', 'Continent'], axis=1)\n# y = data['Income group']\n\n# # Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\n# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# # Checking the shape of the datasets\n# (X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)"
  },
  {
    "objectID": "Naive Bayes.html#hw-3.2.2-feature-selection",
    "href": "Naive Bayes.html#hw-3.2.2-feature-selection",
    "title": "Naive Bayes",
    "section": "HW-3.2.2: Feature selection",
    "text": "HW-3.2.2: Feature selection\nDecide which features are relevant for our classification task(Naive Bayes model) and drop the rest.\nThe ‘Income group’ is a categorical variable that could be used as a label for classification, so we predict the ‘Income group’ based on health and economic indicators.\nmethod: 1. EDA to find the correlation between the features and the target variable\nCorralation matrix\n\nprint(df_mental_health.columns)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# # Load the cleaned dataset\n# file_path = './Data/mental_health.csv'\n# data = pd.read_csv(file_path)\n\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    # 'Income group',\n    # 'Continent',\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group'])\nprint(encoded_data.head())\n# encoded_data.to_csv('./mental_health_encoded.csv', index=False)\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\nprint(encoded_data.head())\nprint(type(encoded_data))\n# print(new_correlation_matrix)\n\n\n\nIndex(['Economy', 'Code', 'Year', 'Schizophrenia (%)', 'Bipolar disorder (%)',\n       'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)',\n       'Income group', 'average_learning_Adjusted_of_school', 'GDP(2022)'],\n      dtype='object')\n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \\\n0              0.101855               4.828830        4.071831   \n1              0.099313               4.829740        4.079531   \n2              0.096692               4.831108        4.088358   \n3              0.094336               4.830864        4.096190   \n4              0.092439               4.829423        4.099582   \n\n   average_learning_Adjusted_of_school  GDP(2022)  Income group_High income  \\\n0                             4.957542    14583.0                         0   \n1                             4.957542    14583.0                         0   \n2                             4.957542    14583.0                         0   \n3                             4.957542    14583.0                         0   \n4                             4.957542    14583.0                         0   \n\n   Income group_Low income  Income group_Lower middle income  \\\n0                        1                                 0   \n1                        1                                 0   \n2                        1                                 0   \n3                        1                                 0   \n4                        1                                 0   \n\n   Income group_Upper middle income  \n0                                 0  \n1                                 0  \n2                                 0  \n3                                 0  \n4                                 0  \n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \\\n0              0.101855               4.828830        4.071831   \n1              0.099313               4.829740        4.079531   \n2              0.096692               4.831108        4.088358   \n3              0.094336               4.830864        4.096190   \n4              0.092439               4.829423        4.099582   \n\n   average_learning_Adjusted_of_school  GDP(2022)  Income group_High income  \\\n0                             4.957542    14583.0                         0   \n1                             4.957542    14583.0                         0   \n2                             4.957542    14583.0                         0   \n3                             4.957542    14583.0                         0   \n4                             4.957542    14583.0                         0   \n\n   Income group_Low income  Income group_Lower middle income  \\\n0                        1                                 0   \n1                        1                                 0   \n2                        1                                 0   \n3                        1                                 0   \n4                        1                                 0   \n\n   Income group_Upper middle income  \n0                                 0  \n1                                 0  \n2                                 0  \n3                                 0  \n4                                 0  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\nStrong Correlation: Values above 0.7 or below -0.7 suggest a strong correlation between the variables. Moderate Correlation: Values between 0.3 and 0.7 (or -0.3 and -0.7) suggest a moderate correlation. Weak Correlation: Values between 0 and 0.3 (or 0 and -0.3) suggest a weak correlation.\n\nPearson Correlation\n\nThe Pearson correlation coefficients you’ve provided show the linear relationship between each feature and the ‘Income group’ target variable. A coefficient close to 1 or -1 indicates a strong positive or negative linear relationship, respectively. A coefficient close to 0 suggests no linear relationship.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# print(df_mental_health.isnull().sum())\n# # Load the cleaned dataset df_mental_health\n\n# For Pearson Correlation, we would ideally have only numeric features and labels\n# so we will encode the categorical features (Income group and Continent) using LabelEncoder\n# Encoding the 'Income group' column to have numerical labels\nlabel_encoder = LabelEncoder()\ndf_mental_health['Income group'] = label_encoder.fit_transform(df_mental_health['Income group'])\n# df_mental_health['Economy'] = label_encoder.fit_transform(df_mental_health['Economy'])\n# # Encoding the 'Continent' column to have numerical labels\n# df_mental_health['Continent'] = label_encoder.fit_transform(df_mental_health['Continent'])\n\n# Compute the Pearson Correlation matrix\ncorrelation_matrix = df_mental_health.corr()\n# print(correlation_matrix)\n\n# Selecting only correlations with the target variable (assuming target is numeric)\ntarget_correlation = correlation_matrix['Income group'].sort_values(ascending=False)\n\n# Review the correlations and decide on a threshold for selecting features\nprint(target_correlation)\n\n# calculate the inter-correlations with other features\n# For multicollinearity, we could remove features that have a high correlation with other features\nthreshold = 0.8  # This is an example threshold, it may need to be adjusted\nhigh_correlation_pairs = []\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i+1, len(correlation_matrix.columns)):\n        if abs(correlation_matrix.iloc[i, j]) &gt; threshold:\n            high_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n\nprint(high_correlation_pairs)\n\nIncome group                           1.000000e+00\nYear                                   1.892221e-14\nGDP(2022)                             -9.632665e-03\nDepression (%)                        -2.334807e-01\nBipolar disorder (%)                  -2.752850e-01\naverage_learning_Adjusted_of_school   -3.443070e-01\nAnxiety disorders (%)                 -3.575221e-01\nSchizophrenia (%)                     -3.745985e-01\nEating disorders (%)                  -5.301981e-01\nName: Income group, dtype: float64\n[]\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_33813/3053976826.py:22: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = df_mental_health.corr()\n\n\nMale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.11%. The range of prevalence rates is quite broad, with the lowest around 0.03% and the highest at about 0.67%. The interquartile range (middle 50% of the data) spans from approximately 0.08% to 0.16%, indicating that half of the reported rates fall within this range.\nFemale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.21%, which is notably higher than that of males. The prevalence rates for females also show a broad range, from about 0.06% to 1.40%. The interquartile range for females is wider than for males, ranging from about 0.14% to 0.40%, reflecting greater variability in the rates reported for females.\nOutlier Identification:\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\nNow, we can decide which features to drop based on their correlation with the target and their inter-correlations with other features\nEating disorders (%): With a correlation coefficient of approximately -0.53, this feature has the strongest negative linear relationship with the ‘Income group’ and is likely to be the most informative for predicting income group.\nAnxiety disorders (%), Schizophrenia (%), average_learning_Adjusted_of_school, Bipolar disorder (%), and Depression (%): All of these features have moderate negative correlations with the ‘Income group’, meaning as these percentages increase, the likelihood of being in a higher income group decreases.\nGDP(2022) and Continent: This feature has a very weak negative correlation with the ‘Income group’, suggesting it might not be very useful for predicting the income group on its own. so we drop this feature\nYear: The correlation is effectively zero, indicating no linear relationship with the ‘Income group’. sp we drop this feature.\nSo in this example, features excluding ‘Year’ and ‘GDP(2022)’ and ’Continent” are used for classification.\nThe ‘Year’ column is dropped because it’s not relevant for classification, and ‘GDP(2022)’ and ‘Continent” are dropped because it has a very weak correlation with the ’Income group’ and is unlikely to be useful for classification.\n\nHW-3.2.3: Naïve Bayes (NB) with Labeled Record Data\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n# print(df_mental_health.info())\n# #save df_mental_health to csv\nprint(df_mental_health.head())\n\n\n# Selecting features and target variable\n# print(X.columns)\nX = df_mental_health.drop(['Economy', 'Code', 'Income group', 'Year','GDP(2022)'], axis=1)\ny = df_mental_health['Income group']\nprint(X.columns)\n\n# Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n# Checking the shape of the datasets\n(X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)\n\n# Train the Naïve Bayes model\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Validate the model\ny_val_pred = nb_classifier.predict(X_val)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nval_classification_report = classification_report(y_val, y_val_pred)\nval_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n\n# Visualize the confusion matrix\nsns.heatmap(val_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Validation Set')\nplt.show()\n\n# Evaluate the model using the validation set\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(f\"Classification Report for Validation Set:\\n{val_classification_report}\")\n\n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  Income group  \\\n0              0.101855               4.828830        4.071831             1   \n1              0.099313               4.829740        4.079531             1   \n2              0.096692               4.831108        4.088358             1   \n3              0.094336               4.830864        4.096190             1   \n4              0.092439               4.829423        4.099582             1   \n\n   average_learning_Adjusted_of_school  GDP(2022)  \n0                             4.957542    14583.0  \n1                             4.957542    14583.0  \n2                             4.957542    14583.0  \n3                             4.957542    14583.0  \n4                             4.957542    14583.0  \nIndex(['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)',\n       'Anxiety disorders (%)', 'Depression (%)',\n       'average_learning_Adjusted_of_school'],\n      dtype='object')\nValidation Accuracy: 0.6282208588957056\nClassification Report for Validation Set:\n              precision    recall  f1-score   support\n\n           0       0.88      0.56      0.69       272\n           1       0.63      0.86      0.73        94\n           2       0.66      0.62      0.64       227\n           3       0.46      0.62      0.53       222\n\n    accuracy                           0.63       815\n   macro avg       0.66      0.67      0.64       815\nweighted avg       0.67      0.63      0.63       815\n\n\n\n\n\n\n\n# Test the model\ny_test_pred = nb_classifier.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_classification_report = classification_report(y_test, y_test_pred)\ntest_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n\n# Visualize the confusion matrix for the test set\nsns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n# Evaluate the model using the test set\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Classification Report for Test Set:\\n{test_classification_report}\")\n\n\n\n\nTest Accuracy: 0.645398773006135\nClassification Report for Test Set:\n              precision    recall  f1-score   support\n\n           0       0.89      0.62      0.73       281\n           1       0.67      0.89      0.76       102\n           2       0.64      0.63      0.63       219\n           3       0.46      0.58      0.51       213\n\n    accuracy                           0.65       815\n   macro avg       0.66      0.68      0.66       815\nweighted avg       0.68      0.65      0.65       815"
  },
  {
    "objectID": "Naive Bayes.html#hw-3.2.4-naïve-bayes-nb-with-labeled-text-data",
    "href": "Naive Bayes.html#hw-3.2.4-naïve-bayes-nb-with-labeled-text-data",
    "title": "Naive Bayes",
    "section": "HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data",
    "text": "HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data\nExample1: predict the mental health condition based on the symptoms\n\nThe dataset is structured with two columns: ‘Symptoms’ and ‘Label’. load the dataset\nPreprocess the text data in the ‘Symptoms’ column. This includes cleaning and tokenizing the text, normalizing it, and removing stop words.\nEncode the ‘Label’ column to numerical values, as Naive Bayes requires numerical input.\nSplit the data into training and testing sets.\nTrain a Naive Bayes classifier on the training set.\nEvaluate the model on the testing set.\n\n\n# Load the data from the uploaded CSV file\ndf_mental_health = pd.read_csv(\"./Data/all_mental_health_symptoms_nb.csv\")\ndf_mental_health.head()  # Display the first few rows to understand the dataset structure\n\n\n\n\n\n\n\n\nSymptoms\nLabel\n\n\n\n\n0\nFeelings of sadness, tearfulness, emptiness or...\nDepression\n\n\n1\nAngry outbursts, irritability or frustration, ...\nDepression\n\n\n2\nLoss of interest or pleasure in most or all no...\nDepression\n\n\n3\nSleep disturbances, including insomnia or slee...\nDepression\n\n\n4\nTiredness and lack of energy, so even small ta...\nDepression\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Preprocessing the text data\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\nX = tfidf_vectorizer.fit_transform(df_mental_health['Symptoms']).toarray()\n\n# Encoding the 'Label' column\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df_mental_health['Label'])\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Training the Naive Bayes classifier\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluating the model calculating the testing accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f'testing accuracy:',accuracy)\n\n# Predicting the training set results to calculate the training accuracy\ny_train_pred = nb_classifier.predict(X_train)\n\n# Calculating the training accuracy\ntraining_accuracy = accuracy_score(y_train, y_train_pred)\ntraining_accuracy\nprint(f'training accuracy:',training_accuracy)\n\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\nprint(report)\n\n\n\ntesting accuracy: 0.3142857142857143\ntraining accuracy: 0.9873417721518988\n                  precision    recall  f1-score   support\n\n         Anxiety       0.27      0.80      0.40         5\nBipolar Disorder       0.50      0.25      0.33         8\n      Depression       0.20      0.12      0.15         8\n Eating Disorder       0.33      0.14      0.20         7\n   Schizophrenia       0.38      0.43      0.40         7\n\n        accuracy                           0.31        35\n       macro avg       0.33      0.35      0.30        35\n    weighted avg       0.34      0.31      0.29        35\n\n\n\nThe accuracy of the Naive Bayes model on the mental health symptoms dataset is not high: 0.31. This is likely due to the following factors:\n\nOverlap of Symptoms: Many mental health conditions have overlapping symptoms, which can confuse the model. For example, symptoms like “anxiety” or “sleep disturbances” can occur in multiple disorders.\n\nExample2: predict if it descrbes a mental health problem based on the symptoms Preprocess the “Symptoms” text data with vectorization.\nEncode the “Label” column into numerical values.\nSplit the data into a training set and a test set.\nTrain a Naive Bayes model. Evaluate the model on the test data.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n# Attempt to read the CSV file while skipping problematic lines\ntry:\n    new_data = pd.read_csv('./Data/symptoms.csv',on_bad_lines='skip')\n    # Display the first few rows of the dataframe and the number of rows skipped\n    new_first_rows = new_data.head()\n    new_num_rows = new_data.shape[0]\n    new_exception_message = None\nexcept Exception as e:\n    new_first_rows, new_num_rows, new_exception_message = None, None, str(e)\n\nnew_first_rows, new_num_rows, new_exception_message\n\n# Initialize the vectorizer and label encoder\nvectorizer = CountVectorizer()\nlabel_encoder = LabelEncoder()\n\n# Vectorize the 'Symptoms' text\nX = vectorizer.fit_transform(new_data['Symptoms'])\n\n# Encode the 'Label' column\ny = label_encoder.fit_transform(new_data['Label'])\n\n# Split the data into training and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\nprint(accuracy)\nprint(classification_rep)\n\n0.8157894736842105\n                 precision    recall  f1-score   support\n\n  Mental health       0.86      0.82      0.84        22\nPhysical health       0.76      0.81      0.79        16\n\n       accuracy                           0.82        38\n      macro avg       0.81      0.82      0.81        38\n   weighted avg       0.82      0.82      0.82        38"
  },
  {
    "objectID": "Cleaning.html",
    "href": "Cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code for cleaning the data set download link\nRaw Data Set download link\nCleaned Data Set download link"
  },
  {
    "objectID": "Cleaning.html#code",
    "href": "Cleaning.html#code",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code for cleaning the data set download link\nRaw Data Set download link\nCleaned Data Set download link"
  },
  {
    "objectID": "Cleaning.html#python-cleaning-process",
    "href": "Cleaning.html#python-cleaning-process",
    "title": "Data Cleaning",
    "section": "Python Cleaning Process",
    "text": "Python Cleaning Process\n\nLoad Data Set\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# import seaborn as sns\nimport glob\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#load data from csv file\nall_data_frames=glob.glob(\"./HW-02/Data/*.csv\")+glob.glob(\"./HW-02/Data/*.xlsx\")\nfor fname_index, fname in enumerate(all_data_frames):\n    print(fname_index, fname)\n\n0 ./HW-02/Data/education.csv\n1 ./HW-02/Data/Countries_Continents.csv\n2 ./HW-02/Data/mental_sports_nb.csv\n3 ./HW-02/Data/age_when_first_had_anxiety_depression.csv\n4 ./HW-02/Data/all_mental_health_symptoms.csv\n5 ./HW-02/Data/GDP_per_captita.csv\n6 ./HW-02/Data/discomfort_speaking_anxiety_depression_2020.csv\n7 ./HW-02/Data/eating_disorders_prevalence_males_vs_females.csv\n8 ./HW-02/Data/most_educated_countries_2023.csv\n9 ./HW-02/Data/GDP.csv\n10 ./HW-02/Data/Income_group.xlsx\n11 ./HW-02/Data/mental_health.xlsx\n\n\n\n\nMental Health Data\n\n\n#A.clean the data named mental_health\nmental_health = all_data_frames[11]\nmental_health_df = pd.read_excel(mental_health)  # Use the 'mental_health' variable\n# show all the columns of the dataframe\n# mental_health_df.columns\nprint(mental_health_df.head())\n\nvectorizer = CountVectorizer()\ncount=vectorizer.fit_transform(mental_health_df['Entity'].values.astype('U'))\nprint(count)\nprint('vocabulary list:',vectorizer.vocabulary_)\nprint('vocabulary size:',len(vectorizer.vocabulary_))\n\n# # drop the columns of the dataframes that do not belong to mental health data\nmental_health_df = mental_health_df.drop(columns=['Drug use disorders (%)', 'Alcohol use disorders (%)'])\nprint(mental_health_df.head())\n#save the dataframe to csv file\n# mental_health_df.to_csv(\"./Part2/Code/mental_health_CV.csv\", index=False)\n\n        Entity Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Drug use disorders (%)  \\\n0              0.101855               4.828830                1.677082   \n1              0.099313               4.829740                1.684746   \n2              0.096692               4.831108                1.694334   \n3              0.094336               4.830864                1.705320   \n4              0.092439               4.829423                1.716069   \n\n   Depression (%)  Alcohol use disorders (%)  \n0        4.071831                   0.672404  \n1        4.079531                   0.671768  \n2        4.088358                   0.670644  \n3        4.096190                   0.669738  \n4        4.099582                   0.669260  \n  (0, 0)    1\n  (1, 0)    1\n  (2, 0)    1\n  (3, 0)    1\n  (4, 0)    1\n  (5, 0)    1\n  (6, 0)    1\n  (7, 0)    1\n  (8, 0)    1\n  (9, 0)    1\n  (10, 0)   1\n  (11, 0)   1\n  (12, 0)   1\n  (13, 0)   1\n  (14, 0)   1\n  (15, 0)   1\n  (16, 0)   1\n  (17, 0)   1\n  (18, 0)   1\n  (19, 0)   1\n  (20, 0)   1\n  (21, 0)   1\n  (22, 0)   1\n  (23, 0)   1\n  (24, 0)   1\n  : :\n  (6443, 247)   1\n  (6444, 247)   1\n  (6445, 247)   1\n  (6446, 247)   1\n  (6447, 247)   1\n  (6448, 247)   1\n  (6449, 247)   1\n  (6450, 247)   1\n  (6451, 247)   1\n  (6452, 247)   1\n  (6453, 247)   1\n  (6454, 247)   1\n  (6455, 247)   1\n  (6456, 247)   1\n  (6457, 247)   1\n  (6458, 247)   1\n  (6459, 247)   1\n  (6460, 247)   1\n  (6461, 247)   1\n  (6462, 247)   1\n  (6463, 247)   1\n  (6464, 247)   1\n  (6465, 247)   1\n  (6466, 247)   1\n  (6467, 247)   1\nvocabulary list: {'afghanistan': 0, 'albania': 3, 'algeria': 4, 'american': 6, 'samoa': 188, 'andean': 8, 'latin': 122, 'america': 5, 'andorra': 9, 'angola': 10, 'antigua': 11, 'and': 7, 'barbuda': 25, 'argentina': 14, 'armenia': 15, 'australasia': 17, 'australia': 18, 'austria': 19, 'azerbaijan': 20, 'bahamas': 21, 'bahrain': 22, 'bangladesh': 23, 'barbados': 24, 'belarus': 26, 'belgium': 27, 'belize': 28, 'benin': 29, 'bermuda': 30, 'bhutan': 31, 'bolivia': 33, 'bosnia': 34, 'herzegovina': 95, 'botswana': 35, 'brazil': 36, 'brunei': 37, 'bulgaria': 38, 'burkina': 39, 'faso': 77, 'burundi': 40, 'cambodia': 41, 'cameroon': 42, 'canada': 43, 'cape': 44, 'verde': 237, 'caribbean': 45, 'central': 46, 'african': 2, 'republic': 179, 'asia': 16, 'europe': 76, 'eastern': 66, 'sub': 208, 'saharan': 185, 'africa': 1, 'chad': 47, 'chile': 48, 'china': 49, 'colombia': 50, 'comoros': 51, 'congo': 52, 'costa': 53, 'rica': 180, 'cote': 54, 'ivoire': 109, 'croatia': 56, 'cuba': 57, 'cyprus': 58, 'czech': 59, 'democratic': 60, 'of': 164, 'denmark': 61, 'djibouti': 62, 'dominica': 63, 'dominican': 64, 'east': 65, 'ecuador': 67, 'egypt': 68, 'el': 69, 'salvador': 187, 'england': 71, 'equatorial': 72, 'guinea': 92, 'eritrea': 73, 'estonia': 74, 'ethiopia': 75, 'fiji': 78, 'finland': 79, 'france': 80, 'gabon': 81, 'gambia': 82, 'georgia': 83, 'germany': 84, 'ghana': 85, 'greece': 86, 'greenland': 87, 'grenada': 88, 'guam': 90, 'guatemala': 91, 'bissau': 32, 'guyana': 93, 'haiti': 94, 'high': 96, 'sdi': 192, 'income': 100, 'pacific': 166, 'middle': 146, 'honduras': 97, 'hungary': 98, 'iceland': 99, 'india': 101, 'indonesia': 102, 'iran': 103, 'iraq': 104, 'ireland': 105, 'israel': 107, 'italy': 108, 'jamaica': 110, 'japan': 111, 'jordan': 112, 'kazakhstan': 113, 'kenya': 114, 'kiribati': 116, 'kuwait': 118, 'kyrgyzstan': 119, 'laos': 121, 'latvia': 123, 'lebanon': 124, 'lesotho': 126, 'liberia': 127, 'libya': 128, 'lithuania': 129, 'low': 130, 'luxembourg': 132, 'macedonia': 133, 'madagascar': 134, 'malawi': 135, 'malaysia': 136, 'maldives': 137, 'mali': 138, 'malta': 139, 'marshall': 141, 'islands': 106, 'mauritania': 142, 'mauritius': 143, 'mexico': 144, 'micronesia': 145, 'country': 55, 'moldova': 147, 'mongolia': 148, 'montenegro': 149, 'morocco': 150, 'mozambique': 151, 'myanmar': 152, 'namibia': 153, 'nepal': 154, 'netherlands': 155, 'new': 156, 'zealand': 246, 'nicaragua': 157, 'niger': 158, 'nigeria': 159, 'north': 160, 'korea': 117, 'northern': 161, 'mariana': 140, 'norway': 162, 'oceania': 163, 'oman': 165, 'pakistan': 167, 'palestine': 168, 'panama': 169, 'papua': 170, 'paraguay': 171, 'peru': 172, 'philippines': 173, 'poland': 174, 'portugal': 175, 'puerto': 177, 'rico': 181, 'qatar': 178, 'romania': 182, 'russia': 183, 'rwanda': 184, 'saint': 186, 'lucia': 131, 'vincent': 239, 'the': 219, 'grenadines': 89, 'sao': 189, 'tome': 223, 'principe': 176, 'saudi': 190, 'arabia': 13, 'scotland': 191, 'senegal': 193, 'serbia': 194, 'seychelles': 195, 'sierra': 196, 'leone': 125, 'singapore': 197, 'slovakia': 198, 'slovenia': 199, 'solomon': 200, 'somalia': 201, 'south': 202, 'sudan': 209, 'southeast': 203, 'southern': 204, 'spain': 205, 'sri': 206, 'lanka': 120, 'suriname': 210, 'swaziland': 211, 'sweden': 212, 'switzerland': 213, 'syria': 214, 'taiwan': 215, 'tajikistan': 216, 'tanzania': 217, 'thailand': 218, 'timor': 220, 'togo': 222, 'tonga': 224, 'trinidad': 225, 'tobago': 221, 'tropical': 226, 'tunisia': 227, 'turkey': 228, 'turkmenistan': 229, 'uganda': 230, 'ukraine': 231, 'united': 232, 'arab': 12, 'emirates': 70, 'kingdom': 115, 'states': 207, 'virgin': 240, 'uruguay': 233, 'uzbekistan': 234, 'vanuatu': 235, 'venezuela': 236, 'vietnam': 238, 'wales': 241, 'western': 242, 'world': 243, 'yemen': 244, 'zambia': 245, 'zimbabwe': 247}\nvocabulary size: 248\n        Entity Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \n0              0.101855               4.828830        4.071831  \n1              0.099313               4.829740        4.079531  \n2              0.096692               4.831108        4.088358  \n3              0.094336               4.830864        4.096190  \n4              0.092439               4.829423        4.099582  \n\n\n\n\nEating Disorders Data\n\neating_disorder = all_data_frames[7]\nprint(eating_disorder)\neating_disorder_df = pd.read_csv(eating_disorder) # Use the 'eating_disorder' variablev\n# show all the columns of the dataframe\nprint(eating_disorder_df.columns)\nprint(eating_disorder_df.head())\n# drop the columns Continent\neating_disorder_df = eating_disorder_df.drop(columns=['Continent','Population (historical estimates)'])\nprint(eating_disorder_df.shape)\n# only keep the rows of the dataframe when the column 'Year' is between 1990-2021\n# eating_disorder_df = eating_disorder_df.loc[eating_disorder_df['Year'] == range(1990,2022)]\neating_disorder_df = eating_disorder_df[eating_disorder_df['Year'].isin(range(1990,2022))]\nprint(eating_disorder_df.shape)\nrename_map={\n    'Entity':'Country',\n    'Code':'Country Code',\n    'Year':'Year',\n    'Eating disorders (share of population) - Sex: Male - Age: Age-standardized': 'Male',\n    'Eating disorders (share of population) - Sex: Female - Age: Age-standardized':'Female',\n}\neating_disorder_df.rename(columns=rename_map, inplace=True)\nprint(eating_disorder_df.head())\n#caculate the mean of the column Male and Female of the dataframe and save it to a new column named All_gender\neating_disorder_df['All_gender'] = eating_disorder_df[['Male', 'Female']].mean(axis=1)\nprint(eating_disorder_df.head())\n#save the dataframe to csv file\n# eating_disorder_df.to_csv(\"./Part2/Code/eating_disorder.csv\", index=False)\n\n./HW-02/Data/eating_disorders_prevalence_males_vs_females.csv\nIndex(['Entity', 'Code', 'Year', 'Eating_disorders_Male',\n       'Eating_disorders_Female', 'Population', 'Continent'],\n      dtype='object')\n        Entity      Code  Year  Eating_disorders_Male  \\\n0     Abkhazia  OWID_ABK  2015                    NaN   \n1  Afghanistan       AFG  1990               0.088487   \n2  Afghanistan       AFG  1991               0.086048   \n3  Afghanistan       AFG  1992               0.083625   \n4  Afghanistan       AFG  1993               0.081628   \n\n   Eating_disorders_Female  Population Continent  \n0                      NaN         NaN      Asia  \n1                 0.161867  10694804.0       NaN  \n2                 0.156910  10745168.0       NaN  \n3                 0.152412  12057436.0       NaN  \n4                 0.147938  14003764.0       NaN  \n\n\nKeyError: \"['Population (historical estimates)'] not found in axis\"\n\n\n\n\nDiscomfort Seaking Anxiety depression_2020 & Contries Continents\n\n#C.clean the data named discomfort_speaking_anxiety_depression_2020 and Contries_Continents\nContries_Continents=all_data_frames[1]\nContries_Continents_df = pd.read_csv(Contries_Continents) \nprint(Contries_Continents_df.head())\nprint(Contries_Continents_df.shape)\ndiscomfort_speaking_anxiety_depression_2020=all_data_frames[6]\ndiscomfort_speaking_anxiety_depression_2020_df = pd.read_csv(discomfort_speaking_anxiety_depression_2020)\nprint(discomfort_speaking_anxiety_depression_2020_df.head())\nprint(discomfort_speaking_anxiety_depression_2020_df.columns)\nprint(discomfort_speaking_anxiety_depression_2020_df.shape)\n#rename the column 'Entity' to 'Country' and 'Share - Question: mh5 - Someone local comfortable speaking about anxiety/depression with someone they know - Answer: Not at all comfortable - Gender: all - Age_group: all' to 'Not_Comfortable'\ndiscomfort_speaking_anxiety_depression_2020_df.rename(columns={'Entity':'Country','Share - Question: mh5 - Someone local comfortable speaking about anxiety/depression with someone they know - Answer: Not at all comfortable - Gender: all - Age_group: all':'Not_Comfortable'}, inplace=True)\n# merge the two dataframes\ndiscomfort_in_continent= discomfort_speaking_anxiety_depression_2020_df.merge(Contries_Continents_df, how='left', left_on='Country', right_on='Country')\nprint(discomfort_in_continent.head())\n##save the dataframe to csv file\n# discomfort_in_continent.to_csv(\"./Part2/Code/discomfort_in_continent.csv\", index=False)\n\n  Continent   Country\n0    Africa   Algeria\n1    Africa    Angola\n2    Africa     Benin\n3    Africa  Botswana\n4    Africa   Burkina\n(194, 2)\n      Entity Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n0                                          27.573915              \n1                                          11.403895              \n2                                          26.333920              \n3                                          26.522500              \n4                                          30.768505              \nIndex(['Entity', 'Code', 'Year',\n       'not_at_all_comfortable_speaking_anxiety_or_depression_percent'],\n      dtype='object')\n(122, 4)\n     Country Code  Year  \\\n0    Albania  ALB  2020   \n1    Algeria  DZA  2020   \n2  Argentina  ARG  2020   \n3       Asia  NaN  2020   \n4  Australia  AUS  2020   \n\n   not_at_all_comfortable_speaking_anxiety_or_depression_percent  \\\n0                                          27.573915               \n1                                          11.403895               \n2                                          26.333920               \n3                                          26.522500               \n4                                          30.768505               \n\n       Continent  \n0         Europe  \n1         Africa  \n2  South America  \n3            NaN  \n4        Oceania"
  },
  {
    "objectID": "Cleaning.html#r-cleaning-process",
    "href": "Cleaning.html#r-cleaning-process",
    "title": "Data Cleaning",
    "section": "R Cleaning Process",
    "text": "R Cleaning Process\n\nLoad Data Set\n\n# Load the required libraries\nlibrary(readxl)\nlibrary(dplyr)\n\n# List all data files\ndata_files &lt;- list.files(path = \"./Part2/Data\", full.names = TRUE)\n\n\n\nMental Health Data\n\n\n# Load mental_health data\nmental_health_file &lt;- data_files[11] # Assuming mental health data is at index 10\nmental_health_df &lt;- read_excel(mental_health_file)\nhead(mental_health_df)\n\n# Drop columns not related to mental health\nmental_health_df &lt;- mental_health_df %&gt;%\n  select(-c(`Drug use disorders (%)`, `Alcohol use disorders (%)`))\nhead(mental_health_df)\n\n# Save the cleaned data to CSV\n# write.csv(mental_health_df, file = \"./Part2/Code/mental_health_r.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Cleaning.html#visualization",
    "href": "Cleaning.html#visualization",
    "title": "Data Cleaning",
    "section": "Visualization",
    "text": "Visualization\nBefore \nAfter"
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html",
    "href": "HW-03/Code/ Naïve Bayes.html",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a probabilistic machine learning model that’s based on Bayes’ Theorem. It’s called “naive” because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, even if these features depend on each other. This simplification makes Naive Bayes easy to build and particularly useful for very large datasets."
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html#hw-3.2.1-prepare-your-data-for-naïve-bayes",
    "href": "HW-03/Code/ Naïve Bayes.html#hw-3.2.1-prepare-your-data-for-naïve-bayes",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "HW-3.2.1: Prepare your Data for Naïve Bayes",
    "text": "HW-3.2.1: Prepare your Data for Naïve Bayes\n1.Handle missing values\n2.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models.\n3.Split the data into training and test sets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\ndf_mental_health=pd.read_csv('../Data/mental_health.csv')\n\nCheck missing values in the dataset\n\nmissing_data=df_mental_health.isnull().sum()\nprint(missing_data)\n\nEconomy                                                             0\nCode                                                                0\nYear                                                                0\nSchizophrenia (%)                                                   0\nBipolar disorder (%)                                                0\nEating disorders (%)                                                0\nAnxiety disorders (%)                                               0\nDepression (%)                                                      0\nIncome group                                                       56\naverage_learning_Adjusted_of_school                               812\nContinent                                                         588\nGDP(2022)                                                         224\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent    2380\ndtype: int64\n\n\n\n1.Handle missing values\na.Dropping columns with more than 1000 null values\n(We don’t Remove Rows with Missing Values because although this is the simplest approach, but it can lead to a significant reduction in data size, which might not be ideal if the missing data is extensive.)\nso after this step, we dropped the column not_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\nthreshold = 1000  # Set the threshold for the minimum number of non-NA values required\ndf_mental_health = df_mental_health.dropna(axis=1, thresh=(len(df_mental_health) - threshold))\n# Display the columns remaining after the operation\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\nNr0=len(df_mental_health.index)\nNc0=len(df_mental_health.columns)\nprint(\"Nrows = \",Nr0,\"\\nNcol=\",Nc0,\"\\nMatrix entries = \", Nr0*Nc0)\n# missing_data1 = encoded_data1.isnull().sum()\n# print(missing_data1)\n\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school    812\nContinent                              588\nGDP(2022)                              224\ndtype: int64\nNrows =  5488 \nNcol= 12 \nMatrix entries =  65856\n\n\nb.Impute Missing Values:\nMean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column. This is a common strategy for numerical data.\nForward or Backward Fill: For time-series data, you can fill missing values with the next or previous values.\nPredictive Imputation: Use a machine learning algorithm to predict the missing values based on other data in the dataset.\nMean/Median Imputation: If the data is normally distributed, you might use the mean. If the distribution is skewed,the median might be more appropriate.\n\nimport seaborn as sns\n#plot the GDP(2022) in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['GDP(2022)'])\n#plot the average_learning_Adjusted_of_school in df_mental_health\nplt.figure(figsize=(10, 5))\nsns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_54217/2106952041.py:4: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['GDP(2022)'])\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_54217/2106952041.py:7: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(df_mental_health['average_learning_Adjusted_of_school'])\n\n\n&lt;Axes: xlabel='average_learning_Adjusted_of_school', ylabel='Density'&gt;\n\n\n\n\n\n\n\n\nso here we impute missing values using the mean for GDP(2022) and the median for average_learning_Adjusted_of_school (assuming the distributions are appropriate for these measures):\n\n# Impute missing values with the mean for GDP(2022)\ndf_mental_health['GDP(2022)'].fillna(df_mental_health['GDP(2022)'].mean(), inplace=True)\n\n# Impute missing values with the median for average_learning_Adjusted_of_school\ndf_mental_health['average_learning_Adjusted_of_school'].fillna(df_mental_health['average_learning_Adjusted_of_school'].median(), inplace=True)\n\nmissing_data = df_mental_health.isnull().sum()\nprint(missing_data)\n\nEconomy                                  0\nCode                                     0\nYear                                     0\nSchizophrenia (%)                        0\nBipolar disorder (%)                     0\nEating disorders (%)                     0\nAnxiety disorders (%)                    0\nDepression (%)                           0\nIncome group                            56\naverage_learning_Adjusted_of_school      0\nContinent                              588\nGDP(2022)                                0\ndtype: int64\n\n\n\nDrop Rows (‘Income group’) with Missing Values:\n\nThe ‘Income group’ and ‘Continent’ columns still have missing values. Before we proceed with the Naive Bayes classification, we need to address these missing values. Since ‘Income group’ will be our target variable for classification, we cannot impute it as we did with the continuous variables. Instead, we should remove the rows with missing ‘Income group’ labels.\nAs for the ‘Continent’ column, it might not be necessary for our classification model if we’re using economic and health indicators as features.(because we can just dont choose the feature in the feature selection). However, if we decide that continent information might be useful, we could either impute the missing values based on the ‘Economy’ column or drop the column if it’s not required.\n\n# Dropping rows where 'Income group' is missing since it's our target variable\ndf_mental_health = df_mental_health.dropna(subset=['Income group'])\n\n# # Dropping the 'Continent' column\n# data_cleaned = df_mental_health.drop('Continent', axis=1)\n\n# Verifying that there are no more missing values\nmissing_values_final = df_mental_health.isnull().sum()\nmissing_values_final, df_mental_health.shape\n\n(Economy                                  0\n Code                                     0\n Year                                     0\n Schizophrenia (%)                        0\n Bipolar disorder (%)                     0\n Eating disorders (%)                     0\n Anxiety disorders (%)                    0\n Depression (%)                           0\n Income group                             0\n average_learning_Adjusted_of_school      0\n Continent                              560\n GDP(2022)                                0\n dtype: int64,\n (5432, 12))\n\n\nThe dataset is now clean and ready for classification. We can proceed with the next step.\n\n# Save the cleaned dataframe to a new CSV file\ndf_mental_health.to_csv('./mental_health_cleaned_not_encoded.csv', index=False)\n\n3.Encode the ‘Income group’ and ‘Continent’ categorical variables into a numerical format suitable for machine learning models and Split the data into training and test sets\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the cleaned dataset\nfile_path = './mental_health_cleaned_not_encoded.csv'\ndata = pd.read_csv(file_path)\n\n# Encoding the 'Income group' column to have numerical labels\nlabel_encoder = LabelEncoder()\ndata['Income group'] = label_encoder.fit_transform(data['Income group'])\n\n# Selecting features and target variable\nX = data.drop(['Economy', 'Code', 'Income group', 'Year', 'Continent'], axis=1)\ny = data['Income group']\n\n# Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Checking the shape of the datasets\n(X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)\n\n(((3802, 7), (815, 7), (815, 7)), ((3802,), (815,), (815,)))"
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html#hw-3.2.2-feature-selection",
    "href": "HW-03/Code/ Naïve Bayes.html#hw-3.2.2-feature-selection",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "HW-3.2.2: Feature selection",
    "text": "HW-3.2.2: Feature selection\nDecide which features are relevant for our classification task(Naive Bayes model) and drop the rest.\nThe ‘Income group’ is a categorical variable that could be used as a label for classification, so we predict the ‘Income group’ based on health and economic indicators.\nmethod: 1. EDA to find the correlation between the features and the target variable (corralation matrix)\n\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    # 'Income group',\n    # 'Continent',\n\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\nprint(encoded_data.head())\nencoded_data.to_csv('./mental_health_encoded.csv', index=False)\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\nprint(encoded_data.head())\nprint(type(encoded_data))\nprint(new_correlation_matrix)\n\n\n\n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \\\n0              0.101855               4.828830        4.071831   \n1              0.099313               4.829740        4.079531   \n2              0.096692               4.831108        4.088358   \n3              0.094336               4.830864        4.096190   \n4              0.092439               4.829423        4.099582   \n\n   average_learning_Adjusted_of_school  GDP(2022)  Income group_High income  \\\n0                             4.957542    14583.0                         0   \n1                             4.957542    14583.0                         0   \n2                             4.957542    14583.0                         0   \n3                             4.957542    14583.0                         0   \n4                             4.957542    14583.0                         0   \n\n   Income group_Low income  Income group_Lower middle income  \\\n0                        1                                 0   \n1                        1                                 0   \n2                        1                                 0   \n3                        1                                 0   \n4                        1                                 0   \n\n   Income group_Upper middle income  Continent_Africa  Continent_Asia  \\\n0                                 0                 0               1   \n1                                 0                 0               1   \n2                                 0                 0               1   \n3                                 0                 0               1   \n4                                 0                 0               1   \n\n   Continent_Europe  Continent_North America  Continent_Oceania  \\\n0                 0                        0                  0   \n1                 0                        0                  0   \n2                 0                        0                  0   \n3                 0                        0                  0   \n4                 0                        0                  0   \n\n   Continent_South America  \n0                        0  \n1                        0  \n2                        0  \n3                        0  \n4                        0  \n       Economy Code  Year  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan  AFG  1990           0.160560              0.697779   \n1  Afghanistan  AFG  1991           0.160312              0.697961   \n2  Afghanistan  AFG  1992           0.160135              0.698107   \n3  Afghanistan  AFG  1993           0.160037              0.698257   \n4  Afghanistan  AFG  1994           0.160022              0.698469   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \\\n0              0.101855               4.828830        4.071831   \n1              0.099313               4.829740        4.079531   \n2              0.096692               4.831108        4.088358   \n3              0.094336               4.830864        4.096190   \n4              0.092439               4.829423        4.099582   \n\n   average_learning_Adjusted_of_school  GDP(2022)  Income group_High income  \\\n0                             4.957542    14583.0                         0   \n1                             4.957542    14583.0                         0   \n2                             4.957542    14583.0                         0   \n3                             4.957542    14583.0                         0   \n4                             4.957542    14583.0                         0   \n\n   Income group_Low income  Income group_Lower middle income  \\\n0                        1                                 0   \n1                        1                                 0   \n2                        1                                 0   \n3                        1                                 0   \n4                        1                                 0   \n\n   Income group_Upper middle income  Continent_Africa  Continent_Asia  \\\n0                                 0                 0               1   \n1                                 0                 0               1   \n2                                 0                 0               1   \n3                                 0                 0               1   \n4                                 0                 0               1   \n\n   Continent_Europe  Continent_North America  Continent_Oceania  \\\n0                 0                        0                  0   \n1                 0                        0                  0   \n2                 0                        0                  0   \n3                 0                        0                  0   \n4                 0                        0                  0   \n\n   Continent_South America  \n0                        0  \n1                        0  \n2                        0  \n3                        0  \n4                        0  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\n                                     Schizophrenia (%)  Bipolar disorder (%)  \\\nSchizophrenia (%)                             1.000000              0.240575   \nBipolar disorder (%)                          0.240575              1.000000   \nEating disorders (%)                          0.688793              0.704006   \nAnxiety disorders (%)                         0.472522              0.658057   \nDepression (%)                                0.125692              0.114153   \nGDP(2022)                                     0.369815             -0.059046   \naverage_learning_Adjusted_of_school           0.636316              0.429033   \nIncome group_High income                      0.585802              0.442182   \nIncome group_Low income                      -0.405724             -0.216036   \nIncome group_Lower middle income             -0.221720             -0.324419   \nIncome group_Upper middle income             -0.083432              0.027109   \nContinent_Africa                             -0.524780             -0.269195   \nContinent_Asia                                0.138752             -0.133996   \nContinent_Europe                              0.279973              0.342498   \nContinent_North America                      -0.039523              0.275602   \nContinent_Oceania                             0.209432             -0.190432   \nContinent_South America                      -0.072790              0.226848   \n\n                                     Eating disorders (%)  \\\nSchizophrenia (%)                                0.688793   \nBipolar disorder (%)                             0.704006   \nEating disorders (%)                             1.000000   \nAnxiety disorders (%)                            0.679320   \nDepression (%)                                   0.195924   \nGDP(2022)                                        0.131819   \naverage_learning_Adjusted_of_school              0.648532   \nIncome group_High income                         0.739056   \nIncome group_Low income                         -0.333568   \nIncome group_Lower middle income                -0.397374   \nIncome group_Upper middle income                -0.123462   \nContinent_Africa                                -0.393812   \nContinent_Asia                                  -0.122490   \nContinent_Europe                                 0.425994   \nContinent_North America                          0.074215   \nContinent_Oceania                                0.000394   \nContinent_South America                          0.085132   \n\n                                     Anxiety disorders (%)  Depression (%)  \\\nSchizophrenia (%)                                 0.472522        0.125692   \nBipolar disorder (%)                              0.658057        0.114153   \nEating disorders (%)                              0.679320        0.195924   \nAnxiety disorders (%)                             1.000000        0.338573   \nDepression (%)                                    0.338573        1.000000   \nGDP(2022)                                         0.085053        0.052842   \naverage_learning_Adjusted_of_school               0.353867       -0.013123   \nIncome group_High income                          0.459848        0.100652   \nIncome group_Low income                          -0.156747        0.191008   \nIncome group_Lower middle income                 -0.250900        0.015965   \nIncome group_Upper middle income                 -0.112419       -0.271149   \nContinent_Africa                                 -0.264615        0.271530   \nContinent_Asia                                   -0.049442       -0.024714   \nContinent_Europe                                  0.187011        0.001411   \nContinent_North America                           0.029437       -0.282403   \nContinent_Oceania                                 0.030380       -0.030521   \nContinent_South America                           0.229374       -0.056769   \n\n                                     GDP(2022)  \\\nSchizophrenia (%)                     0.369815   \nBipolar disorder (%)                 -0.059046   \nEating disorders (%)                  0.131819   \nAnxiety disorders (%)                 0.085053   \nDepression (%)                        0.052842   \nGDP(2022)                             1.000000   \naverage_learning_Adjusted_of_school   0.223242   \nIncome group_High income              0.083878   \nIncome group_Low income              -0.086646   \nIncome group_Lower middle income     -0.105537   \nIncome group_Upper middle income      0.085280   \nContinent_Africa                     -0.134962   \nContinent_Asia                        0.179471   \nContinent_Europe                      0.042354   \nContinent_North America              -0.042112   \nContinent_Oceania                    -0.032099   \nContinent_South America              -0.009233   \n\n                                     average_learning_Adjusted_of_school  \\\nSchizophrenia (%)                                               0.636316   \nBipolar disorder (%)                                            0.429033   \nEating disorders (%)                                            0.648532   \nAnxiety disorders (%)                                           0.353867   \nDepression (%)                                                 -0.013123   \nGDP(2022)                                                       0.223242   \naverage_learning_Adjusted_of_school                             1.000000   \nIncome group_High income                                        0.640737   \nIncome group_Low income                                        -0.504080   \nIncome group_Lower middle income                               -0.325065   \nIncome group_Upper middle income                                0.039467   \nContinent_Africa                                               -0.570805   \nContinent_Asia                                                  0.061933   \nContinent_Europe                                                0.560514   \nContinent_North America                                        -0.001007   \nContinent_Oceania                                              -0.055448   \nContinent_South America                                         0.031498   \n\n                                     Income group_High income  \\\nSchizophrenia (%)                                    0.585802   \nBipolar disorder (%)                                 0.442182   \nEating disorders (%)                                 0.739056   \nAnxiety disorders (%)                                0.459848   \nDepression (%)                                       0.100652   \nGDP(2022)                                            0.083878   \naverage_learning_Adjusted_of_school                  0.640737   \nIncome group_High income                             1.000000   \nIncome group_Low income                             -0.272814   \nIncome group_Lower middle income                    -0.430693   \nIncome group_Upper middle income                    -0.414144   \nContinent_Africa                                    -0.389138   \nContinent_Asia                                      -0.073196   \nContinent_Europe                                     0.437275   \nContinent_North America                             -0.029037   \nContinent_Oceania                                   -0.062101   \nContinent_South America                             -0.027233   \n\n                                     Income group_Low income  \\\nSchizophrenia (%)                                  -0.405724   \nBipolar disorder (%)                               -0.216036   \nEating disorders (%)                               -0.333568   \nAnxiety disorders (%)                              -0.156747   \nDepression (%)                                      0.191008   \nGDP(2022)                                          -0.086646   \naverage_learning_Adjusted_of_school                -0.504080   \nIncome group_High income                           -0.272814   \nIncome group_Low income                             1.000000   \nIncome group_Lower middle income                   -0.244323   \nIncome group_Upper middle income                   -0.234936   \nContinent_Africa                                    0.452508   \nContinent_Asia                                     -0.084071   \nContinent_Europe                                   -0.206793   \nContinent_North America                            -0.137062   \nContinent_Oceania                                  -0.091711   \nContinent_South America                            -0.096450   \n\n                                     Income group_Lower middle income  \\\nSchizophrenia (%)                                           -0.221720   \nBipolar disorder (%)                                        -0.324419   \nEating disorders (%)                                        -0.397374   \nAnxiety disorders (%)                                       -0.250900   \nDepression (%)                                               0.015965   \nGDP(2022)                                                   -0.105537   \naverage_learning_Adjusted_of_school                         -0.325065   \nIncome group_High income                                    -0.430693   \nIncome group_Low income                                     -0.244323   \nIncome group_Lower middle income                             1.000000   \nIncome group_Upper middle income                            -0.370894   \nContinent_Africa                                             0.230024   \nContinent_Asia                                               0.176326   \nContinent_Europe                                            -0.298539   \nContinent_North America                                     -0.105328   \nContinent_Oceania                                            0.115292   \nContinent_South America                                     -0.102536   \n\n                                     Income group_Upper middle income  \\\nSchizophrenia (%)                                           -0.083432   \nBipolar disorder (%)                                         0.027109   \nEating disorders (%)                                        -0.123462   \nAnxiety disorders (%)                                       -0.112419   \nDepression (%)                                              -0.271149   \nGDP(2022)                                                    0.085280   \naverage_learning_Adjusted_of_school                          0.039467   \nIncome group_High income                                    -0.414144   \nIncome group_Low income                                     -0.234936   \nIncome group_Lower middle income                            -0.370894   \nIncome group_Upper middle income                             1.000000   \nContinent_Africa                                            -0.170437   \nContinent_Asia                                              -0.036598   \nContinent_Europe                                            -0.001172   \nContinent_North America                                      0.244202   \nContinent_Oceania                                            0.019655   \nContinent_South America                                      0.208010   \n\n                                     Continent_Africa  Continent_Asia  \\\nSchizophrenia (%)                           -0.524780        0.138752   \nBipolar disorder (%)                        -0.269195       -0.133996   \nEating disorders (%)                        -0.393812       -0.122490   \nAnxiety disorders (%)                       -0.264615       -0.049442   \nDepression (%)                               0.271530       -0.024714   \nGDP(2022)                                   -0.134962        0.179471   \naverage_learning_Adjusted_of_school         -0.570805        0.061933   \nIncome group_High income                    -0.389138       -0.073196   \nIncome group_Low income                      0.452508       -0.084071   \nIncome group_Lower middle income             0.230024        0.176326   \nIncome group_Upper middle income            -0.170437       -0.036598   \nContinent_Africa                             1.000000       -0.299560   \nContinent_Asia                              -0.299560        1.000000   \nContinent_Europe                            -0.313921       -0.263675   \nContinent_North America                     -0.208067       -0.174764   \nContinent_Oceania                           -0.139222       -0.116939   \nContinent_South America                     -0.146416       -0.122981   \n\n                                     Continent_Europe  \\\nSchizophrenia (%)                            0.279973   \nBipolar disorder (%)                         0.342498   \nEating disorders (%)                         0.425994   \nAnxiety disorders (%)                        0.187011   \nDepression (%)                               0.001411   \nGDP(2022)                                    0.042354   \naverage_learning_Adjusted_of_school          0.560514   \nIncome group_High income                     0.437275   \nIncome group_Low income                     -0.206793   \nIncome group_Lower middle income            -0.298539   \nIncome group_Upper middle income            -0.001172   \nContinent_Africa                            -0.313921   \nContinent_Asia                              -0.263675   \nContinent_Europe                             1.000000   \nContinent_North America                     -0.183143   \nContinent_Oceania                           -0.122545   \nContinent_South America                     -0.128876   \n\n                                     Continent_North America  \\\nSchizophrenia (%)                                  -0.039523   \nBipolar disorder (%)                                0.275602   \nEating disorders (%)                                0.074215   \nAnxiety disorders (%)                               0.029437   \nDepression (%)                                     -0.282403   \nGDP(2022)                                          -0.042112   \naverage_learning_Adjusted_of_school                -0.001007   \nIncome group_High income                           -0.029037   \nIncome group_Low income                            -0.137062   \nIncome group_Lower middle income                   -0.105328   \nIncome group_Upper middle income                    0.244202   \nContinent_Africa                                   -0.208067   \nContinent_Asia                                     -0.174764   \nContinent_Europe                                   -0.183143   \nContinent_North America                             1.000000   \nContinent_Oceania                                  -0.081223   \nContinent_South America                            -0.085420   \n\n                                     Continent_Oceania  \\\nSchizophrenia (%)                             0.209432   \nBipolar disorder (%)                         -0.190432   \nEating disorders (%)                          0.000394   \nAnxiety disorders (%)                         0.030380   \nDepression (%)                               -0.030521   \nGDP(2022)                                    -0.032099   \naverage_learning_Adjusted_of_school          -0.055448   \nIncome group_High income                     -0.062101   \nIncome group_Low income                      -0.091711   \nIncome group_Lower middle income              0.115292   \nIncome group_Upper middle income              0.019655   \nContinent_Africa                             -0.139222   \nContinent_Asia                               -0.116939   \nContinent_Europe                             -0.122545   \nContinent_North America                      -0.081223   \nContinent_Oceania                             1.000000   \nContinent_South America                      -0.057156   \n\n                                     Continent_South America  \nSchizophrenia (%)                                  -0.072790  \nBipolar disorder (%)                                0.226848  \nEating disorders (%)                                0.085132  \nAnxiety disorders (%)                               0.229374  \nDepression (%)                                     -0.056769  \nGDP(2022)                                          -0.009233  \naverage_learning_Adjusted_of_school                 0.031498  \nIncome group_High income                           -0.027233  \nIncome group_Low income                            -0.096450  \nIncome group_Lower middle income                   -0.102536  \nIncome group_Upper middle income                    0.208010  \nContinent_Africa                                   -0.146416  \nContinent_Asia                                     -0.122981  \nContinent_Europe                                   -0.128876  \nContinent_North America                            -0.085420  \nContinent_Oceania                                  -0.057156  \nContinent_South America                             1.000000  \n\n\n\n\n\nStrong Correlation: Values above 0.7 or below -0.7 suggest a strong correlation between the variables. Moderate Correlation: Values between 0.3 and 0.7 (or -0.3 and -0.7) suggest a moderate correlation. Weak Correlation: Values between 0 and 0.3 (or 0 and -0.3) suggest a weak correlation.\n\nPearson Correlation\n\nThe Pearson correlation coefficients you’ve provided show the linear relationship between each feature and the ‘Income group’ target variable. A coefficient close to 1 or -1 indicates a strong positive or negative linear relationship, respectively. A coefficient close to 0 suggests no linear relationship.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming data is already loaded and appropriate for Pearson Correlation\n# For Pearson Correlation, we would ideally have only numeric features and labels\n\n# Compute the Pearson Correlation matrix\ncorrelation_matrix = data.corr()\n# print(correlation_matrix)\n\n# Selecting only correlations with the target variable (assuming target is numeric)\ntarget_correlation = correlation_matrix['Income group'].sort_values(ascending=False)\n\n# Review the correlations and decide on a threshold for selecting features\nprint(target_correlation)\n\n# For multicollinearity, we could remove features that have a high correlation with other features\nthreshold = 0.8  # This is an example threshold, it may need to be adjusted\nhigh_correlation_pairs = []\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i+1, len(correlation_matrix.columns)):\n        if abs(correlation_matrix.iloc[i, j]) &gt; threshold:\n            high_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n\n# Now, you can decide which features to drop based on their correlation with the target\n# and their inter-correlations with other features\n\nIncome group                           1.000000e+00\nYear                                   1.892221e-14\nGDP(2022)                             -9.632665e-03\nDepression (%)                        -2.334807e-01\nBipolar disorder (%)                  -2.752850e-01\naverage_learning_Adjusted_of_school   -3.443070e-01\nAnxiety disorders (%)                 -3.575221e-01\nSchizophrenia (%)                     -3.745985e-01\nEating disorders (%)                  -5.301981e-01\nName: Income group, dtype: float64\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_54217/2072872300.py:8: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = data.corr()\n\n\nEating disorders (%): With a correlation coefficient of approximately -0.52, this feature has the strongest negative linear relationship with the ‘Income group’ and is likely to be the most informative for predicting income group.\nAnxiety disorders (%), Schizophrenia (%), average_learning_Adjusted_of_school, Bipolar disorder (%), and Depression (%): All of these features have moderate negative correlations with the ‘Income group’, meaning as these percentages increase, the likelihood of being in a higher income group decreases.\nGDP(2022): This feature has a very weak negative correlation with the ‘Income group’, suggesting it might not be very useful for predicting the income group on its own.\nYear: The correlation is effectively zero, indicating no linear relationship with the ‘Income group’.\n3.Chisquare test\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n# Import the necessary libraries\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Initialize the SelectKBest with chi-squared score function\n# Using 'k='all'' to select all features for now, can change to a specific number based on the chi-squared scores\nkbest = SelectKBest(score_func=chi2, k='all')\n\n# Fit the SelectKBest to the training data and transform it\nX_train_kbest = kbest.fit_transform(X_train, y_train)\n\n# Transform the validation and test data\nX_val_kbest = kbest.transform(X_val)\nX_test_kbest = kbest.transform(X_test)\n\n# Get the scores for each feature\nfeature_scores = kbest.scores_\n\n# Create a DataFrame to view the scores and columns\nfeature_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Score': feature_scores})\n\n# Print the feature scores\nprint(feature_scores_df.sort_values(by='Score', ascending=False))\n\n                               Feature         Score\n6                            GDP(2022)  4.832444e+08\n5  average_learning_Adjusted_of_school  1.590596e+03\n3                Anxiety disorders (%)  2.765138e+02\n2                 Eating disorders (%)  2.371816e+02\n4                       Depression (%)  5.058270e+01\n1                 Bipolar disorder (%)  3.730221e+01\n0                    Schizophrenia (%)  1.404252e+01\n\n\nIn this example, features excluding ‘Year’ and ‘GDP(2022)’ are used for classification. The ‘Year’ column is dropped because it’s not relevant for classification, and ‘GDP(2022)’ is dropped because it has a very weak correlation with the ‘Income group’ and is unlikely to be useful for classification.\n\nHW-3.2.3: Naïve Bayes (NB) with Labeled Record Data\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Selecting features and target variable\n# print(X.columns)\nX = X.drop(['GDP(2022)', 'average_learning_Adjusted_of_school'], axis=1)\n\n# Splitting the dataset into training (70%), validation (15%), and testing (15%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# # Checking the shape of the datasets\n(X_train.shape, X_val.shape, X_test.shape), (y_train.shape, y_val.shape, y_test.shape)\n\n\n# Train the Naïve Bayes model\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Validate the model\ny_val_pred = nb_classifier.predict(X_val)\nval_accuracy = accuracy_score(y_val, y_val_pred)\nval_classification_report = classification_report(y_val, y_val_pred)\nval_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n\n# Visualize the confusion matrix\nsns.heatmap(val_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Validation Set')\nplt.show()\n\n# Evaluate the model using the validation set\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(f\"Classification Report for Validation Set:\\n{val_classification_report}\")\n\n\n\n\nValidation Accuracy: 0.5901840490797546\nClassification Report for Validation Set:\n              precision    recall  f1-score   support\n\n           0       0.84      0.58      0.69       272\n           1       0.60      0.83      0.70        94\n           2       0.57      0.58      0.58       227\n           3       0.42      0.50      0.46       222\n\n    accuracy                           0.59       815\n   macro avg       0.61      0.63      0.61       815\nweighted avg       0.62      0.59      0.60       815\n\n\n\n\n# Test the model\ny_test_pred = nb_classifier.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_classification_report = classification_report(y_test, y_test_pred)\ntest_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n\n# Visualize the confusion matrix for the test set\nsns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n# Evaluate the model using the test set\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Classification Report for Test Set:\\n{test_classification_report}\")\n\n\n\n\nTest Accuracy: 0.6024539877300613\nClassification Report for Test Set:\n              precision    recall  f1-score   support\n\n           0       0.84      0.63      0.72       281\n           1       0.62      0.84      0.71       102\n           2       0.54      0.59      0.56       219\n           3       0.44      0.46      0.45       213\n\n    accuracy                           0.60       815\n   macro avg       0.61      0.63      0.61       815\nweighted avg       0.63      0.60      0.61       815"
  },
  {
    "objectID": "HW-03/Code/ Naïve Bayes.html#hw-3.2.4-naïve-bayes-nb-with-labeled-text-data",
    "href": "HW-03/Code/ Naïve Bayes.html#hw-3.2.4-naïve-bayes-nb-with-labeled-text-data",
    "title": "HW-3.2.0: Introduction to Naive Bayes",
    "section": "HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data",
    "text": "HW-3.2.4: Naïve Bayes (NB) with Labeled Text Data\nLabel the data, either manually or through automated methods, based on your classification goals. Preprocess the text data by cleaning and tokenizing the text, normalizing it, and removing stop words. Extract features using methods like TF-IDF or count vectorization. Train your Naive Bayes model, typically a Multinomial Naive Bayes for text data. Evaluate and refine your model, testing its accuracy and other metrics on a separate test set.\nload the data\n\n# Load the data from the uploaded CSV file\ndf_mental_health = pd.read_csv(\"../Data/all_mental_health_symptoms_nb.csv\")\ndf_mental_health.head()  # Display the first few rows to understand the dataset structure\n\n\n\n\n\n\n\n\nSymptoms\nLabel\n\n\n\n\n0\nFeelings of sadness, tearfulness, emptiness or...\nDepression\n\n\n1\nAngry outbursts, irritability or frustration, ...\nDepression\n\n\n2\nLoss of interest or pleasure in most or all no...\nDepression\n\n\n3\nSleep disturbances, including insomnia or slee...\nDepression\n\n\n4\nTiredness and lack of energy, so even small ta...\nDepression\n\n\n\n\n\n\n\nThe dataset is structured with two columns: ‘Symptoms’ and ‘Label’. Preprocess the text data in the ‘Symptoms’ column. Encode the ‘Label’ column to numerical values, as Naive Bayes requires numerical input. Split the data into training and testing sets. Train a Naive Bayes classifier on the training set. Evaluate the model on the testing set.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Preprocessing the text data\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\nX = tfidf_vectorizer.fit_transform(df_mental_health['Symptoms']).toarray()\n\n# Encoding the 'Label' column\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df_mental_health['Label'])\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Training the Naive Bayes classifier\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluating the model calculating the testing accuracy\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f'testing accuracy:',accuracy)\n\n# Predicting the training set results to calculate the training accuracy\ny_train_pred = nb_classifier.predict(X_train)\n\n# Calculating the training accuracy\ntraining_accuracy = accuracy_score(y_train, y_train_pred)\ntraining_accuracy\nprint(f'training accuracy:',training_accuracy)\n\nreport = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n\nprint(report)\n\n\n\ntesting accuracy: 0.3142857142857143\ntraining accuracy: 0.9873417721518988\n                  precision    recall  f1-score   support\n\n         Anxiety       0.27      0.80      0.40         5\nBipolar Disorder       0.50      0.25      0.33         8\n      Depression       0.20      0.12      0.15         8\n Eating Disorder       0.33      0.14      0.20         7\n   Schizophrenia       0.38      0.43      0.40         7\n\n        accuracy                           0.31        35\n       macro avg       0.33      0.35      0.30        35\n    weighted avg       0.34      0.31      0.29        35\n\n\n\nThe accuracy of the Naive Bayes model on the mental health symptoms dataset is not high: 0.56. This is likely due to the following factors: Overlap of Symptoms: Many mental health conditions have overlapping symptoms, which can confuse the model. For example, symptoms like “anxiety” or “sleep disturbances” can occur in multiple disorders.\nPreprocess the “Symptoms” text data with vectorization.\nEncode the “Label” column into numerical values.\nSplit the data into a training set and a test set.\nTrain a Naive Bayes model. Evaluate the model on the test data.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n# Attempt to read the CSV file while skipping problematic lines\ntry:\n    new_data = pd.read_csv('../Data/mental_sports_nb.csv',on_bad_lines='skip')\n    # Display the first few rows of the dataframe and the number of rows skipped\n    new_first_rows = new_data.head()\n    new_num_rows = new_data.shape[0]\n    new_exception_message = None\nexcept Exception as e:\n    new_first_rows, new_num_rows, new_exception_message = None, None, str(e)\n\nnew_first_rows, new_num_rows, new_exception_message\n\n# Initialize the vectorizer and label encoder\nvectorizer = CountVectorizer()\nlabel_encoder = LabelEncoder()\n\n# Vectorize the 'Symptoms' text\nX = vectorizer.fit_transform(new_data['Symptoms'])\n\n# Encode the 'Label' column\ny = label_encoder.fit_transform(new_data['Label'])\n\n# Split the data into training and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a Naive Bayes classifier\nnb_classifier = MultinomialNB()\n\n# Train the classifier\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\nprint(accuracy)\nprint(classification_rep)\n\n0.9565217391304348\n               precision    recall  f1-score   support\n\nMental Health       0.92      1.00      0.96        12\n       Sports       1.00      0.91      0.95        11\n\n     accuracy                           0.96        23\n    macro avg       0.96      0.95      0.96        23\n weighted avg       0.96      0.96      0.96        23"
  },
  {
    "objectID": "HW-04/HW-4.html",
    "href": "HW-04/HW-4.html",
    "title": "Homework-4",
    "section": "",
    "text": "During module-4, we will focus on partition clustering (k-means), hierarchical clustering (with dendrograms), and density clustering (DBSCAN), as well as dimensionality reduction. There are other clustering options as well and you can, and should, always do more than the baseline requirements!\nWe will also explore different distance (or similarity) measures, visualizations, methods for choosing the number of clusters (such as elbow and silhouette methods), and how to interpret the results of clustering analysis.\nBefore getting started, recall the following fundamental concepts:\n\nSupervised learning:\n\nIn supervised learning we try to learn some KNOWN mapping from an input feature space \\(\\mathbf X\\) to an output target space \\(Y\\) (i.e. regression or classification).\n\nUn-supervised learning:\n\nTypically when doing unsupervised learning (i.e. clustering or dimensionality reduction) you DO NOT know if any relevant mappings (relationships) in the data.\nIn this case, there is NO KNOWN “target” space \\(Y\\) containing the “ground truth” labels (targets).\nInstead, you have some features space \\(\\mathbf X\\) and you are trying to systematically figure out if the data in that space forms groups (clusters).\nThis “un-labeled” nature of the data-set is what makes it “unsupervised”.\nIt is possible to already have some categorical labels in your data before clustering. In this case, the clustering algorithm may or may-not find clusters corresponding to the existing labels.\nIf it finds new labels, this suggests that there may be unknown groups in the data which are NOT associated with the existing labels.\n\n\n\n\nSoftware: For this assignment you MUST use Python. However, you can, and should, also repeat the exercise in R if you want.\n\n\nThe project’s objective is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex, multimodal data while preserving essential information and enhancing data visualization.\nThis project will allow you to apply your knowledge of PCA and t-SNE to a real-world scenario and gain valuable experience in dimensionality reduction and data visualization. It’s an opportunity to showcase your skills and creativity in tackling complex data analysis challenges.\nInstructions:\n\nDimensionality Reduction with PCA:\n\nApply PCA to your record dataset.\nDetermine the optimal number of principal components to retain.\nVisualize the reduced-dimensional data using PCA.\nAnalyze and interpret the results.\n\nDimensionality Reduction with t-SNE:\n\nImplement t-SNE on the same dataset.\nExplore different perplexity values and their impact.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare t-SNE results with PCA results.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in terms of preserving data structure and information.\nCompare the visualization capabilities of PCA and t-SNE.\nDiscuss the trade-offs and scenarios where one technique may outperform the other.\n\n\nComponents:\n\nProject Proposal:\n\nA brief proposal outlining your project’s objectives, dataset selection, and the tools or libraries you plan to use (e.g., Python, scikit-learn).\n\nCode Implementation:\n\nPython code for implementing PCA and t-SNE on the selected dataset.\nCode should include parameter tuning for t-SNE (perplexity) and visualization of the results.\n\nProject Report:\n\nA comprehensive project report detailing the steps taken, results obtained, and your analysis.\nInclude visualizations, comparisons, and insights gained from the dimensionality reduction techniques.\n\n\n\n\n\nIn this homework you will choose appropriate portions of one or more of your datasets and apply k-means, DBSCAN, and Hierarchical clustering.\nYou can do this for either your record data OR text data, you can choose which data-set you prefer to work with. You can also do both.\nYou will also need to document the process on your clustering tab on your GU-domains website.\nIn the clustering tab please have the following sub-sections\n\nIntroduction:\n\nProvided brief summary (1 to 2 paragraphs) about your feature data \\(X\\), and what you are trying to achieve with your clustering analysis.\n\nTheory:\n\nWrite a brief technical write up about how EACH clustering method works (KMEAN, DBSAN, hierarchical clustering). Also include details on model selection methods that you use (elbow, silhouette, etc).\nDon’t go “too deep”, around 2 to 4 paragraphs, per method, is fine. Write it in a way that a boss with a non-technical background would understand. Describe the method from a “big picture” point of view, how it works and what it is supposed to do.\n\nDo this “in your own words”. DO NOT copy this section from any other source. If you do so you will receive a zero on the assignment and will be referred to the department for further disciplinary action.\n\nMethods:\n\nIn this section, describe your coding workflow;\n\n\nIf you are not using Quarto, then provide links to the relevant code in this section.\n\n\nIf you are using Quarto (recommended), then include your code in-line using the “code folding” option, so that users can toggle the code on/off as needed. The notebook should also be hosted on Github. You can always do the entire thing in .ipynb then render it to HTML using Quarto at the end.\n\n\nPlease use the following workflow\n\nData selection: If you have not done so already, create either a numeric record feature dataset \\(\\mathbf X\\) AND/OR a text feature dataset \\(\\mathbf X\\) from your existing data.\n\nRemove the labels (targets) \\(Y\\) as needed so that it is suitable for clustering.\nIf you have labels, you can use them at the end, to check if the clustering predictions coincided with the existing labels in the data-set. In general this may or may-not be true.\nBut these labels should not be used as part of the clustering analysis.\n\nFeature selection: (optional)\n\nIf you want, you can perform filter based feature selection on your data-set \\(\\mathbf X\\) as a pre-processing step before clustering\nYou can also use optimal feature sets obtained during previous assignments\n\nHyper-parameter tuning\n\nFor each of the three clustering algorithms, perform any relevant parameter tuning in an attempt to achieve the optimal clustering results\ne.g. For k-means, Use Elbow and Silhouette methods to illustrate the ideal number of clusters. Visualize your results.\nAlso, when relevant, explore different choices of distance metric for the algorithm. Which distance metric seems to works best in which cases and why?\n\nFinal results\n\nOnce you have everything “dialed in”, re-do the analysis one last time with the optimal parameter choice to get your “final results”.\n\n\n\nResults:\n\nUsing your “final results”, discuss, illustrate, and compare the results of your various clustering analysis methods.\nWhich method seemed to work the best and why, which was easier to use or preferable, etc.\nCan you make connections between the optimal cluster predictions, after parameter tuning, with any of the labels in the data set. Do they coincide? Why or why not?\nDid the clustering results provide any new insights into your data?\nExplore the results, and create as many meaningful visualizations as you need. Be creative, and experiment with different image aesthetics.\nEnsure all visualizations are professional, ascetically pleasing, labeled, captioned, use color, are clear, and support your discussion and goals.\n\nConclusions:\n\nIn this section, the goal is to summarize & wrap-up the report. It explains what was found, in a way that would make sense to a general readership.\nThis area is non-technical. Technical descriptions of what you did should be in the methods or results sections, not conclusions.\nThe Conclusions should focus on key and important findings and how these findings affect real-life and real people.\n\nReferences:\n\nReference all non-original content.\nIdeally (but optionally) use .bibtex combined with Quarto to provide in-line internal citations\nSee the following link for an example:\n\nhttps://drive.google.com/open?id=12tYQnDuHS4ZxSTXgwsR4pRLhBYAMy14q&authuser=jh2343%40georgetown.edu&usp=drive_fs"
  },
  {
    "objectID": "HW-04/HW-4.html#assignment",
    "href": "HW-04/HW-4.html#assignment",
    "title": "Homework-4",
    "section": "",
    "text": "Software: For this assignment you MUST use Python. However, you can, and should, also repeat the exercise in R if you want.\n\n\nThe project’s objective is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex, multimodal data while preserving essential information and enhancing data visualization.\nThis project will allow you to apply your knowledge of PCA and t-SNE to a real-world scenario and gain valuable experience in dimensionality reduction and data visualization. It’s an opportunity to showcase your skills and creativity in tackling complex data analysis challenges.\nInstructions:\n\nDimensionality Reduction with PCA:\n\nApply PCA to your record dataset.\nDetermine the optimal number of principal components to retain.\nVisualize the reduced-dimensional data using PCA.\nAnalyze and interpret the results.\n\nDimensionality Reduction with t-SNE:\n\nImplement t-SNE on the same dataset.\nExplore different perplexity values and their impact.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare t-SNE results with PCA results.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in terms of preserving data structure and information.\nCompare the visualization capabilities of PCA and t-SNE.\nDiscuss the trade-offs and scenarios where one technique may outperform the other.\n\n\nComponents:\n\nProject Proposal:\n\nA brief proposal outlining your project’s objectives, dataset selection, and the tools or libraries you plan to use (e.g., Python, scikit-learn).\n\nCode Implementation:\n\nPython code for implementing PCA and t-SNE on the selected dataset.\nCode should include parameter tuning for t-SNE (perplexity) and visualization of the results.\n\nProject Report:\n\nA comprehensive project report detailing the steps taken, results obtained, and your analysis.\nInclude visualizations, comparisons, and insights gained from the dimensionality reduction techniques.\n\n\n\n\n\nIn this homework you will choose appropriate portions of one or more of your datasets and apply k-means, DBSCAN, and Hierarchical clustering.\nYou can do this for either your record data OR text data, you can choose which data-set you prefer to work with. You can also do both.\nYou will also need to document the process on your clustering tab on your GU-domains website.\nIn the clustering tab please have the following sub-sections\n\nIntroduction:\n\nProvided brief summary (1 to 2 paragraphs) about your feature data \\(X\\), and what you are trying to achieve with your clustering analysis.\n\nTheory:\n\nWrite a brief technical write up about how EACH clustering method works (KMEAN, DBSAN, hierarchical clustering). Also include details on model selection methods that you use (elbow, silhouette, etc).\nDon’t go “too deep”, around 2 to 4 paragraphs, per method, is fine. Write it in a way that a boss with a non-technical background would understand. Describe the method from a “big picture” point of view, how it works and what it is supposed to do.\n\nDo this “in your own words”. DO NOT copy this section from any other source. If you do so you will receive a zero on the assignment and will be referred to the department for further disciplinary action.\n\nMethods:\n\nIn this section, describe your coding workflow;\n\n\nIf you are not using Quarto, then provide links to the relevant code in this section.\n\n\nIf you are using Quarto (recommended), then include your code in-line using the “code folding” option, so that users can toggle the code on/off as needed. The notebook should also be hosted on Github. You can always do the entire thing in .ipynb then render it to HTML using Quarto at the end.\n\n\nPlease use the following workflow\n\nData selection: If you have not done so already, create either a numeric record feature dataset \\(\\mathbf X\\) AND/OR a text feature dataset \\(\\mathbf X\\) from your existing data.\n\nRemove the labels (targets) \\(Y\\) as needed so that it is suitable for clustering.\nIf you have labels, you can use them at the end, to check if the clustering predictions coincided with the existing labels in the data-set. In general this may or may-not be true.\nBut these labels should not be used as part of the clustering analysis.\n\nFeature selection: (optional)\n\nIf you want, you can perform filter based feature selection on your data-set \\(\\mathbf X\\) as a pre-processing step before clustering\nYou can also use optimal feature sets obtained during previous assignments\n\nHyper-parameter tuning\n\nFor each of the three clustering algorithms, perform any relevant parameter tuning in an attempt to achieve the optimal clustering results\ne.g. For k-means, Use Elbow and Silhouette methods to illustrate the ideal number of clusters. Visualize your results.\nAlso, when relevant, explore different choices of distance metric for the algorithm. Which distance metric seems to works best in which cases and why?\n\nFinal results\n\nOnce you have everything “dialed in”, re-do the analysis one last time with the optimal parameter choice to get your “final results”.\n\n\n\nResults:\n\nUsing your “final results”, discuss, illustrate, and compare the results of your various clustering analysis methods.\nWhich method seemed to work the best and why, which was easier to use or preferable, etc.\nCan you make connections between the optimal cluster predictions, after parameter tuning, with any of the labels in the data set. Do they coincide? Why or why not?\nDid the clustering results provide any new insights into your data?\nExplore the results, and create as many meaningful visualizations as you need. Be creative, and experiment with different image aesthetics.\nEnsure all visualizations are professional, ascetically pleasing, labeled, captioned, use color, are clear, and support your discussion and goals.\n\nConclusions:\n\nIn this section, the goal is to summarize & wrap-up the report. It explains what was found, in a way that would make sense to a general readership.\nThis area is non-technical. Technical descriptions of what you did should be in the methods or results sections, not conclusions.\nThe Conclusions should focus on key and important findings and how these findings affect real-life and real people.\n\nReferences:\n\nReference all non-original content.\nIdeally (but optionally) use .bibtex combined with Quarto to provide in-line internal citations\nSee the following link for an example:\n\nhttps://drive.google.com/open?id=12tYQnDuHS4ZxSTXgwsR4pRLhBYAMy14q&authuser=jh2343%40georgetown.edu&usp=drive_fs"
  },
  {
    "objectID": "HW-04/Code/clean.html",
    "href": "HW-04/Code/clean.html",
    "title": "Bella Shi's Website",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf_mental_health = pd.read_csv('../Data/mental_health.csv')\n#select rows when Year=2017\ndf_mental_health_2017 = df_mental_health[df_mental_health['Year']==2017]\n# print(df_mental_health_2017.head())\n\ndf_gdp_per_capita = pd.read_csv('../Data/GDP_percaptita_mental_health_2017.csv')\nprint(df_gdp_per_capita.head())\n\n# Merge the two DataFrames on the Economy column\nmerged_df = df_gdp_per_capita.merge(df_mental_health_2017[['Economy', 'average_learning_Adjusted_of_school', 'Income group']], on='Economy', how='left')\n\n# # Display the merged DataFrame\n# print(merged_df.head())\n\n# Save the DataFrame to a CSV file\nmerged_df.to_csv('../Data/merged_df.csv', index=False)\n\n       Economy       2017  Schizophrenia (%)  Bipolar disorder (%)  \\\n0  Afghanistan    635.789           0.166158              0.708089   \n1      Albania   4525.887           0.201025              0.704480   \n2      Algeria   4014.707           0.197913              0.818687   \n3      Andorra  40017.741           0.263512              0.963331   \n4       Angola     4039.3           0.172794              0.623904   \n\n   Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \n0              0.107142               4.882481        4.136347  \n1              0.174046               3.385245        2.208414  \n2              0.213612               5.065876        3.661094  \n3              0.644559               5.305375        3.729532  \n4              0.173643               3.296906        4.160484"
  },
  {
    "objectID": "HW-03/Code/EDA.html",
    "href": "HW-03/Code/EDA.html",
    "title": "Data Understanding",
    "section": "",
    "text": "import pandas as pd\n#load the data sets\ndf_age_first_depression=pd.read_csv('../Data/age_when_first_anxiety_or_depression.csv')\ndf_eating_disorder=pd.read_csv('../Data/eating_disorder_male_female.csv')\ndf_mental_health=pd.read_csv('../Data/mental_health.csv')\ndf_gdp_mental_health_2017=pd.read_csv('../Data/GDP_percaptita_mental_health_2017.csv')\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicators like income group, average learning adjusted years of school, continent, GDP for 2022, and a column related to comfort speaking about anxiety or depression, which has many missing values (NaN).\nEconomy: The name of the economy or country. Code: The country code.\nYear: The year of the data record.\nSchizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage.\nEating disorders (%): The prevalence of eating disorders as a percentage.\nAnxiety disorders (%): The prevalence of anxiety disorders as a percentage.\nDepression (%): The prevalence of depression as a percentage.\nIncome group: The income group classification of the economy.\naverage_learning_Adjusted_of_school: Some metric related to schooling, perhaps average years of schooling adjusted for learning.\nContinent: The continent where the economy is located.\nGDP(2022): The GDP of the economy for the year 2022.\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent: The percentage of people not at all comfortable speaking about anxiety or depression\nRecords: 5,488 Variables: 13 (1 integer, 8 floats, 4 objects) Features: Includes country data, year, percentages for various mental health conditions, income group, average learning, continent, GDP, and comfort speaking about anxiety/depression.\nThis dataset includes the prevalence of eating disorders among males and females, as well as a combined figure for all genders, across different countries and years.\nCountry: The name of the country.\nCountry Code: The corresponding country code.\nYear: The year of the observation.\nEating_disorders_Male: The prevalence of eating disorders among males.\nEating_disorders_Female: The prevalence of eating disorders among females.\nAll_gender: The prevalence of eating disorders across all genders.\nRecords: 6,420 Variables: 6 (1 integer, 3 floats, 2 objects) Features: Includes country data, year, and eating disorder prevalence separated by male, female, and all genders.\nThe dataset contains information on the age at which individuals first experienced anxiety or depression, categorized by different entities (which seem to represent regions or income categories).\nEntity: The region or income category.\nAge: The age category when anxiety or depression was first experienced.\nPercentage: The percentage of individuals in that entity and age category.\nRecords: 42 Variables: 3 (2 objects, 1 float) Features: Includes entity (could be country or other types of entities), age category, and percentage of individuals with first anxiety or depression experience.\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicator like GDP per captita.\nEconomy: The name of the economy or country. Year: The GDP_per_captita of the year 2017 of the economy or country. Schizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage. Eating disorders (%): The prevalence of eating disorders as a percentage. Anxiety disorders (%): The prevalence of anxiety disorders as a percentage. Depression (%): The prevalence of depression as a percentage.\nRecords: 166 Variables, 7 features ( 2 objects, 5 float) features:\ndatasets={\n    'age_first_depression':df_age_first_depression,\n    'eating_disorder':df_eating_disorder,\n    'mental_health':df_mental_health,\n    'gdp_mental_health_2017':df_gdp_mental_health_2017\n}\n\nfor name,df in datasets.items():\n    print(f\"{name} Dataset - First 5 Rows:\")\n    display(df.head())\n    print(f\"\\n{name} Dataset - Info:\")\n    display(df.info())\n    print(f\"{name} Dataset -Shape:\")\n    display(df.shape)\n    print('--------------------------------------------------------------------')\n\nage_first_depression Dataset - First 5 Rows:\n\nage_first_depression Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Entity      42 non-null     object \n 1   Age         42 non-null     object \n 2   Percentage  42 non-null     float64\ndtypes: float64(1), object(2)\nmemory usage: 1.1+ KB\nage_first_depression Dataset -Shape:\n--------------------------------------------------------------------\neating_disorder Dataset - First 5 Rows:\n\neating_disorder Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6420 entries, 0 to 6419\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Economy                  6420 non-null   object \n 1   Code                     6150 non-null   object \n 2   Year                     6420 non-null   int64  \n 3   Eating_disorders_Male    6420 non-null   float64\n 4   Eating_disorders_Female  6420 non-null   float64\n 5   All_gender               6420 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 301.1+ KB\neating_disorder Dataset -Shape:\n--------------------------------------------------------------------\nmental_health Dataset - First 5 Rows:\n\nmental_health Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5488 entries, 0 to 5487\nData columns (total 13 columns):\n #   Column                                                         Non-Null Count  Dtype  \n---  ------                                                         --------------  -----  \n 0   Economy                                                        5488 non-null   object \n 1   Code                                                           5488 non-null   object \n 2   Year                                                           5488 non-null   int64  \n 3   Schizophrenia (%)                                              5488 non-null   float64\n 4   Bipolar disorder (%)                                           5488 non-null   float64\n 5   Eating disorders (%)                                           5488 non-null   float64\n 6   Anxiety disorders (%)                                          5488 non-null   float64\n 7   Depression (%)                                                 5488 non-null   float64\n 8   Income group                                                   5432 non-null   object \n 9   average_learning_Adjusted_of_school                            4676 non-null   float64\n 10  Continent                                                      4900 non-null   object \n 11  GDP(2022)                                                      5264 non-null   float64\n 12  not_at_all_comfortable_speaking_anxiety_or_depression_percent  3108 non-null   float64\ndtypes: float64(8), int64(1), object(4)\nmemory usage: 557.5+ KB\nmental_health Dataset -Shape:\n--------------------------------------------------------------------\ngdp_mental_health_2017 Dataset - First 5 Rows:\n\ngdp_mental_health_2017 Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 166 entries, 0 to 165\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Economy                166 non-null    object \n 1   2017                   166 non-null    object \n 2   Schizophrenia (%)      166 non-null    float64\n 3   Bipolar disorder (%)   166 non-null    float64\n 4   Eating disorders (%)   166 non-null    float64\n 5   Anxiety disorders (%)  166 non-null    float64\n 6   Depression (%)         166 non-null    float64\ndtypes: float64(5), object(2)\nmemory usage: 9.2+ KB\ngdp_mental_health_2017 Dataset -Shape:\n--------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nEntity\nAge\nPercentage\n\n\n\n\n0\nAfrica\nAges &lt;13\n1.271836\n\n\n1\nAsia\nAges &lt;13\n7.795371\n\n\n2\nEurope\nAges &lt;13\n9.083381\n\n\n3\nHigh-income countries\nAges &lt;13\n2.473921\n\n\n4\nLower-middle-income countries\nAges &lt;13\n8.800553\n\n\n\n\n\n\n\nNone\n\n\n(42, 3)\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nEating_disorders_Male\nEating_disorders_Female\nAll_gender\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.088487\n0.161867\n0.125177\n\n\n1\nAfghanistan\nAFG\n1991\n0.086048\n0.156910\n0.121479\n\n\n2\nAfghanistan\nAFG\n1992\n0.083625\n0.152412\n0.118018\n\n\n3\nAfghanistan\nAFG\n1993\n0.081628\n0.147938\n0.114783\n\n\n4\nAfghanistan\nAFG\n1994\n0.079439\n0.143980\n0.111710\n\n\n\n\n\n\n\nNone\n\n\n(6420, 6)\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.160560\n0.697779\n0.101855\n4.828830\n4.071831\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n1\nAfghanistan\nAFG\n1991\n0.160312\n0.697961\n0.099313\n4.829740\n4.079531\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n2\nAfghanistan\nAFG\n1992\n0.160135\n0.698107\n0.096692\n4.831108\n4.088358\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n3\nAfghanistan\nAFG\n1993\n0.160037\n0.698257\n0.094336\n4.830864\n4.096190\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n4\nAfghanistan\nAFG\n1994\n0.160022\n0.698469\n0.092439\n4.829423\n4.099582\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n\n\n\n\n\nNone\n\n\n(5488, 13)\n\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n\n\n\n\n\n\n\nNone\n\n\n(166, 7)"
  },
  {
    "objectID": "HW-03/Code/EDA.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "href": "HW-03/Code/EDA.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "title": "Data Understanding",
    "section": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation",
    "text": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation\nCalculate and report basic summary statistics for the datasets.\nThis will include mean, median, mode, standard deviation, and variance for numerical variables. For categorical variables, we’ll provide frequency distributions.​\n\ndescription_stats={\n    \"age_first_depression_or_anxiety\":df_age_first_depression.describe(include='all'),\n    \"eating_disorder\":df_eating_disorder.describe(include='all'),\n    'gdp_mental_health_2017':df_gdp_mental_health_2017.describe(include='all'),\n    \"mental_health\":df_mental_health.describe(include='all'),\n}\ndescription_stats\n\n{'age_first_depression_or_anxiety':         Entity       Age  Percentage\n count       42        42   42.000000\n unique       7         6         NaN\n top     Africa  Ages &lt;13         NaN\n freq         6         7         NaN\n mean       NaN       NaN   16.666667\n std        NaN       NaN   10.060695\n min        NaN       NaN    1.271836\n 25%        NaN       NaN    8.871260\n 50%        NaN       NaN   14.426575\n 75%        NaN       NaN   22.251861\n max        NaN       NaN   42.724724,\n 'eating_disorder':             Economy  Code         Year  Eating_disorders_Male  \\\n count          6420  6150  6420.000000            6420.000000   \n unique          214   205          NaN                    NaN   \n top     Afghanistan   AFG          NaN                    NaN   \n freq             30    30          NaN                    NaN   \n mean            NaN   NaN  2004.500000               0.119775   \n std             NaN   NaN     8.656116               0.068943   \n min             NaN   NaN  1990.000000               0.033360   \n 25%             NaN   NaN  1997.000000               0.071057   \n 50%             NaN   NaN  2004.500000               0.098256   \n 75%             NaN   NaN  2012.000000               0.148705   \n max             NaN   NaN  2019.000000               0.672270   \n \n         Eating_disorders_Female   All_gender  \n count               6420.000000  6420.000000  \n unique                      NaN          NaN  \n top                         NaN          NaN  \n freq                        NaN          NaN  \n mean                   0.273787     0.196781  \n std                    0.214920     0.140066  \n min                    0.056762     0.045083  \n 25%                    0.121105     0.096171  \n 50%                    0.187430     0.144395  \n 75%                    0.352395     0.252282  \n max                    1.395754     1.034012  ,\n 'gdp_mental_health_2017':             Economy     2017  Schizophrenia (%)  Bipolar disorder (%)  \\\n count           166      166         166.000000            166.000000   \n unique          166      166                NaN                   NaN   \n top     Afghanistan  635.789                NaN                   NaN   \n freq              1        1                NaN                   NaN   \n mean            NaN      NaN           0.210030              0.736341   \n std             NaN      NaN           0.040753              0.157692   \n min             NaN      NaN           0.149087              0.411127   \n 25%             NaN      NaN           0.182119              0.626204   \n 50%             NaN      NaN           0.201004              0.708670   \n 75%             NaN      NaN           0.234878              0.845505   \n max             NaN      NaN           0.363326              1.206088   \n \n         Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \n count             166.000000             166.000000      166.000000  \n unique                   NaN                    NaN             NaN  \n top                      NaN                    NaN             NaN  \n freq                     NaN                    NaN             NaN  \n mean                0.260652               4.024311        3.443300  \n std                 0.167330               1.176412        0.621952  \n min                 0.079896               2.066871        2.196154  \n 25%                 0.136695               3.187924        2.958046  \n 50%                 0.204115               3.593617        3.463656  \n 75%                 0.307116               4.712918        3.828232  \n max                 0.943081               8.539931        5.636661  ,\n 'mental_health':             Economy  Code         Year  Schizophrenia (%)  \\\n count          5488  5488  5488.000000        5488.000000   \n unique          196   196          NaN                NaN   \n top     Afghanistan   AFG          NaN                NaN   \n freq             28    28          NaN                NaN   \n mean            NaN   NaN  2003.500000           0.208183   \n std             NaN   NaN     8.078483           0.041998   \n min             NaN   NaN  1990.000000           0.146902   \n 25%             NaN   NaN  1996.750000           0.179452   \n 50%             NaN   NaN  2003.500000           0.198509   \n 75%             NaN   NaN  2010.250000           0.230554   \n max             NaN   NaN  2017.000000           0.375110   \n \n         Bipolar disorder (%)  Eating disorders (%)  Anxiety disorders (%)  \\\n count            5488.000000           5488.000000            5488.000000   \n unique                   NaN                   NaN                    NaN   \n top                      NaN                   NaN                    NaN   \n freq                     NaN                   NaN                    NaN   \n mean                0.716884              0.234023               3.946979   \n std                 0.164246              0.154147               1.134810   \n min                 0.314535              0.073908               2.023393   \n 25%                 0.615732              0.121760               3.178912   \n 50%                 0.693954              0.180378               3.515140   \n 75%                 0.830217              0.278681               4.659540   \n max                 1.206597              0.943991               8.967330   \n \n         Depression (%) Income group  average_learning_Adjusted_of_school  \\\n count      5488.000000         5432                          4676.000000   \n unique             NaN            4                                  NaN   \n top                NaN  High income                                  NaN   \n freq               NaN         1764                                  NaN   \n mean          3.474504          NaN                             7.760170   \n std           0.671741          NaN                             2.528810   \n min           2.139903          NaN                             2.251002   \n 25%           2.955355          NaN                             5.684793   \n 50%           3.461421          NaN                             7.855914   \n 75%           3.877343          NaN                            10.048229   \n max           6.602754          NaN                            12.775495   \n \n        Continent     GDP(2022)  \\\n count       4900  5.264000e+03   \n unique         6           NaN   \n top       Africa           NaN   \n freq        1428           NaN   \n mean         NaN  3.929245e+05   \n std          NaN  1.451071e+06   \n min          NaN  2.230000e+02   \n 25%          NaN  1.240875e+04   \n 50%          NaN  4.239550e+04   \n 75%          NaN  2.550042e+05   \n max          NaN  1.796317e+07   \n \n         not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n count                                         3108.000000              \n unique                                                NaN              \n top                                                   NaN              \n freq                                                  NaN              \n mean                                            30.009805              \n std                                             11.940808              \n min                                              5.833535              \n 25%                                             21.170624              \n 50%                                             30.920343              \n 75%                                             37.729200              \n max                                             58.778120              }\n\n\n\nMental Health Dataset\nCorrelation Heatmap: To see how different types of mental health disorders are correlated with each other.\nTime Series Plot: To observe the trend of a specific disorder over time for a particular economy or aggregated globally.\nBar Chart: To compare the prevalence of different disorders in a specific year across multiple economies.\nBox Plot: To visualize the distribution of prevalence rates for a particular disorder across different income groups or continents.\nScatter Plot: To examine the relationship between GDP and the prevalence of a particular disorder or the discomfort in speaking about mental health issues.\nHistograms for mental disorders\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the aesthetics for the plots\nsns.set(style=\"whitegrid\")\n\n# Define a function to create histograms for given columns\ndef plot_histograms(data, columns, bins=20, figsize=(15, 5)):\n    fig, axes = plt.subplots(1, len(columns), figsize=figsize)\n    for ax, col in zip(axes, columns):\n        sns.histplot(data[col].dropna(), bins=bins, ax=ax, kde=True)  # Drop NaN for plotting\n        ax.set_title(f'Distribution of {col}')\n    plt.tight_layout()\n    return fig\n\n# Select columns to plot for Mental Health Data (excluding 'Year' and non-numerical columns)\nmental_health_columns = ['Schizophrenia (%)', 'Bipolar disorder (%)', \n                         'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)']\n\n# Plot histograms for the selected columns\nhistograms_mental_health = plot_histograms(df_mental_health, mental_health_columns)\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_income_group=df_mental_health.groupby('Income group').mean()\ndf_income_group=df_income_group.drop(columns=['Year','GDP(2022)','not_at_all_comfortable_speaking_anxiety_or_depression_percent'])\nprint(type(df_income_group))\nprint(df_income_group)\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 6))\ndf_income_group.plot(kind='bar', ax=ax)\n\n# Customization\nax.set_title('Mean Percentage of Mental Health Disorders by Income Group')\nax.set_ylabel('Mean Percentage')\nax.set_xlabel('Income Group')\nplt.xticks(rotation=45)\nplt.legend(title='Mental Health Disorders')\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n                     Schizophrenia (%)  Bipolar disorder (%)  \\\nIncome group                                                   \nHigh income                   0.243514              0.822296   \nLow income                    0.164570              0.626751   \nLower middle income           0.192946              0.631165   \nUpper middle income           0.202089              0.724714   \n\n                     Eating disorders (%)  Anxiety disorders (%)  \\\nIncome group                                                       \nHigh income                      0.399097               4.707860   \nLow income                       0.102740               3.500061   \nLower middle income              0.134964               3.493755   \nUpper middle income              0.202037               3.739220   \n\n                     Depression (%)  average_learning_Adjusted_of_school  \nIncome group                                                              \nHigh income                3.575551                            10.382413  \nLow income                 3.804767                             4.031739  \nLower middle income        3.495127                             6.440634  \nUpper middle income        3.172075                             7.940104  \n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1122149726.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_income_group=df_mental_health.groupby('Income group').mean()\n\n\n\n\n\n\nCorralation Analysis\nCorrelation Heatmap\n‘Income group’ and ‘Continent’ to the list of columns for the correlation matrix, which are categorical variables. The correlation matrix typically requires numerical variables. To include these categorical variables, we would need to convert them into a numerical format using techniques such as one-hot encoding.\n\nimport numpy as np\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    # 'Income group',\n    # 'Continent',\n\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\n\n\n\n\n\nThe updated heatmap now includes the one-hot encoded categorical variables ‘Income group’ and ‘Continent’. Due to the number of categories, the annotations were turned off to keep the heatmap readable. Each square still represents the correlation between two variables, with warmer colors indicating a stronger positive correlation and cooler colors a stronger negative correlation.\nTime Series Plot\nGlobal trend of ‘Depression (%)’ over time.\n\n# Time Series Plot for 'Depression (%)' trend globally over time\n\n# Calculating the global average of 'Depression (%)' for each year\nglobal_depression_trend = df_mental_health.groupby('Year')['Depression (%)'].mean().reset_index()\n\n# Plotting the global trend of 'Depression (%)' over time\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=global_depression_trend, x='Year', y='Depression (%)')\nplt.title('Global Trend of Depression Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Depression (%)')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\nThe Time Series Plot above shows the global trend of depression over time, depicting the average percentage of depression prevalence across all economies for each year.\n#### Data Grouping and Segmentation Bar Chart\ncompare the prevalence of ‘Anxiety disorders (%)’ and ‘Eating disorders (%)’ in the most recent year available across the top 10 economies by GDP. To do this, we first need to identify the top 10 economies by GDP and then gather the data for the specified disorders.\n\n# Bar Chart: Prevalence of 'Anxiety disorders (%)' and 'Eating disorders (%)' ,'Bipolar disorder (%),in the most recent year across top 10 economies by GDP\n\n# Identifying the most recent year available in the dataset\nmost_recent_year = df_mental_health['Year'].max()\n\n# Identifying the top 10 economies by GDP in the most recent year\ntop_economies = df_mental_health[df_mental_health['Year'] == most_recent_year].sort_values(by='GDP(2022)', ascending=False).head(10)\n\n# Preparing the data for the bar chart\nbar_chart_data = top_economies[['Economy', 'Anxiety disorders (%)', 'Eating disorders (%)','Bipolar disorder (%)']].set_index('Economy')\n\n# Plotting the bar chart\nbar_chart_data.plot(kind='bar', figsize=(14, 7))\nplt.title(f'Prevalence of Anxiety and Eating Disorders in {most_recent_year} Across Top 10 Economies by GDP')\nplt.xlabel('Economy')\nplt.ylabel('Prevalence (%)')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y')\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe Bar Chart displays the prevalence of anxiety and eating disorders in the most recent year available across the top 10 economies as measured by GDP. Each economy is represented by a bar, with separate segments for anxiety disorders and eating disorders, allowing for a direct comparison between these conditions within each economy.\nwe can draw the conclusion that the Eating disorders and Anxiety disorders, Bipolar disorder (%) are not related to GDP\nThe Box Plot\nBox Plot: Distribution of ‘Bipolar disorder (%)’ prevalence rates across different income groups\n\n\n# Filter the data for 'Bipolar disorder (%)' for the most recent year\nbipolar_data_recent_year = df_mental_health[df_mental_health['Year'] == most_recent_year]\n\n# Plotting the box plot\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=bipolar_data_recent_year, x='Income group', y='Bipolar disorder (%)')\nplt.title(f'Distribution of Bipolar Disorder Prevalence Across Income Groups in {most_recent_year}')\nplt.xlabel('Income Group')\nplt.ylabel('Bipolar Disorder Prevalence (%)')\nplt.grid(axis='y')\n\n# Show the box plot\nplt.show()\n\n\n\n\nThe Box Plot above shows the distribution of bipolar disorder prevalence rates across different income groups for the most recent year available. The plot provides insights into the median prevalence rates, the interquartile ranges, and any potential outliers within each income group category.\nWe can draw the conclusion that the Bipolar Disorder is realted to Income group.\nScatter Plot:\nExamine the relationship between GDP and the prevalence of depression for the most recent year across all economies.​\n\n# Scatter Plot: Relationship between 'GDP(2022)' and 'Depression (%)' for the most recent year\n\n# Plotting the scatter plot\nplt.figure(figsize=(10, 7))\nsns.scatterplot(data=bipolar_data_recent_year, x='GDP(2022)', y='Depression (%)')\nplt.title(f'Relationship Between GDP and Depression Prevalence in {most_recent_year}')\nplt.xlabel('GDP (2022)')\nplt.ylabel('Depression Prevalence (%)')\nplt.grid(True)\n\n# Show the scatter plot\nplt.show()\n\n\n\n\n\nwe can draw the conclusion that Depression Prevalence (%) are not related to GDP\nThe scatter plot\n\n# Let's create a function that generates scatter plots for any given list of disorders.\n# This function will adjust the number of subplots based on the number of disorders.\n\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\neducation_col = 'average_learning_Adjusted_of_school'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\n# Now let's use the function to create scatter plots\ncreate_scatter_plots(df_mental_health, education_col, mental_health_disorders, 2)\n\n\n\n\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone.\n\n\n\nEating disorder Dataset\nTime Series Line Plot: Showing the trend of eating disorder prevalence over years for a specific country or averaged globally.\nBar Chart: Comparing the prevalence of eating disorders between males and females across different countries.\nBox Plot: Displaying the distribution of eating disorder prevalence for all countries across different years to see the variability and outliers.\nHeatmap: Visualizing the prevalence of eating disorders across countries and years in a color-coded format.\nScatter Plot: Comparing the male vs. female prevalence of eating disorders to see the correlation between genders.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the average prevalence of eating disorders for each year across all countries\naverage_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n# Time Series Line Plot for the global trend\nplt.figure(figsize=(14, 7))\n\n# Plotting the trends for each gender and all genders combined\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Male', label='Male')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Female', label='Female')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='All_gender', label='All Genders')\n\nplt.title('Global Average Prevalence of Eating Disorders Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Prevalence')\nplt.legend()\nplt.grid(True)\n\n# Show the line plot\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/758625183.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  average_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n\n\n\n\nThe line plot above shows the global average prevalence of eating disorders over time for males, females, and all genders combined. The trends can be compared to see how the prevalence has changed over the years.\nBar chart compare the prevalence of eating disorders between males and females across different countries. For clarity, we will take a subset of countries to avoid cluttering the chart.\n\n# Randomly select 10 countries for comparison\nrandom_countries = df_eating_disorder['Economy'].drop_duplicates().sample(10, random_state=1)\n\n# Filter the data for the selected countries and the latest year available\nlatest_year = df_eating_disorder['Year'].max()\ncomparison_data = df_eating_disorder[(df_eating_disorder['Economy'].isin(random_countries)) & (df_eating_disorder['Year'] == latest_year)]\n\n# Bar Chart: Comparing the prevalence of eating disorders between males and females across different countries\nplt.figure(figsize=(14, 7))\n\n# Plot for males\nsns.barplot(x='Economy', y='Eating_disorders_Male', data=comparison_data, color='lightblue', label='Male')\n\n# Plot for females on top of males to create a grouped bar chart\nsns.barplot(x='Economy', y='Eating_disorders_Female', data=comparison_data, color='pink', label='Female')\n\nplt.title(f'Comparison of Eating Disorders Prevalence by Gender in {latest_year}')\nplt.xlabel('Economy')\nplt.ylabel('Prevalence of Eating Disorders')\nplt.xticks(rotation=45, ha='right')\nplt.legend()\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe bar chart above compares the prevalence of eating disorders between males (in light blue) and females (in pink) for a randomly selected subset of 10 countries in the latest year available in the dataset.\nData Grouping and Segmentation For the third visualization, We can group the data by country and by year to analyze trends over time and segment by gender to compare the prevalence rates.\nIdentifying Outliers We will look for countries with prevalence rates that are significantly higher or lower than the global average for each year and by gender.\n\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nMale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.11%. The range of prevalence rates is quite broad, with the lowest around 0.03% and the highest at about 0.67%. The interquartile range (middle 50% of the data) spans from approximately 0.08% to 0.16%, indicating that half of the reported rates fall within this range.\nFemale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.21%, which is notably higher than that of males. The prevalence rates for females also show a broad range, from about 0.06% to 1.40%. The interquartile range for females is wider than for males, ranging from about 0.14% to 0.40%, reflecting greater variability in the rates reported for females.\nOutlier Identification:\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\n\n# For the heatmap focused on China, we will select all years of data for China only.\nchina_data = df_eating_disorder[df_eating_disorder['Country'] == 'China'].pivot('Year', 'Country Code', 'All_gender')\n\n# Heatmap: Visualizing the prevalence of eating disorders across years in China\nplt.figure(figsize=(10, 8))\nsns.heatmap(china_data, cmap=\"YlOrRd\", linewidths=.5, annot=True, fmt=\".2f\")\n\nplt.title('Heatmap of Eating Disorders Prevalence Across Years in China')\nplt.xlabel('Country Code')\nplt.ylabel('Year')\n\n# Show the heatmap\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1600456578.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  china_data = df_eating_disorder[df_eating_disorder['Country'] == 'China'].pivot('Year', 'Country Code', 'All_gender')\n\n\n\n\n\nheatmap\nThe heatmap above visualizes the prevalence of eating disorders across different years in China. Each cell in the heatmap corresponds to a specific year and shows the prevalence rate, with the color intensity reflecting the magnitude of that rate.​\ncreate a scatter plot to compare the male vs. female prevalence of eating disorders to see the correlation between genders. We’ll use data from the most recent year for all countries to see the relationship.​\n\n# Scatter Plot: Comparing male vs. female prevalence of eating disorders\n# Filter the dataset for the latest year\nscatter_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\nplt.figure(figsize=(10, 7))\n\n# Plotting the scatter plot for male vs. female prevalence\nsns.scatterplot(data=scatter_data, x='Eating_disorders_Male', y='Eating_disorders_Female', hue='Country', legend=False)\n\nplt.title(f'Correlation Between Male and Female Prevalence of Eating Disorders in {latest_year}')\nplt.xlabel('Male Prevalence')\nplt.ylabel('Female Prevalence')\n\n# Plot a 45 degree line to show y=x for reference\nmax_val = max(scatter_data['Eating_disorders_Male'].max(), scatter_data['Eating_disorders_Female'].max())\nplt.plot([0, max_val], [0, max_val], '--', color='gray')\n\n# Show the scatter plot\nplt.show()\n\n\n\n\nThe scatter plot above compares the prevalence of eating disorders between males (on the x-axis) and females (on the y-axis) for the most recent year available in the dataset. Each point represents a country, and the gray dashed line indicates where the prevalence would be equal for both genders. Points above the line show countries where the prevalence is higher in females compared to males, which seems to be the case for all countries shown.\n\n\nAge First Depression or Anxiety Data\n\n# Select columns to plot for Age First Depression or Anxiety Data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the style of seaborn\nsns.set_style(\"whitegrid\")\n\n# Bar Chart: Showing the percentage of individuals for each age category within each entity\nplt.figure(figsize=(10, 8))\nbar_chart = sns.barplot(x='Percentage', y='Entity', hue='Age', data=df_age_first_depression, ci=None)\nplt.title('Percentage of Individuals by Entity and Age Category for First Anxiety or Depression')\nplt.xlabel('Percentage')\nplt.ylabel('Entity')\nplt.legend(title='Age Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/626523418.py:10: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  bar_chart = sns.barplot(x='Percentage', y='Entity', hue='Age', data=df_age_first_depression, ci=None)\n\n\n\n\n\nCreate a pie chart showing the distribution of age categories for the onset of anxiety or depression for the “World” entity, I’ll first filter the dataset for the ‘World’ entity, and then plot the data.\n\n# Filter the data for the 'World' entity\nworld_data = df_age_first_depression[df_age_first_depression['Entity'] == 'World']\n\n# Pie Chart: Showing the distribution of age categories for the World entity\nplt.figure(figsize=(8, 8))\nplt.pie(world_data['Percentage'], labels=world_data['Age'], autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Age Categories for First Anxiety or Depression in the World')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n# Show the pie chart\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSince the data does not seem to have a time component or an inherent sequence, we’ll skip the line chart and instead create a heatmap to visualize the relationship between entities and age groups.\n\n# Create a pivot table for the heatmap\nheatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n# Heatmap: Visualize the percentage of individuals by entity and age category\nplt.figure(figsize=(12, 10))\nheatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Heatmap of Percentage by Entity and Age Category for First Anxiety or Depression')\nplt.xlabel('Age Category')\nplt.ylabel('Entity')\n\n# Show the heatmap\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/601293865.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  heatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n\n\n\n\nThe heatmap above visualizes the percentages by entity and age category for the first experience of anxiety or depression. The color intensity represents the magnitude of the percentages, with cooler colors indicating lower percentages and warmer colors indicating higher percentages.\nFinally create a stacked bar chart to represent the cumulative percentages of age categories for each entity.\ngrouping the data by Entity and visualizing the distribution of the Percentage across different Age brackets. This will help us understand if there are significant differences in the age of onset for anxiety or depression across different parts of the world and different income levels.​\n\n# Stacked Bar Chart: Create a crosstab to prepare data for the stacked plot\nstacked_data = pd.crosstab(index=df_age_first_depression['Entity'], columns=df_age_first_depression['Age'], values=df_age_first_depression['Percentage'], aggfunc='sum', normalize='index')\n\n# Plotting the stacked bar chart\nstacked_data.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='viridis')\nplt.title('Stacked Bar Chart of Age Categories for First Anxiety or Depression by Entity')\nplt.xlabel('Entity')\nplt.ylabel('Percentage')\nplt.legend(title='Age Category', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xticks(rotation=45, ha='right')\n\n# Show the stacked bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe bar chart and table display the distribution of the age of onset for anxiety or depression across various entities, which include continents, income categories, and the world overall. From the visualization and the data, we can observe the following:\nVariability in Age of Onset: There is considerable variability in the age of onset for anxiety or depression among different entities. For example, in Europe, a higher percentage reports the onset during the age brackets of “Ages 20-29” and “Ages ≥40”, whereas in Africa, the onset is more reported in the younger age bracket “Ages 13-19” and “Ages 20-29”.\nHigh-Income Countries: In high-income countries, the age bracket “Ages 13-19” has the highest reported onset, followed by “Ages ≥40” and “Ages 20-29”.\nLower-Middle-Income Countries: Here, the age bracket “Ages 20-29” has the highest reported onset, suggesting that the onset of anxiety or depression may occur later compared to high-income countries.\nDon’t Know/Refused: A significant portion of respondents across all entities did not provide a specific age bracket or refused to answer, with Africa having the highest percentage in this category. This could reflect cultural differences in discussing mental health or data collection challenges.\n\n\nGDP_mental_health_2017_Dataset\nHistograms for mental health indicators to see their distribution across the dataset.\n\n# Histograms for mental health indicators\nplt.figure(figsize=(15, 10))\n\n# List of mental health indicators to plot\nmental_health_indicators = ['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)', \n                            'Anxiety disorders (%)']\n\n# Plotting each histogram\nfor i, indicator in enumerate(mental_health_indicators, 1):\n    plt.subplot(2, 2, i)\n    sns.histplot(df_gdp_mental_health_2017[indicator], kde=False, bins=30)\n    plt.title(f'Distribution of {indicator}')\n    plt.xlabel(indicator)\n    plt.ylabel('Frequency')\n\n# Adjusting layout\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe histograms provide a distribution of each mental health indicator across the countries in the dataset for the year 2017\nscatter plots\nvisualize the potential correlation between GDP per capita and each mental health indicator.\n\n# Renaming the '2017' column to 'GDP_per_capita' for clarity\ndf_gdp_mental_health_2017.rename(columns={'2017': 'GDP_per_capita'}, inplace=True)\n\n# Ensuring 'GDP_per_capita' is a numeric column\ndf_gdp_mental_health_2017['GDP_per_capita'] = pd.to_numeric(df_gdp_mental_health_2017['GDP_per_capita'], errors='coerce')\n\n# List of mental health indicators to plot\nmental_health_indicators = ['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)', 'Anxiety disorders (%)']\n\n# Scatter plots of GDP per capita vs each mental health indicator\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plotting scatter plots\nfor i, indicator in enumerate(mental_health_indicators):\n    sns.scatterplot(ax=axes[i//2, i%2], \n                    data=df_gdp_mental_health_2017, \n                    x='GDP_per_capita', \n                    y=indicator)\n    axes[i//2, i%2].set_title(f'GDP per Capita vs {indicator}')\n    axes[i//2, i%2].set_xlabel('GDP per Capita')\n    axes[i//2, i%2].set_ylabel(indicator)\n\n# Adjusting layout\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatter plots have been generated, showing the relationship between GDP per capita and each mental health indicator.\nSchizophrenia (%) vs GDP per Capita: There does not appear to be a clear trend indicating that the prevalence of schizophrenia is related to GDP per capita. The distribution of points seems scattered without a distinct pattern.\nBipolar Disorder (%) vs GDP per Capita: Similar to schizophrenia, bipolar disorder prevalence does not show a clear correlation with GDP per capita based on the scatter plot.\nEating Disorders (%) vs GDP per Capita: For eating disorders, there might be a slight positive trend, suggesting that higher GDP per capita could be associated with a higher reported prevalence of eating disorders. This could be due to better reporting and diagnosis in higher-income countries, but further statistical analysis would be needed to confirm any correlation.\nAnxiety Disorders (%) vs GDP per Capita: The scatter plot does not reveal a strong correlation between the prevalence of anxiety disorders and GDP per capita. However, there is some spread in the data that might warrant a closer look with more sophisticated statistical tools.\nGeneral Observations: The prevalence rates for the mental health conditions do not show strong visual evidence of a correlation with GDP per capita. This might suggest that mental health issues are widespread and not necessarily directly related to the economic status of a country.\nIt is important to note that these are preliminary insights based solely on visual analysis. To draw more concrete conclusions, you would need to perform a quantitative analysis, such as calculating the correlation coefficients or conducting regression analysis. Additionally, it’s important to consider other factors that could influence mental health statistics, such as healthcare access, cultural attitudes towards mental health, and the quality of data reporting in different countries.\nHeatmap of the correlation matrix to understand how different indicators are related to each other. This will include GDP per capita and the mental health indicators.​\n\n# Correlation matrix heatmap\nplt.figure(figsize=(10, 8))\n\n# Calculate correlation matrix\ncorr_matrix = df_gdp_mental_health_2017.corr()\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n\n# Title\nplt.title('Correlation Matrix Heatmap')\n\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1859813442.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_matrix = df_gdp_mental_health_2017.corr()\n\n\n\n\n\nThe heatmap displays the correlation coefficients between the GDP per capita and mental health indicators. Values close to 1 or -1 indicate a strong positive or negative correlation, respectively, while values close to 0 indicate a weak or no correlation."
  },
  {
    "objectID": "HW-03/Code/EDA.html#hypothesis-generation",
    "href": "HW-03/Code/EDA.html#hypothesis-generation",
    "title": "Data Understanding",
    "section": "Hypothesis Generation",
    "text": "Hypothesis Generation\n\nAge First Depression or Anxiety Data\n\nThe cultural, economic, and social environments in high-income countries might contribute to an earlier age of onset for anxiety or depression.\nIn regions with lower-middle-income levels, the onset of anxiety or depression might occur later, potentially due to different life stressors or lower levels of mental health awareness and reporting.\n\n\n\nEating disorder Dataset\n\nThe prevalence of eating disorders is higher in countries with higher GDP per capita, which could reflect better diagnostic capabilities or different societal pressures.\nThere may be significant differences in the prevalence of eating disorders between genders across different countries, potentially reflecting cultural, social, or biological factors.\n\n\n\nGDP_mental_health_2017_Dataset\n\nHigher GDP per capita might be associated with higher reported rates of mental health disorders, potentially due to better health services and reporting mechanisms.\nThe prevalence of different mental health disorders will vary across economies, possibly due to genetic, environmental, cultural, or socioeconomic factors.\n\n\n\nMental Health Dataset\n\nEducational outcomes may be associated with the prevalence or reporting of certain mental health conditions, such as schizophrenia and eating disorders.\nThe lack of correlation between GDP and comfort in discussing mental health could imply that economic development does not directly influence cultural attitudes toward mental health openness.\n\n3.The correlations between different mental health conditions suggest the possibility of common underlying risk factors or increased comorbidity."
  },
  {
    "objectID": "HW-03/Code/EDA.html#identiy-outliers",
    "href": "HW-03/Code/EDA.html#identiy-outliers",
    "title": "Data Understanding",
    "section": "Identiy Outliers",
    "text": "Identiy Outliers\n\nAge First Depression or Anxiety Data\nThe data does not have individual country-level entries, so it’s not possible to identify outliers in the traditional sense. However, the “Don’t Know/Refused” category stands out and could be considered an anomaly in the reporting of mental health data.\n\n\nEating disorder Dataset\nWe will look for countries with prevalence rates that are significantly higher or lower than the global average for each year and by gender.\n\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\n\n\nGDP_mental_health_2017_Dataset\n\n\nMental Health Dataset"
  },
  {
    "objectID": "HW-03/Code/EDA.html#report-and-discuss-methods-and-findings",
    "href": "HW-03/Code/EDA.html#report-and-discuss-methods-and-findings",
    "title": "Data Understanding",
    "section": "Report and discuss methods and findings",
    "text": "Report and discuss methods and findings\n\nAge First Depression or Anxiety Data\nThe bar chart and table display the distribution of the age of onset for anxiety or depression across various entities, which include continents, income categories, and the world overall. From the visualization and the data, we can observe the following:\nVariability in Age of Onset: There is considerable variability in the age of onset for anxiety or depression among different entities. For example, in Europe, a higher percentage reports the onset during the age brackets of “Ages 20-29” and “Ages ≥40”, whereas in Africa, the onset is more reported in the younger age bracket “Ages 13-19” and “Ages 20-29”.\nHigh-Income Countries: In high-income countries, the age bracket “Ages 13-19” has the highest reported onset, followed by “Ages ≥40” and “Ages 20-29”.\nLower-Middle-Income Countries: Here, the age bracket “Ages 20-29” has the highest reported onset, suggesting that the onset of anxiety or depression may occur later compared to high-income countries.\nDon’t Know/Refused: A significant portion of respondents across all entities did not provide a specific age bracket or refused to answer, with Africa having the highest percentage in this category. This could reflect cultural differences in discussing mental health or data collection challenges.\n\n\nEating disorder Dataset\nThe prevalence of eating disorders is significantly higher in females than in males, which aligns with the general understanding of these conditions.\nThe variability in prevalence rates among females is greater, indicating that eating disorders in females may be more influenced by a variety of factors, possibly including cultural aspects, societal pressures, or biological predispositions.\nThe global average prevalence of eating disorders over time for males, females, and all genders combined is growing up.\n\n\nGDP_mental_health_2017_Dataset\nEating Disorders (%) vs GDP per Capita: For eating disorders, there might be a slight positive trend, suggesting that higher GDP per capita could be associated with a higher reported prevalence of eating disorders.\n\n\nMental Health Dataset\nwe can draw the conclusion that the Eating disorders and Anxiety disorders, Bipolar disorder (%) are not related to GDP.\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone."
  },
  {
    "objectID": "HW-03/Code/EDA.html#tools-and-software",
    "href": "HW-03/Code/EDA.html#tools-and-software",
    "title": "Data Understanding",
    "section": "Tools and Software",
    "text": "Tools and Software\n\nPandas: For data manipulation and analysis.\nMatplotlib: For creating the bar chart visualization.\nSeaborn: Although not used in the last plot, it is an excellent tool for creating heatmaps and other complex visualizations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mental Health Disorder",
    "section": "",
    "text": "Significance\nmental health disorders have a profound impact on individuals, families, communities, and societies. Addressing these disorders is not only a matter of public health but also a moral and ethical imperative to reduce suffering and improve the overall well-being of populations.\n\nHuman Suffering: Mental health disorders can cause immense suffering for individuals and their families. They can lead to reduced quality of life, disability, and, in some cases, loss of life through suicide.\nImproved Health Outcomes: Effective population health management can lead to improved health outcomes for individuals and communities. It can help reduce mortality rates, lower the incidence of chronic diseases, and enhance overall quality of life.\nEconomic Impact: These disorders also have a significant economic impact due to medical costs, lost productivity, and reduced quality of life. They can lead to increased healthcare spending and lower workplace productivity.\nStigma and Discrimination:Mental health disorders are often associated with stigma and discrimination. This can prevent people from seeking help and contribute to their suffering in silence.\nComorbidityt: Mental health disorders often co-occur with other health conditions. For example, people with depression may also have chronic physical conditions, making their overall health worse.\nLifestyle Factors:Mental health disorders can contribute to unhealthy lifestyle choices, such as poor diet, lack of physical activity, and substance abuse, which can increase the risk of other health issues.\nImpact on Families: Mental health disorders don’t just affect the individual but also their families and caregivers. This can lead to strain on relationships and increased caregiving responsibilities.\nPreventable and Treatable: Many mental health disorders are preventable and treatable. Early intervention and effective treatments, including therapy and medication, can improve symptoms and help people lead fulfilling lives.\nResearch and Innovation:The study of mental health has led to advancements in neuroscience, psychology, and psychiatry. Research in this field not only benefits those with mental health disorders but also contributes to our understanding of the human brain and behavior.\nLegislation and Advocacy: The significance of mental health has led to legislative changes and advocacy efforts. Laws and policies are evolving to provide better support and resources for individuals with mental health issues.\n\nIn summary, population health management is important because it shifts the healthcare paradigm from a reactive, illness-centered approach to a proactive, wellness-focused one. It aims to improve health outcomes, reduce healthcare costs, promote health equity, and enhance the overall health and well-being of populations, making it a critical component of modern healthcare systems.\n\n\nQuestions\n\nWhat are the range of mental health disorder types, and how prevalent are they in the population?\nHow has this prevalence changed over time in the population?\nWhat is the relationship between socialeconimic factors(income(high-income, lower-middle-income, upper-middle-income), GDP, average outcomes, and employment status) correlate with mental health outcomes (depression,anxiety,eating disorders, bipolar disorder)?\nHow does comfort talking about mental health vary by country and continent (Africa, Asia, Europe)?\nHow does comfort talking about mental health vary by education level?\nHow does the prevalence of eating disorders vary by country and income level(high-income, lower-middle-income, upper-middle-income)?\nWhich set of factors “better” explains mental health outcomes: socioeconomic or geographic or educational factors?\nIs this true for all mental health disorders, and if not, which disorders are “most talked about” in which countries?\nHow is the data actually gathered? Is it gathered using surveys? Or is it medical diagnoses? Or both?\nAre there issues with using the survey data available? Similarly, are there issues with using the medical data available?\n\n\n\nPublication\n(Pedrelli et al. 2015)\n(Fergusson et al. 2015)\n\n\n\n\n\nReferences\n\nFergusson, D. M., G. F. H. McLeod, L. J. Horwood, N. R. Swain, S. Chapple, and R. Poulton. 2015. “Life Satisfaction and Mental Health Problems (18 to 35 Years).” Psychological Medicine 45 (11): 2427–36. https://doi.org/10.1017/S0033291715000422.\n\n\nPedrelli, Paola, Maren Nyer, Albert Yeung, Courtney Zulauf, and Timothy Wilens. 2015. “College Students: Mental Health Problems and Treatment Considerations.” Academic Psychiatry 39 (5): 503–11. https://doi.org/10.1007/s40596-014-0205-9."
  },
  {
    "objectID": "Exploration.html",
    "href": "Exploration.html",
    "title": "Exploration",
    "section": "",
    "text": "import pandas as pd\n#load the data sets\ndf_age_first_depression=pd.read_csv('./Data/age_when_first_anxiety_or_depression.csv')\ndf_eating_disorder=pd.read_csv('./Data/eating_disorder_male_female.csv')\ndf_mental_health=pd.read_csv('./Data/mental_health.csv')\ndf_gdp_mental_health_2017=pd.read_csv('./Data/GDP_percaptita_mental_health_2017.csv')\ndf_mental_physical=pd.read_csv('./Data/symptoms.csv')"
  },
  {
    "objectID": "Exploration.html#data-understanding",
    "href": "Exploration.html#data-understanding",
    "title": "Exploration",
    "section": "Data Understanding",
    "text": "Data Understanding\n\nMental Health Dataset\n\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicators like income group, average learning adjusted years of school, continent, GDP for 2022, and a column related to comfort speaking about anxiety or depression, which has many missing values (NaN).\nEconomy: The name of the economy or country. Code: The country code.\nYear: The year of the data record.\nSchizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage.\nEating disorders (%): The prevalence of eating disorders as a percentage.\nAnxiety disorders (%): The prevalence of anxiety disorders as a percentage.\nDepression (%): The prevalence of depression as a percentage.\nIncome group: The income group classification of the economy.\naverage_learning_Adjusted_of_school: Some metric related to schooling, perhaps average years of schooling adjusted for learning.\nContinent: The continent where the economy is located.\nGDP(2022): The GDP of the economy for the year 2022.\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent: The percentage of people not at all comfortable speaking about anxiety or depression\nRecords: 5,488 Variables: 13 (1 integer, 8 floats, 4 objects) Features: Includes country data, year, percentages for various mental health conditions, income group, average learning, continent, GDP, and comfort speaking about anxiety/depression.\n\nname='mental_health'\ndf=df_mental_health\nprint(f\"{'mental_health'} Dataset - First 5 Rows:\")\ndisplay(df_mental_health.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\nmental_health Dataset - First 5 Rows:\n\nmental_health Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5488 entries, 0 to 5487\nData columns (total 13 columns):\n #   Column                                                         Non-Null Count  Dtype  \n---  ------                                                         --------------  -----  \n 0   Economy                                                        5488 non-null   object \n 1   Code                                                           5488 non-null   object \n 2   Year                                                           5488 non-null   int64  \n 3   Schizophrenia (%)                                              5488 non-null   float64\n 4   Bipolar disorder (%)                                           5488 non-null   float64\n 5   Eating disorders (%)                                           5488 non-null   float64\n 6   Anxiety disorders (%)                                          5488 non-null   float64\n 7   Depression (%)                                                 5488 non-null   float64\n 8   Income group                                                   5432 non-null   object \n 9   average_learning_Adjusted_of_school                            4676 non-null   float64\n 10  Continent                                                      4900 non-null   object \n 11  GDP(2022)                                                      5264 non-null   float64\n 12  not_at_all_comfortable_speaking_anxiety_or_depression_percent  3108 non-null   float64\ndtypes: float64(8), int64(1), object(4)\nmemory usage: 557.5+ KB\nmental_health Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\nIncome group\naverage_learning_Adjusted_of_school\nContinent\nGDP(2022)\nnot_at_all_comfortable_speaking_anxiety_or_depression_percent\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.160560\n0.697779\n0.101855\n4.828830\n4.071831\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n1\nAfghanistan\nAFG\n1991\n0.160312\n0.697961\n0.099313\n4.829740\n4.079531\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n2\nAfghanistan\nAFG\n1992\n0.160135\n0.698107\n0.096692\n4.831108\n4.088358\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n3\nAfghanistan\nAFG\n1993\n0.160037\n0.698257\n0.094336\n4.830864\n4.096190\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n4\nAfghanistan\nAFG\n1994\n0.160022\n0.698469\n0.092439\n4.829423\n4.099582\nLow income\n4.957542\nAsia\n14583.0\nNaN\n\n\n\n\n\n\n\nNone\n\n\n(5488, 13)\n\n\n\nEating Disorder Dataset\n\nThis dataset includes the prevalence of eating disorders among males and females, as well as a combined figure for all genders, across different countries and years.\nCountry: The name of the country.\nCountry Code: The corresponding country code.\nYear: The year of the observation.\nEating_disorders_Male: The prevalence of eating disorders among males.\nEating_disorders_Female: The prevalence of eating disorders among females.\nAll_gender: The prevalence of eating disorders across all genders.\nRecords: 6,420 Variables: 6 (1 integer, 3 floats, 2 objects) Features: Includes country data, year, and eating disorder prevalence separated by male, female, and all genders.\n\nname='eating_disorder'\ndf=df_eating_disorder\n\nprint(f\"{'eating_disorder'} Dataset - First 5 Rows:\")\ndisplay(df_eating_disorder.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\neating_disorder Dataset - First 5 Rows:\n\neating_disorder Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6420 entries, 0 to 6419\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Economy                  6420 non-null   object \n 1   Code                     6150 non-null   object \n 2   Year                     6420 non-null   int64  \n 3   Eating_disorders_Male    6420 non-null   float64\n 4   Eating_disorders_Female  6420 non-null   float64\n 5   All_gender               6420 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 301.1+ KB\neating_disorder Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\nCode\nYear\nEating_disorders_Male\nEating_disorders_Female\nAll_gender\n\n\n\n\n0\nAfghanistan\nAFG\n1990\n0.088487\n0.161867\n0.125177\n\n\n1\nAfghanistan\nAFG\n1991\n0.086048\n0.156910\n0.121479\n\n\n2\nAfghanistan\nAFG\n1992\n0.083625\n0.152412\n0.118018\n\n\n3\nAfghanistan\nAFG\n1993\n0.081628\n0.147938\n0.114783\n\n\n4\nAfghanistan\nAFG\n1994\n0.079439\n0.143980\n0.111710\n\n\n\n\n\n\n\nNone\n\n\n(6420, 6)\n\n\n\nAge When First Anxiety or Depression Dataset\n\nThe dataset contains information on the age at which individuals first experienced anxiety or depression, categorized by different entities (which seem to represent regions or income categories).\nEntity: The region or income category.\nAge: The age category when anxiety or depression was first experienced.\nPercentage: The percentage of individuals in that entity and age category.\nRecords: 42 Variables: 3 (2 objects, 1 float) Features: Includes entity (could be country or other types of entities), age category, and percentage of individuals with first anxiety or depression experience.\n\nname='age_first_depression'\ndf=df_age_first_depression\nprint(f\"{'age_first_depression'} Dataset - First 5 Rows:\")\ndisplay(df_age_first_depression.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\nage_first_depression Dataset - First 5 Rows:\n\nage_first_depression Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 42 entries, 0 to 41\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Entity      42 non-null     object \n 1   Age         42 non-null     object \n 2   Percentage  42 non-null     float64\ndtypes: float64(1), object(2)\nmemory usage: 1.1+ KB\nage_first_depression Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEntity\nAge\nPercentage\n\n\n\n\n0\nAfrica\nAges &lt;13\n1.271836\n\n\n1\nAsia\nAges &lt;13\n7.795371\n\n\n2\nEurope\nAges &lt;13\n9.083381\n\n\n3\nHigh-income countries\nAges &lt;13\n2.473921\n\n\n4\nLower-middle-income countries\nAges &lt;13\n8.800553\n\n\n\n\n\n\n\nNone\n\n\n(42, 3)\n\n\n\nname='df_gdp_mental_health_2017'\ndf=df_gdp_mental_health_2017\nprint(f\"{'name'} Dataset - First 5 Rows:\")\ndisplay(df.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\nname Dataset - First 5 Rows:\n\ndf_gdp_mental_health_2017 Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 166 entries, 0 to 165\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Economy                166 non-null    object \n 1   2017                   166 non-null    object \n 2   Schizophrenia (%)      166 non-null    float64\n 3   Bipolar disorder (%)   166 non-null    float64\n 4   Eating disorders (%)   166 non-null    float64\n 5   Anxiety disorders (%)  166 non-null    float64\n 6   Depression (%)         166 non-null    float64\ndtypes: float64(5), object(2)\nmemory usage: 9.2+ KB\ndf_gdp_mental_health_2017 Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n\n\n\n\n\n\n\nNone\n\n\n(166, 7)\n\n\n\nGDP_per_captita Dataset\n\nThe dataset contains various mental health-related metrics for different economies over several years, with information from different years and countries, with percentages for various mental health conditions such as schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression. Additional columns include economic indicator like GDP per captita.\nEconomy: The name of the economy or country. Year: The GDP_per_captita of the year 2017 of the economy or country. Schizophrenia (%): The prevalence of schizophrenia as a percentage. Bipolar disorder (%): The prevalence of bipolar disorder as a percentage. Eating disorders (%): The prevalence of eating disorders as a percentage. Anxiety disorders (%): The prevalence of anxiety disorders as a percentage. Depression (%): The prevalence of depression as a percentage.\nRecords: 166 Variables, 7 features ( 2 objects, 5 float) features:\n\nname='df_gdp_mental_health_2017'\ndf=df_gdp_mental_health_2017\nprint(f\"{'name'} Dataset - First 5 Rows:\")\ndisplay(df.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\nname Dataset - First 5 Rows:\n\ndf_gdp_mental_health_2017 Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 166 entries, 0 to 165\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Economy                166 non-null    object \n 1   2017                   166 non-null    object \n 2   Schizophrenia (%)      166 non-null    float64\n 3   Bipolar disorder (%)   166 non-null    float64\n 4   Eating disorders (%)   166 non-null    float64\n 5   Anxiety disorders (%)  166 non-null    float64\n 6   Depression (%)         166 non-null    float64\ndtypes: float64(5), object(2)\nmemory usage: 9.2+ KB\ndf_gdp_mental_health_2017 Dataset -Shape:\n\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n\n\n\n\n\n\n\nNone\n\n\n(166, 7)\n\n\n5.Symtoms dataset\nThis dataset contains 2 columns,one is symtoms of health problem and another is related to the respective health problem(mental or Physical).\n\nname='df_mental_physical'\ndf=df_mental_physical\nprint(f\"{'name'} Dataset - First 5 Rows:\")\ndisplay(df.head())\nprint(f\"\\n{name} Dataset - Info:\")\ndisplay(df.info())\nprint(f\"{name} Dataset -Shape:\")\ndisplay(df.shape)\n\nname Dataset - First 5 Rows:\n\ndf_mental_physical Dataset - Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 186 entries, 0 to 185\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Symptoms  186 non-null    object\n 1   Label     186 non-null    object\ndtypes: object(2)\nmemory usage: 3.0+ KB\ndf_mental_physical Dataset -Shape:\n\n\n\n\n\n\n\n\n\nSymptoms\nLabel\n\n\n\n\n0\nI often experience shortness of breath.\nPhysical health\n\n\n1\nI have been feeling persistent fatigue lately.\nPhysical health\n\n\n2\nI am experiencing unexplained weight loss.\nPhysical health\n\n\n3\nI have a consistent cough that won't go away.\nPhysical health\n\n\n4\nI am dealing with frequent headaches.\nPhysical health\n\n\n\n\n\n\n\nNone\n\n\n(186, 2)"
  },
  {
    "objectID": "Exploration.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "href": "Exploration.html#descriptive-statistics-data-visualization-corralation-analysis-data-grouping-and-segmentation",
    "title": "Exploration",
    "section": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation",
    "text": "Descriptive Statistics & Data Visualization & Corralation Analysis & Data Grouping and Segmentation\nCalculate and report basic summary statistics for the datasets.\nThis will include mean, median, mode, standard deviation, and variance for numerical variables. For categorical variables, we’ll provide frequency distributions.​\n\ndescription_stats={\n    \"age_first_depression_or_anxiety\":df_age_first_depression.describe(include='all'),\n    \"eating_disorder\":df_eating_disorder.describe(include='all'),\n    'gdp_mental_health_2017':df_gdp_mental_health_2017.describe(include='all'),\n    \"mental_health\":df_mental_health.describe(include='all'),\n}\ndescription_stats\n\n{'age_first_depression_or_anxiety':         Entity       Age  Percentage\n count       42        42   42.000000\n unique       7         6         NaN\n top     Africa  Ages &lt;13         NaN\n freq         6         7         NaN\n mean       NaN       NaN   16.666667\n std        NaN       NaN   10.060695\n min        NaN       NaN    1.271836\n 25%        NaN       NaN    8.871260\n 50%        NaN       NaN   14.426575\n 75%        NaN       NaN   22.251861\n max        NaN       NaN   42.724724,\n 'eating_disorder':             Economy  Code         Year  Eating_disorders_Male  \\\n count          6420  6150  6420.000000            6420.000000   \n unique          214   205          NaN                    NaN   \n top     Afghanistan   AFG          NaN                    NaN   \n freq             30    30          NaN                    NaN   \n mean            NaN   NaN  2004.500000               0.119775   \n std             NaN   NaN     8.656116               0.068943   \n min             NaN   NaN  1990.000000               0.033360   \n 25%             NaN   NaN  1997.000000               0.071057   \n 50%             NaN   NaN  2004.500000               0.098256   \n 75%             NaN   NaN  2012.000000               0.148705   \n max             NaN   NaN  2019.000000               0.672270   \n \n         Eating_disorders_Female   All_gender  \n count               6420.000000  6420.000000  \n unique                      NaN          NaN  \n top                         NaN          NaN  \n freq                        NaN          NaN  \n mean                   0.273787     0.196781  \n std                    0.214920     0.140066  \n min                    0.056762     0.045083  \n 25%                    0.121105     0.096171  \n 50%                    0.187430     0.144395  \n 75%                    0.352395     0.252282  \n max                    1.395754     1.034012  ,\n 'gdp_mental_health_2017':             Economy     2017  Schizophrenia (%)  Bipolar disorder (%)  \\\n count           166      166         166.000000            166.000000   \n unique          166      166                NaN                   NaN   \n top     Afghanistan  635.789                NaN                   NaN   \n freq              1        1                NaN                   NaN   \n mean            NaN      NaN           0.210030              0.736341   \n std             NaN      NaN           0.040753              0.157692   \n min             NaN      NaN           0.149087              0.411127   \n 25%             NaN      NaN           0.182119              0.626204   \n 50%             NaN      NaN           0.201004              0.708670   \n 75%             NaN      NaN           0.234878              0.845505   \n max             NaN      NaN           0.363326              1.206088   \n \n         Eating disorders (%)  Anxiety disorders (%)  Depression (%)  \n count             166.000000             166.000000      166.000000  \n unique                   NaN                    NaN             NaN  \n top                      NaN                    NaN             NaN  \n freq                     NaN                    NaN             NaN  \n mean                0.260652               4.024311        3.443300  \n std                 0.167330               1.176412        0.621952  \n min                 0.079896               2.066871        2.196154  \n 25%                 0.136695               3.187924        2.958046  \n 50%                 0.204115               3.593617        3.463656  \n 75%                 0.307116               4.712918        3.828232  \n max                 0.943081               8.539931        5.636661  ,\n 'mental_health':             Economy  Code         Year  Schizophrenia (%)  \\\n count          5488  5488  5488.000000        5488.000000   \n unique          196   196          NaN                NaN   \n top     Afghanistan   AFG          NaN                NaN   \n freq             28    28          NaN                NaN   \n mean            NaN   NaN  2003.500000           0.208183   \n std             NaN   NaN     8.078483           0.041998   \n min             NaN   NaN  1990.000000           0.146902   \n 25%             NaN   NaN  1996.750000           0.179452   \n 50%             NaN   NaN  2003.500000           0.198509   \n 75%             NaN   NaN  2010.250000           0.230554   \n max             NaN   NaN  2017.000000           0.375110   \n \n         Bipolar disorder (%)  Eating disorders (%)  Anxiety disorders (%)  \\\n count            5488.000000           5488.000000            5488.000000   \n unique                   NaN                   NaN                    NaN   \n top                      NaN                   NaN                    NaN   \n freq                     NaN                   NaN                    NaN   \n mean                0.716884              0.234023               3.946979   \n std                 0.164246              0.154147               1.134810   \n min                 0.314535              0.073908               2.023393   \n 25%                 0.615732              0.121760               3.178912   \n 50%                 0.693954              0.180378               3.515140   \n 75%                 0.830217              0.278681               4.659540   \n max                 1.206597              0.943991               8.967330   \n \n         Depression (%) Income group  average_learning_Adjusted_of_school  \\\n count      5488.000000         5432                          4676.000000   \n unique             NaN            4                                  NaN   \n top                NaN  High income                                  NaN   \n freq               NaN         1764                                  NaN   \n mean          3.474504          NaN                             7.760170   \n std           0.671741          NaN                             2.528810   \n min           2.139903          NaN                             2.251002   \n 25%           2.955355          NaN                             5.684793   \n 50%           3.461421          NaN                             7.855914   \n 75%           3.877343          NaN                            10.048229   \n max           6.602754          NaN                            12.775495   \n \n        Continent     GDP(2022)  \\\n count       4900  5.264000e+03   \n unique         6           NaN   \n top       Africa           NaN   \n freq        1428           NaN   \n mean         NaN  3.929245e+05   \n std          NaN  1.451071e+06   \n min          NaN  2.230000e+02   \n 25%          NaN  1.240875e+04   \n 50%          NaN  4.239550e+04   \n 75%          NaN  2.550042e+05   \n max          NaN  1.796317e+07   \n \n         not_at_all_comfortable_speaking_anxiety_or_depression_percent  \n count                                         3108.000000              \n unique                                                NaN              \n top                                                   NaN              \n freq                                                  NaN              \n mean                                            30.009805              \n std                                             11.940808              \n min                                              5.833535              \n 25%                                             21.170624              \n 50%                                             30.920343              \n 75%                                             37.729200              \n max                                             58.778120              }\n\n\n\nMental Health Dataset\nCorrelation Heatmap: To see how different types of mental health disorders are correlated with each other.\nTime Series Plot: To observe the trend of a specific disorder over time for a particular economy or aggregated globally.\nBar Chart: To compare the prevalence of different disorders in a specific year across multiple economies.\nBox Plot: To visualize the distribution of prevalence rates for a particular disorder across different income groups or continents.\nScatter Plot: To examine the relationship between GDP and the prevalence of a particular disorder or the discomfort in speaking about mental health issues.\nHistograms for mental disorders\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the aesthetics for the plots\nsns.set(style=\"whitegrid\")\n\n# Define a function to create histograms for given columns\ndef plot_histograms(data, columns, bins=20, figsize=(15, 5)):\n    fig, axes = plt.subplots(1, len(columns), figsize=figsize)\n    for ax, col in zip(axes, columns):\n        sns.histplot(data[col].dropna(), bins=bins, ax=ax, kde=True)  # Drop NaN for plotting\n        ax.set_title(f'Distribution of {col}')\n    plt.tight_layout()\n    return fig\n\n# Select columns to plot for Mental Health Data (excluding 'Year' and non-numerical columns)\nmental_health_columns = ['Schizophrenia (%)', 'Bipolar disorder (%)', \n                         'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)']\n\n# Plot histograms for the selected columns\nhistograms_mental_health = plot_histograms(df_mental_health, mental_health_columns)\n\n\n\n\nTime Series Plot\nGlobal trend of ‘Depression (%)’ over time.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Assuming df_mental_health is your DataFrame\n# Replace 'df_mental_health' with the actual name of your DataFrame if different\n\n# Aggregate the data by Year and calculate the mean for each disorder\nannual_prevalence = df_mental_health.groupby('Year').mean().reset_index()\n\n# Plotting each mental health disorder prevalence over time\nplt.figure(figsize=(14, 7))\n\n# For each disorder, we plot a line chart\nfor column in annual_prevalence.columns[1:-3]:  # Selecting only disorder columns\n    plt.plot(annual_prevalence['Year'], annual_prevalence[column], label=column)\n\n# Adding titles and labels\nplt.title('Mental Health Disorders Prevalence Over Time')\nplt.xlabel('Year')\nplt.ylabel('Prevalence (%)')\n\n# Adjusting the Y-axis ticks to have increments of 0.3\nplt.yticks(np.arange(start=0, stop=annual_prevalence.iloc[:, 1:-3].max().max() + 0.1, step=0.1))\n\nplt.legend(loc='upper right')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/3393605761.py:9: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  annual_prevalence = df_mental_health.groupby('Year').mean().reset_index()\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Assuming df_mental_health is your DataFrame\n# Replace 'df_mental_health' with the actual name of your DataFrame if different\n\n# Aggregate the data by Year and calculate the mean for each disorder\nannual_prevalence = df_mental_health.groupby('Year').mean().reset_index()\n\n# Determine the number of disorders to plot\nnum_disorders = len(annual_prevalence.columns[1:-3])\n\n# Creating subplots\nfig, axes = plt.subplots(num_disorders, 1, figsize=(14, 6 * num_disorders))\n\n# Plotting each disorder in a separate subplot\nfor i, column in enumerate(annual_prevalence.columns[1:-3]):\n    axes[i].plot(annual_prevalence['Year'], annual_prevalence[column], label=column)\n    axes[i].set_title(column)\n    axes[i].set_xlabel('Year')\n    axes[i].set_ylabel('Prevalence (%)')\n    axes[i].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/2989151303.py:8: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  annual_prevalence = df_mental_health.groupby('Year').mean().reset_index()\n\n\n\n\n\nAnxiety disorders and depression show higher prevalence rates compared to other disorders like schizophrenia, bipolar disorder, and eating disorders. This could indicate that anxiety and depression are more common mental health concerns in the population.\nThe prevalence rates for disorders like schizophrenia, bipolar disorder, eating disorders, anxiety disorders, and depression appear relatively stable over the years in the dataset. But e schizophrenia, bipolar disorder, eating disorders, anxiety disorders slightly increases over the years, while the depression does not.\nThe Bar plot\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_income_group=df_mental_health.groupby('Income group').mean()\n# print(df_income_group)\ndf_income_group=df_income_group.drop(columns=['Year','GDP(2022)','not_at_all_comfortable_speaking_anxiety_or_depression_percent','average_learning_Adjusted_of_school'])\n# print(type(df_income_group))\nprint(df_income_group)\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 6))\ndf_income_group.plot(kind='bar', ax=ax)\n\n# Customization\nax.set_title('Mean Percentage of Mental Health Disorders by Income Group')\nax.set_ylabel('Mean Percentage')\nax.set_xlabel('Income Group')\nplt.xticks(rotation=45)\nplt.legend(title='Mental Health Disorders')\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n                     Schizophrenia (%)  Bipolar disorder (%)  \\\nIncome group                                                   \nHigh income                   0.243514              0.822296   \nLow income                    0.164570              0.626751   \nLower middle income           0.192946              0.631165   \nUpper middle income           0.202089              0.724714   \n\n                     Eating disorders (%)  Anxiety disorders (%)  \\\nIncome group                                                       \nHigh income                      0.399097               4.707860   \nLow income                       0.102740               3.500061   \nLower middle income              0.134964               3.493755   \nUpper middle income              0.202037               3.739220   \n\n                     Depression (%)  \nIncome group                         \nHigh income                3.575551  \nLow income                 3.804767  \nLower middle income        3.495127  \nUpper middle income        3.172075  \n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/4104045031.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_income_group=df_mental_health.groupby('Income group').mean()\n\n\n\n\n\nThe updated heatmap now includes the one-hot encoded categorical variables ‘Income group’ and ‘Continent’. Due to the number of categories, the annotations were turned off to keep the heatmap readable. Each square still represents the correlation between two variables, with warmer colors indicating a stronger positive correlation and cooler colors a stronger negative correlation.\n\nData Grouping and Segmentation\nThe Box Plot\nBox Plot: Distribution of ‘Bipolar disorder (%)’ prevalence rates across different income groups\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Assuming df_mental_health is your DataFrame\n# Replace 'df_mental_health' with the actual name of your DataFrame if different\n\nmost_recent_year = df_mental_health['Year'].max()\n\n# Disorders to plot\ndisorders = ['Bipolar disorder (%)', 'Schizophrenia (%)', 'Eating disorders (%)', 'Anxiety disorders (%)', 'Depression (%)']\n\n# Creating box plots for each disorder\nfor disorder in disorders:\n    # Filter the data for the disorder for the most recent year\n    disorder_data_recent_year = df_mental_health[df_mental_health['Year'] == most_recent_year]\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(data=disorder_data_recent_year, x='Income group', y=disorder)\n    plt.title(f'Distribution of {disorder} Across Income Groups in {most_recent_year}')\n    plt.xlabel('Income Group')\n    plt.ylabel(f'{disorder} Prevalence (%)')\n    plt.grid(axis='y')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot provides insights into the median prevalence rates, the interquartile ranges, and any potential outliers within each income group category.\nThe Box Plot above shows the distribution of ‘Bipolar disorder (%)’, ‘Schizophrenia (%)’, ‘Eating disorders (%)’, ’Anxiety disorders (%)’prevalence vary by income group,while the Depression doesn’t vary by income group.\nThe scatter plot\nScatter Plot 1: Relationship between ‘average learning adjusted years of school’ and ‘Depression (%)’\n\n# This function will adjust the number of subplots based on the number of disorders.\n\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\neducation_col = 'average_learning_Adjusted_of_school'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\n# Now let's use the function to create scatter plots\ncreate_scatter_plots(df_mental_health, education_col, mental_health_disorders, 2)\n\n\n\n\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone.\nScatter Plot 2: Relationship between ‘GDP(2022)’ and ‘Depression (%)’\n\n# This function will adjust the number of subplots based on the number of disorders.\n\ndef create_scatter_plots(dataframe, x_col, y_cols, row_col_count):\n    # Calculate the number of rows and columns needed for the subplots\n    total_plots = len(y_cols)\n    nrows = (total_plots + row_col_count - 1) // row_col_count  # Ceiling division\n    ncols = row_col_count\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 5))\n    if nrows == 1:\n        axes = [axes]  # Ensure axes is always a list\n    else:\n        axes = axes.flatten()\n\n    # Generate scatter plots\n    for i, y_col in enumerate(y_cols):\n        sns.scatterplot(ax=axes[i], data=dataframe, x=x_col, y=y_col)\n        axes[i].set_title(f'{y_col} vs {x_col}')\n        axes[i].set_xlabel(x_col)\n        axes[i].set_ylabel(f'Prevalence of {y_col}')\n\n    # Hide any unused subplots\n    for j in range(i+1, len(axes)):\n        axes[j].set_visible(False)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\ngdp_col = 'GDP(2022)'\nmental_health_disorders = [\n    'Schizophrenia (%)',  # The prevalence of schizophrenia as a percentage\n    'Bipolar disorder (%)',  # The prevalence of bipolar disorder as a percentage\n    'Eating disorders (%)',  # The prevalence of eating disorders as a percentage\n    'Anxiety disorders (%)' , # The prevalence of anxiety disorders as a percentage\n    'Depression (%)'  # The prevalence of depression as a percentage\n]\n\n# Now let's use the function to create scatter plots\ncreate_scatter_plots(df_mental_health, gdp_col, mental_health_disorders, 2)\n\n\n\n\nThe scatter plots show that there’s a cluster of data points at lower GDP values with a wide range of mental health problems prevalence and they does not appear to be a clear trend or correlation between GDP and the prevalence of these mental health problems.\n\n\nCorralation Analysis\nCorrelation Heatmap\n‘Income group’ and ‘Continent’ to the list of columns for the correlation matrix, which are categorical variables. The correlation matrix typically requires numerical variables. To include these categorical variables, I convert them into a numerical format using techniques such as one-hot encoding.\n\nimport numpy as np\ncorrelation_columns = [\n    'Schizophrenia (%)', \n    'Bipolar disorder (%)', \n    'Eating disorders (%)', \n    'Anxiety disorders (%)', \n    'Depression (%)', \n    'GDP(2022)',\n    'average_learning_Adjusted_of_school',\n    # 'Income group',\n    # 'Continent',\n\n]\n# One-hot encoding the 'Income group' and 'Continent' columns to include them in the correlation matrix\nencoded_data = pd.get_dummies(df_mental_health, columns=['Income group', 'Continent'])\n\n# Updating the correlation_columns list to include the newly created one-hot encoded columns\nnew_correlation_columns = correlation_columns + list(encoded_data.columns[encoded_data.columns.str.startswith('Income group_')]) + list(encoded_data.columns[encoded_data.columns.str.startswith('Continent_')])\n\n# Calculate the new correlation matrix including the one-hot encoded columns\nnew_correlation_matrix = encoded_data[new_correlation_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(new_correlation_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask\nsns.heatmap(new_correlation_matrix, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .5})\n\n# Add title\nplt.title('Correlation Heatmap for Mental Health Metrics with Categorical Variables')\n\n# Show the heatmap\nplt.show()\n\n\n\n\n\n\n\nEating disorder Dataset\nTime Series Line Plot: Showing the trend of eating disorder prevalence over years for a specific country or averaged globally.\nBar Chart: Comparing the prevalence of eating disorders between males and females across different countries.\nBox Plot: Displaying the distribution of eating disorder prevalence for all countries across different years to see the variability and outliers.\nHeatmap: Visualizing the prevalence of eating disorders across countries and years in a color-coded format.\nScatter Plot: Comparing the male vs. female prevalence of eating disorders to see the correlation between genders.\nTime Series Line Plot:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the average prevalence of eating disorders for each year across all countries\naverage_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n# Time Series Line Plot for the global trend\nplt.figure(figsize=(14, 7))\n\n# Plotting the trends for each gender and all genders combined\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Male', label='Male')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='Eating_disorders_Female', label='Female')\nsns.lineplot(data=average_eating_disorders_per_year, x='Year', y='All_gender', label='All Genders')\n\nplt.title('Global Average Prevalence of Eating Disorders Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Prevalence')\nplt.legend()\nplt.grid(True)\n\n# Show the line plot\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/758625183.py:5: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  average_eating_disorders_per_year = df_eating_disorder.groupby('Year').mean().reset_index()\n\n\n\n\n\nThe line plot above shows the global average prevalence of eating disorders over time for males, females, and all genders combined. The trends can be compared to see how the prevalence has changed over the years. Eating disorders were more than twice as prevalent among females (3.8%) than males (1.5%)\n\nData Grouping and Segmentation\nGrouped bar chart: compare the prevalence of eating disorders between males and females across different countries. For clarity, we will take a subset of countries to avoid cluttering the chart.\n\n# Randomly select 10 countries for comparison\nrandom_countries = df_eating_disorder['Economy'].drop_duplicates().sample(10, random_state=1)\n\n# Filter the data for the selected countries and the latest year available\nlatest_year = df_eating_disorder['Year'].max()\ncomparison_data = df_eating_disorder[(df_eating_disorder['Economy'].isin(random_countries)) & (df_eating_disorder['Year'] == latest_year)]\n\n# Prepare data for grouped bar chart\ncomparison_data['Economy'] = comparison_data['Economy'].astype('category')\ncomparison_data['Economy'].cat.set_categories(random_countries, inplace=True)\ncomparison_data.sort_values('Economy', inplace=True)\n\n# Determine the positions of the bars\npos = np.arange(len(random_countries))\nbar_width = 0.4\n\n# Create the bar chart\nplt.figure(figsize=(14, 7))\n\n# Plot for males\nplt.bar(pos - bar_width/2, comparison_data['Eating_disorders_Male'], bar_width, color='lightblue', label='Male')\n\n# Plot for females\nplt.bar(pos + bar_width/2, comparison_data['Eating_disorders_Female'], bar_width, color='lightgreen', label='Female')\n\nplt.title(f'Comparison of Eating Disorders Prevalence by Gender in {latest_year}')\nplt.xlabel('Economy')\nplt.ylabel('Prevalence of Eating Disorders')\nplt.xticks(pos, random_countries, rotation=45, ha='right')\nplt.legend()\n\n# Show the bar chart\nplt.tight_layout()\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/2505907414.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  comparison_data['Economy'] = comparison_data['Economy'].astype('category')\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/2505907414.py:10: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  comparison_data['Economy'].cat.set_categories(random_countries, inplace=True)\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/2505907414.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  comparison_data.sort_values('Economy', inplace=True)\n\n\n\n\n\nThe bar chart above compares the prevalence of eating disorders between males (in light blue) and females (in pink) for a randomly selected subset of 10 countries in the latest year available in the dataset.\n\n\nIdentifying Outliers\nlooking for countries with prevalence rates that are significantly higher or lower than the global average for each year and by gender.\n\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nMale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.11%. The range of prevalence rates is quite broad, with the lowest around 0.03% and the highest at about 0.67%. The interquartile range (middle 50% of the data) spans from approximately 0.08% to 0.16%, indicating that half of the reported rates fall within this range.\nFemale Eating Disorder Prevalence Rate:\nThe median prevalence rate is approximately 0.21%, which is notably higher than that of males. The prevalence rates for females also show a broad range, from about 0.06% to 1.40%. The interquartile range for females is wider than for males, ranging from about 0.14% to 0.40%, reflecting greater variability in the rates reported for females.\nOutlier Identification:\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\nRemove the outliers for year 2019\n\n# Calculate the interquartile range for males\nQ1_male = latest_data['Eating_disorders_Male'].quantile(0.25)\nQ3_male = latest_data['Eating_disorders_Male'].quantile(0.75)\nIQR_male = Q3_male - Q1_male\nlower_bound_male = Q1_male - 1.5 * IQR_male\nupper_bound_male = Q3_male + 1.5 * IQR_male\n\n# Remove outliers for males\nlatest_data_no_outliers_male = latest_data[(latest_data['Eating_disorders_Male'] &gt;= lower_bound_male) & \n                                           (latest_data['Eating_disorders_Male'] &lt;= upper_bound_male)]\n\n# Calculate the interquartile range for females\nQ1_female = latest_data['Eating_disorders_Female'].quantile(0.25)\nQ3_female = latest_data['Eating_disorders_Female'].quantile(0.75)\nIQR_female = Q3_female - Q1_female\nlower_bound_female = Q1_female - 1.5 * IQR_female\nupper_bound_female = Q3_female + 1.5 * IQR_female\n\n# Remove outliers for females\nlatest_data_no_outliers_female = latest_data[(latest_data['Eating_disorders_Female'] &gt;= lower_bound_female) & \n                                             (latest_data['Eating_disorders_Female'] &lt;= upper_bound_female)]\n\n# Now let's plot the data without outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence without outliers\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data_no_outliers_male['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year) + ' (No Outliers)')\n\n# Box plot for female prevalence without outliers\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data_no_outliers_female['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year) + ' (No Outliers)')\n\nplt.tight_layout()\nplt.show()\n#save the data sets\n# latest_data.to_csv('./Data/eating_disorder_male_female_rm_outliers.csv',index=False)\n\n\n\n\n\n\nHeatmap\n\n# For the heatmap , select all years of data for US only.\nus_data = df_eating_disorder[df_eating_disorder['Economy'] == 'United States'].pivot('Year', 'Code', 'All_gender')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(us_data, cmap=\"YlOrRd\", linewidths=.5, annot=True, fmt=\".2f\")\n\nplt.title('Heatmap of Eating Disorders Prevalence Across Years in US')\nplt.xlabel('Country Code')\nplt.ylabel('Year')\n# Show the heatmap\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_22998/2620888285.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  us_data = df_eating_disorder[df_eating_disorder['Economy'] == 'United States'].pivot('Year', 'Code', 'All_gender')\n\n\n\n\n\nHeatmap\nThe heatmap above visualizes the prevalence of eating disorders across different years in China. The prevalence of eating disorders remained relatively stable over the 30-year period, with a range between 0.41% to 0.46%.\nThe mid-1990s show the highest prevalence, with a peak around 1995 and 1996.\nFrom 2014 onwards, there appears to be a slight decrease in prevalence, with rates consistently around 0.42% to 0.43%.\ncreate a scatter plot to compare the male vs. female prevalence of eating disorders to see the correlation between genders. We’ll use data from the most recent year for all countries to see the relationship.​\n\n# Scatter Plot: Comparing male vs. female prevalence of eating disorders\n# Filter the dataset for the latest year\nscatter_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\nplt.figure(figsize=(10, 7))\n\n# Plotting the scatter plot for male vs. female prevalence\nsns.scatterplot(data=scatter_data, x='Eating_disorders_Male', y='Eating_disorders_Female', hue='Country', legend=False)\n\nplt.title(f'Correlation Between Male and Female Prevalence of Eating Disorders in {latest_year}')\nplt.xlabel('Male Prevalence')\nplt.ylabel('Female Prevalence')\n\n# Plot a 45 degree line to show y=x for reference\nmax_val = max(scatter_data['Eating_disorders_Male'].max(), scatter_data['Eating_disorders_Female'].max())\nplt.plot([0, max_val], [0, max_val], '--', color='gray')\n\n# Show the scatter plot\nplt.show()\n\n\n\n\nThe scatter plot above compares the prevalence of eating disorders between males (on the x-axis) and females (on the y-axis) for the most recent year available in the dataset. Each point represents a country, and the gray dashed line indicates where the prevalence would be equal for both genders. Points above the line show countries where the prevalence is higher in females compared to males, which seems to be the case for all countries shown.\n\n\n\n3.Age when first have depression or anxiety Data\nPie chart\nShowing the distribution of age categories for the onset of anxiety or depression for the “World” entity, I’ll first filter the dataset for the ‘World’ entity, and then plot the data.\n\n# Filter the data for the 'World' entity\nworld_data = df_age_first_depression[df_age_first_depression['Entity'] == 'World']\n\n# Pie Chart: Showing the distribution of age categories for the World entity\nplt.figure(figsize=(8, 8))\nplt.pie(world_data['Percentage'], labels=world_data['Age'], autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of Age Categories for First Anxiety or Depression in the World')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n# Show the pie chart\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nHeatmap\nvisualize the relationship between entities and age groups\n\n# Create a pivot table for the heatmap\nheatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n# Heatmap: Visualize the percentage of individuals by entity and age category\nplt.figure(figsize=(12, 10))\nheatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Heatmap of Percentage by Entity and Age Category for First Anxiety or Depression')\nplt.xlabel('Age Category')\nplt.ylabel('Entity')\n\n# Show the heatmap\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/601293865.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  heatmap_data = df_age_first_depression.pivot(\"Entity\", \"Age\", \"Percentage\")\n\n\n\n\n\n\nStacked bar chart\nGrouping the data by Entity and visualizing the distribution of the Percentage across different Age brackets.\n\n# Stacked Bar Chart: Create a crosstab to prepare data for the stacked plot\nstacked_data = pd.crosstab(index=df_age_first_depression['Entity'], columns=df_age_first_depression['Age'], values=df_age_first_depression['Percentage'], aggfunc='sum', normalize='index')\n\n# Plotting the stacked bar chart\nstacked_data.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='viridis')\nplt.title('Stacked Bar Chart of Age Categories for First Anxiety or Depression by Entity')\nplt.xlabel('Entity')\nplt.ylabel('Percentage')\nplt.legend(title='Age Category', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xticks(rotation=45, ha='right')\n\n# Show the stacked bar chart\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n4.GDP_mental_health_2017_Dataset\nScatter plots\nvisualize the potential correlation between GDP per capita and each mental health indicator.\n\n# Renaming the '2017' column to 'GDP_per_capita' for clarity\ndf_gdp_mental_health_2017.rename(columns={'2017': 'GDP_per_capita'}, inplace=True)\n\n# Ensuring 'GDP_per_capita' is a numeric column\ndf_gdp_mental_health_2017['GDP_per_capita'] = pd.to_numeric(df_gdp_mental_health_2017['GDP_per_capita'], errors='coerce')\n\n# List of mental health indicators to plot\nmental_health_indicators = ['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)', 'Anxiety disorders (%)']\n\n# Scatter plots of GDP per capita vs each mental health indicator\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plotting scatter plots\nfor i, indicator in enumerate(mental_health_indicators):\n    sns.scatterplot(ax=axes[i//2, i%2], \n                    data=df_gdp_mental_health_2017, \n                    x='GDP_per_capita', \n                    y=indicator)\n    axes[i//2, i%2].set_title(f'GDP per Capita vs {indicator}')\n    axes[i//2, i%2].set_xlabel('GDP per Capita')\n    axes[i//2, i%2].set_ylabel(indicator)\n\n# Adjusting layout\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatter plots have been generated, showing the relationship between GDP per capita and each mental health indicator.\nSchizophrenia (%) vs GDP per Capita: There does not appear to be a clear trend indicating that the prevalence of schizophrenia is related to GDP per capita. The distribution of points seems scattered without a distinct pattern.\nBipolar Disorder (%) vs GDP per Capita: Similar to schizophrenia, bipolar disorder prevalence does not show a clear correlation with GDP per capita based on the scatter plot.\nEating Disorders (%) vs GDP per Capita: For eating disorders, there might be a slight positive trend, suggesting that higher GDP per capita could be associated with a higher reported prevalence of eating disorders. This could be due to better reporting and diagnosis in higher-income countries, but further statistical analysis would be needed to confirm any correlation.\nAnxiety Disorders (%) vs GDP per Capita: The scatter plot does not reveal a strong correlation between the prevalence of anxiety disorders and GDP per capita. However, there is some spread in the data that might warrant a closer look with more sophisticated statistical tools.\nGeneral Observations: The prevalence rates for the mental health conditions do not show strong visual evidence of a correlation with GDP per capita. This might suggest that mental health issues are widespread and not necessarily directly related to the economic status of a country.\nIt is important to note that these are preliminary insights based solely on visual analysis. To draw more concrete conclusions, you would need to perform a quantitative analysis, such as calculating the correlation coefficients or conducting regression analysis. Additionally, it’s important to consider other factors that could influence mental health statistics, such as healthcare access, cultural attitudes towards mental health, and the quality of data reporting in different countries.\nHeatmap\nThe correlation matrix to understand how different indicators are related to each other. This will include GDP per capita and the mental health indicators.​\n\n# Correlation matrix heatmap\nplt.figure(figsize=(10, 8))\n\n# Calculate correlation matrix\ncorr_matrix = df_gdp_mental_health_2017.corr()\n\n# Generate a heatmap\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n\n# Title\nplt.title('Correlation Matrix Heatmap')\n\nplt.show()\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_4733/1859813442.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_matrix = df_gdp_mental_health_2017.corr()\n\n\n\n\n\nThe heatmap displays the correlation coefficients between the GDP per capita and mental health indicators. Values close to 1 or -1 indicate a strong positive or negative correlation, respectively, while values close to 0 indicate a weak or no correlation.\nHigher GDP per capita could be associated with a higher reported prevalence of eating disorders and Schizophrenia."
  },
  {
    "objectID": "Exploration.html#hypothesis-generation",
    "href": "Exploration.html#hypothesis-generation",
    "title": "Exploration",
    "section": "Hypothesis Generation",
    "text": "Hypothesis Generation\n\nAge First Depression or Anxiety Data\n\nThe cultural, economic, and social environments in high-income countries might contribute to an earlier age of onset for anxiety or depression.\nIn regions with lower-middle-income levels, the onset of anxiety or depression might occur later, potentially due to different life stressors or lower levels of mental health awareness and reporting.\n\n\n\nEating disorder Dataset\n\nThe prevalence of eating disorders is higher in countries with higher GDP per capita, which could reflect better diagnostic capabilities or different societal pressures.\nThere may be significant differences in the prevalence of eating disorders between genders across different countries, potentially reflecting cultural, social, or biological factors.\n\n\n\nGDP_mental_health_2017_Dataset\n\nHigher GDP per capita might be associated with higher reported rates of mental health disorders, potentially due to better health services and reporting mechanisms.\nThe prevalence of different mental health disorders will vary across economies, possibly due to genetic, environmental, cultural, or socioeconomic factors.\n\n\n\nMental Health Dataset\n\nEducational outcomes may be associated with the prevalence or reporting of certain mental health conditions, such as schizophrenia and eating disorders.\nThe lack of correlation between GDP and comfort in discussing mental health could imply that economic development does not directly influence cultural attitudes toward mental health openness.\n\n3.The correlations between different mental health conditions suggest the possibility of common underlying risk factors or increased comorbidity."
  },
  {
    "objectID": "Exploration.html#identiy-outliers",
    "href": "Exploration.html#identiy-outliers",
    "title": "Exploration",
    "section": "Identiy Outliers",
    "text": "Identiy Outliers\n\nAge First Depression or Anxiety Data\nThe data does not have individual country-level entries, so it’s not possible to identify outliers in the traditional sense. However, the “Don’t Know/Refused” category stands out and could be considered an anomaly in the reporting of mental health data.\n\n\nEating disorder Dataset\nWe will look for countries with prevalence rates that are significantly higher or lower than the global average for each year and by gender.\n\n# For the trend analysis, let's select the most recent year available for all countries\nlatest_year = df_eating_disorder['Year'].max()\n\n# Now, let's prepare the data for that year to identify outliers\nlatest_data = df_eating_disorder[df_eating_disorder['Year'] == latest_year]\n\n# Generating box plots for male and female prevalence rates to identify outliers\nplt.figure(figsize=(16, 8))\n\n# Box plot for male prevalence\nplt.subplot(1, 2, 1)\nsns.boxplot(y=latest_data['Eating_disorders_Male'])\nplt.title('Male Eating Disorder Prevalence Rate in ' + str(latest_year))\n\n# Box plot for female prevalence\nplt.subplot(1, 2, 2)\nsns.boxplot(y=latest_data['Eating_disorders_Female'])\nplt.title('Female Eating Disorder Prevalence Rate in ' + str(latest_year))\n\nplt.tight_layout()\nplt.show()\n\n# We can also calculate the summary statistics for each gender\nmale_stats = latest_data['Eating_disorders_Male'].describe()\nfemale_stats = latest_data['Eating_disorders_Female'].describe()\n\n(male_stats, female_stats)\n\n\n\n\n(count    214.000000\n mean       0.131786\n std        0.074714\n min        0.033747\n 25%        0.081544\n 50%        0.111878\n 75%        0.156691\n max        0.672270\n Name: Eating_disorders_Male, dtype: float64,\n count    214.000000\n mean       0.299529\n std        0.230234\n min        0.057713\n 25%        0.141947\n 50%        0.210908\n 75%        0.398235\n max        1.395754\n Name: Eating_disorders_Female, dtype: float64)\n\n\nFor males, any country with a prevalence rate significantly higher than 0.16% could be considered an outlier. For females, countries with rates above 0.40% would be outliers, with the maximum reported rate being quite extreme at 1.40%.\n\n\nGDP_mental_health_2017_Dataset\n\n\nMental Health Dataset"
  },
  {
    "objectID": "Exploration.html#report-and-discuss-methods-and-findings",
    "href": "Exploration.html#report-and-discuss-methods-and-findings",
    "title": "Exploration",
    "section": "Report and discuss methods and findings",
    "text": "Report and discuss methods and findings\n\nAge First Depression or Anxiety Data\nThe bar chart and table display the distribution of the age of onset for anxiety or depression across various entities, which include continents, income categories, and the world overall. From the visualization and the data, we can observe the following:\nVariability in Age of Onset: There is considerable variability in the age of onset for anxiety or depression among different entities. For example, in Europe, a higher percentage reports the onset during the age brackets of “Ages 20-29” and “Ages ≥40”, whereas in Africa, the onset is more reported in the younger age bracket “Ages 13-19” and “Ages 20-29”.\nHigh-Income Countries: In high-income countries, the age bracket “Ages 13-19” has the highest reported onset, followed by “Ages ≥40” and “Ages 20-29”.\nLower-Middle-Income Countries: Here, the age bracket “Ages 20-29” has the highest reported onset, suggesting that the onset of anxiety or depression may occur later compared to high-income countries.\nDon’t Know/Refused: A significant portion of respondents across all entities did not provide a specific age bracket or refused to answer, with Africa having the highest percentage in this category. This could reflect cultural differences in discussing mental health or data collection challenges.\n\n\nEating disorder Dataset\nThe prevalence of eating disorders is significantly higher in females than in males, which aligns with the general understanding of these conditions.\nThe variability in prevalence rates among females is greater, indicating that eating disorders in females may be more influenced by a variety of factors, possibly including cultural aspects, societal pressures, or biological predispositions.\nThe global average prevalence of eating disorders over time for males, females, and all genders combined is growing up.\n\n\nGDP_mental_health_2017_Dataset\nEating Disorders (%) vs GDP per Capita: For eating disorders, there might be a slight positive trend, suggesting that higher GDP per capita could be associated with a higher reported prevalence of eating disorders.\n\n\nMental Health Dataset\nwe can draw the conclusion that the Eating disorders and Anxiety disorders, Bipolar disorder (%) are not related to GDP.\nSchizophrenia: There appears to be a cluster of points towards the lower end of the educational scale with varying prevalence rates. As education levels increase, the prevalence rates seem to spread out, indicating a less clear relationship.\nBipolar Disorder: The data points are dispersed across the educational spectrum with no clear trend indicating a strong relationship between education and the prevalence of bipolar disorder.\nEating Disorders: This plot shows a somewhat more dispersed distribution, suggesting that higher education levels might not necessarily correlate with higher or lower prevalence rates of eating disorders.\nAnxiety Disorders: There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and anxiety disorders may be influenced by factors other than education alone.\nDepression (%): There’s a wide spread of prevalence rates at all levels of education, suggesting that the relationship between education and Depression disorders may be influenced by factors other than education alone."
  },
  {
    "objectID": "Exploration.html#tools-and-software",
    "href": "Exploration.html#tools-and-software",
    "title": "Exploration",
    "section": "Tools and Software",
    "text": "Tools and Software\n\nPandas: For data manipulation and analysis.\nMatplotlib: For creating the bar chart visualization.\nSeaborn: Although not used in the last plot, it is an excellent tool for creating heatmaps and other complex visualizations."
  },
  {
    "objectID": "reduction.html",
    "href": "reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "first,we after looking at the datasets and the features, we can see that there are some features that are highly correlated with each other. This is a problem because it can lead to overfitting. To solve this problem, we can use dimensionality reduction techniques to reduce the number of features in the dataset. This will help us to avoid overfitting and also reduce the computational cost of the model.\n\n\n\nload the dataset\nHandle missing values and non-numeric entries.\nConvert categorical data to numerical.\nNormalize the data. so the numerical data has a mean of 0 and standard deviation of 1.(This is crucial for PCA and t-SNE because they are sensitive to the scale of the data.)\n\n\n#1.load data\nimport pandas as pd\ndf_reduction=pd.read_csv('./Data/Reduction.csv')\ndf_reduction.head()\n# df_reduction = df_reduction.drop('Economy', axis=1)\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\naverage_learning_Adjusted_of_school\nIncome group\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n4.957542\nLow income\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n8.526723\nUpper middle income\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n7.045445\nLower middle income\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\nNaN\nHigh income\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n4.193318\nLower middle income\n\n\n\n\n\n\n\n\n\n# 2.Handling Non-Numeric Values\n# Non-numeric entries such as \"no data\" are replaced with NaN, so they can be imputed later.\n#  Replace non-numeric entries with NaN\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n#checking for missing values\ndf_reduction.isnull().sum()\n\n# Replace non-numeric entries with NaN\ndf_reduction.iloc[:, 1:8] = df_reduction.iloc[:, 1:8].apply(pd.to_numeric, errors='coerce')\n# [1:8] means from column 1 to column 7 because column 0 and 8 are not numeric\n# The apply(pd.to_numeric, errors='coerce') method attempts to convert all entries in the specified columns to numeric values.\n# If it encounters any entry that cannot be converted (like a string 'no data'), it replaces it with NaN (Not a Number), a standard missing dat\n\n\n# Impute missing values with the mean\nimputer = SimpleImputer(strategy='mean')\ndf_reduction.iloc[:, 1:8] = imputer.fit_transform(df_reduction.iloc[:, 1:8])\n\n# 3.Encoding Categorical Data, so Categorical data is encoded numerically.\n# Label encoding assigns each unique value to a different integer.\n# we only encode the Income group column because it is ordinal, we don't encode the the Economy because it is nominal and it's not relevant to the analysis.\nlabel_encoder = LabelEncoder()\ndf_reduction['Income group'] = label_encoder.fit_transform(df_reduction['Income group'])\ndf_reduction.isnull().sum()\n# if there is missing value in the Income group column, we can use the following code to fill the missing value with the most frequent value\n# df_reduction['Income group'] = df_reduction['Income group'].fillna(df_reduction['Income group'].mode()[0])\nscaled_data_df = pd.DataFrame(df_reduction, columns=df_reduction.columns[1:])\n\n# 4.Normalizing the Data (The data is normalized to ensure that all features contribute equally to the analysis.)\n# Normalization standardizes the range of features in your dataset, ensuring that each feature contributes equally to the analysis and is not dominated by those with larger magnitudes.\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_reduction.iloc[:, 1:])\n\n# Display the first few rows of the processed data\nscaled_data_df = pd.DataFrame(scaled_data, columns=df_reduction.columns[1:])\nprint(scaled_data_df.head())\n#save our data\nscaled_data_df.to_csv('./Data/Reduction_cleaned.csv', index=False)\n\n       2017  Schizophrenia (%)  Bipolar disorder (%)  Eating disorders (%)  \\\n0 -0.702995          -1.079808             -0.179703             -0.920185   \n1 -0.501116          -0.221633             -0.202660             -0.519143   \n2 -0.527644          -0.298235              0.523775             -0.281972   \n3  1.340754           1.316321              1.443806              2.301250   \n4 -0.526368          -0.916482             -0.715177             -0.521562   \n\n   Anxiety disorders (%)  Depression (%)  average_learning_Adjusted_of_school  \\\n0               0.731688        1.117681                        -1.204944e+00   \n1              -0.544877       -1.991509                         2.928827e-01   \n2               0.888053        0.351238                        -3.287439e-01   \n3               1.092253        0.461607                        -3.727291e-16   \n4              -0.620196        1.156607                        -1.525655e+00   \n\n   Income group  \n0     -0.439277  \n1      1.199375  \n2      0.380049  \n3     -1.258603  \n4      0.380049  \n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_67549/2609797013.py:14: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n  df_reduction.iloc[:, 1:8] = df_reduction.iloc[:, 1:8].apply(pd.to_numeric, errors='coerce')\n\n\n\n\n\n\nApply PCA.\nDetermine the optimal number of principal components.\nVisualize the PCA results.\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1.Applying PCA\npca = PCA()\npca_data = pca.fit_transform(scaled_data_df)\n\n# 2.Determining the optimal number of principal components\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\n# 3.Plotting the explained variance\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\nplt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.title('Explained Variance by Different Principal Components')\nplt.show()\nprint('explained_variance is', explained_variance)\nprint('cumulative_variance is', cumulative_variance)\n\n# Using this plot, we can select the number of components that capture a sufficient amount of variance (e.g., 95%)\noptimal_components = sum(cumulative_variance &lt;= 0.70) + 1\nprint('optimal_components is', optimal_components)\n\n\n\n\n\nexplained_variance is [0.56077972 0.14864573 0.09922325 0.07872168 0.04445305 0.03308806\n 0.0249608  0.01012771]\ncumulative_variance is [0.56077972 0.70942545 0.8086487  0.88737038 0.93182343 0.96491149\n 0.98987229 1.        ]\noptimal_components is 2\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Applying PCA\npca = PCA()\npca_data = pca.fit_transform(scaled_data_df)\n\n# Explained variance ratio\nexplained_variance = pca.explained_variance_ratio_\n\n# Plotting the cumulative explained variance\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(explained_variance))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance by Different Principal Components')\nplt.grid(True)\nplt.show()\n\nexplained_variance.cumsum()\n\n\n\n\narray([0.56077972, 0.70942545, 0.8086487 , 0.88737038, 0.93182343,\n       0.96491149, 0.98987229, 1.        ])\n\n\nThe 2 plots above shows both the individual and cumulative explained variance by each principal component.\nThe first principal component alone accounts for approximately 56.1% of the variance. About 70.9% of the variance is explained by the first two components. With three components, this increases to approximately 80.9%. To capture more than 88% of the variance, we need to consider the first four components.\nvisualize the data in the reduced dimension using PCA Reducing the data to the first two and three principal components\n\n# Reducing the data to the first three principal components\npca_3 = PCA(n_components=3)\npca_3_data = pca_3.fit_transform(scaled_data_df)\n\n# Creating a DataFrame for the PCA results\npca_df = pd.DataFrame(data=pca_3_data, columns=['PC1', 'PC2', 'PC3'])\n\n\n# Plotting the first two principal components\nplt.figure(figsize=(10, 8))\nplt.scatter(pca_df['PC1'], pca_df['PC2'])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA: First Two Principal Components')\nplt.grid(True)\nplt.show()\n    \n# Plotting the first three principal components\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'])\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')\nax.set_title('PCA: First Three Principal Components')\nplt.show()\n\nexplained_variance.cumsum()\n\n\n\n\n\n\n\narray([0.56077972, 0.70942545, 0.8086487 , 0.88737038, 0.93182343,\n       0.96491149, 0.98987229, 1.        ])\n\n\nFirst Two Principal Components: The scatter plot of the first two principal components shows how the data points are spread out. This plot can be useful to identify clusters or patterns, although it’s not immediately clear if distinct clusters are present in this case.\nFirst Three Principal Components: The 3D plot including the first three principal components gives a more detailed view. This additional dimension can sometimes reveal structures that are not visible in two dimensions."
  },
  {
    "objectID": "reduction.html#dimensionality-reduction",
    "href": "reduction.html#dimensionality-reduction",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "first,we after looking at the datasets and the features, we can see that there are some features that are highly correlated with each other. This is a problem because it can lead to overfitting. To solve this problem, we can use dimensionality reduction techniques to reduce the number of features in the dataset. This will help us to avoid overfitting and also reduce the computational cost of the model.\n\n\n\nload the dataset\nHandle missing values and non-numeric entries.\nConvert categorical data to numerical.\nNormalize the data. so the numerical data has a mean of 0 and standard deviation of 1.(This is crucial for PCA and t-SNE because they are sensitive to the scale of the data.)\n\n\n#1.load data\nimport pandas as pd\ndf_reduction=pd.read_csv('./Data/Reduction.csv')\ndf_reduction.head()\n# df_reduction = df_reduction.drop('Economy', axis=1)\n\n\n\n\n\n\n\n\nEconomy\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\nDepression (%)\naverage_learning_Adjusted_of_school\nIncome group\n\n\n\n\n0\nAfghanistan\n635.789\n0.166158\n0.708089\n0.107142\n4.882481\n4.136347\n4.957542\nLow income\n\n\n1\nAlbania\n4525.887\n0.201025\n0.704480\n0.174046\n3.385245\n2.208414\n8.526723\nUpper middle income\n\n\n2\nAlgeria\n4014.707\n0.197913\n0.818687\n0.213612\n5.065876\n3.661094\n7.045445\nLower middle income\n\n\n3\nAndorra\n40017.741\n0.263512\n0.963331\n0.644559\n5.305375\n3.729532\nNaN\nHigh income\n\n\n4\nAngola\n4039.3\n0.172794\n0.623904\n0.173643\n3.296906\n4.160484\n4.193318\nLower middle income\n\n\n\n\n\n\n\n\n\n# 2.Handling Non-Numeric Values\n# Non-numeric entries such as \"no data\" are replaced with NaN, so they can be imputed later.\n#  Replace non-numeric entries with NaN\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n#checking for missing values\ndf_reduction.isnull().sum()\n\n# Replace non-numeric entries with NaN\ndf_reduction.iloc[:, 1:8] = df_reduction.iloc[:, 1:8].apply(pd.to_numeric, errors='coerce')\n# [1:8] means from column 1 to column 7 because column 0 and 8 are not numeric\n# The apply(pd.to_numeric, errors='coerce') method attempts to convert all entries in the specified columns to numeric values.\n# If it encounters any entry that cannot be converted (like a string 'no data'), it replaces it with NaN (Not a Number), a standard missing dat\n\n\n# Impute missing values with the mean\nimputer = SimpleImputer(strategy='mean')\ndf_reduction.iloc[:, 1:8] = imputer.fit_transform(df_reduction.iloc[:, 1:8])\n\n# 3.Encoding Categorical Data, so Categorical data is encoded numerically.\n# Label encoding assigns each unique value to a different integer.\n# we only encode the Income group column because it is ordinal, we don't encode the the Economy because it is nominal and it's not relevant to the analysis.\nlabel_encoder = LabelEncoder()\ndf_reduction['Income group'] = label_encoder.fit_transform(df_reduction['Income group'])\ndf_reduction.isnull().sum()\n# if there is missing value in the Income group column, we can use the following code to fill the missing value with the most frequent value\n# df_reduction['Income group'] = df_reduction['Income group'].fillna(df_reduction['Income group'].mode()[0])\nscaled_data_df = pd.DataFrame(df_reduction, columns=df_reduction.columns[1:])\n\n# 4.Normalizing the Data (The data is normalized to ensure that all features contribute equally to the analysis.)\n# Normalization standardizes the range of features in your dataset, ensuring that each feature contributes equally to the analysis and is not dominated by those with larger magnitudes.\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_reduction.iloc[:, 1:])\n\n# Display the first few rows of the processed data\nscaled_data_df = pd.DataFrame(scaled_data, columns=df_reduction.columns[1:])\nprint(scaled_data_df.head())\n#save our data\nscaled_data_df.to_csv('./Data/Reduction_cleaned.csv', index=False)\n\n       2017  Schizophrenia (%)  Bipolar disorder (%)  Eating disorders (%)  \\\n0 -0.702995          -1.079808             -0.179703             -0.920185   \n1 -0.501116          -0.221633             -0.202660             -0.519143   \n2 -0.527644          -0.298235              0.523775             -0.281972   \n3  1.340754           1.316321              1.443806              2.301250   \n4 -0.526368          -0.916482             -0.715177             -0.521562   \n\n   Anxiety disorders (%)  Depression (%)  average_learning_Adjusted_of_school  \\\n0               0.731688        1.117681                        -1.204944e+00   \n1              -0.544877       -1.991509                         2.928827e-01   \n2               0.888053        0.351238                        -3.287439e-01   \n3               1.092253        0.461607                        -3.727291e-16   \n4              -0.620196        1.156607                        -1.525655e+00   \n\n   Income group  \n0     -0.439277  \n1      1.199375  \n2      0.380049  \n3     -1.258603  \n4      0.380049  \n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_67549/2609797013.py:14: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n  df_reduction.iloc[:, 1:8] = df_reduction.iloc[:, 1:8].apply(pd.to_numeric, errors='coerce')\n\n\n\n\n\n\nApply PCA.\nDetermine the optimal number of principal components.\nVisualize the PCA results.\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1.Applying PCA\npca = PCA()\npca_data = pca.fit_transform(scaled_data_df)\n\n# 2.Determining the optimal number of principal components\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\n# 3.Plotting the explained variance\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\nplt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.title('Explained Variance by Different Principal Components')\nplt.show()\nprint('explained_variance is', explained_variance)\nprint('cumulative_variance is', cumulative_variance)\n\n# Using this plot, we can select the number of components that capture a sufficient amount of variance (e.g., 95%)\noptimal_components = sum(cumulative_variance &lt;= 0.70) + 1\nprint('optimal_components is', optimal_components)\n\n\n\n\n\nexplained_variance is [0.56077972 0.14864573 0.09922325 0.07872168 0.04445305 0.03308806\n 0.0249608  0.01012771]\ncumulative_variance is [0.56077972 0.70942545 0.8086487  0.88737038 0.93182343 0.96491149\n 0.98987229 1.        ]\noptimal_components is 2\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Applying PCA\npca = PCA()\npca_data = pca.fit_transform(scaled_data_df)\n\n# Explained variance ratio\nexplained_variance = pca.explained_variance_ratio_\n\n# Plotting the cumulative explained variance\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(explained_variance))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance by Different Principal Components')\nplt.grid(True)\nplt.show()\n\nexplained_variance.cumsum()\n\n\n\n\narray([0.56077972, 0.70942545, 0.8086487 , 0.88737038, 0.93182343,\n       0.96491149, 0.98987229, 1.        ])\n\n\nThe 2 plots above shows both the individual and cumulative explained variance by each principal component.\nThe first principal component alone accounts for approximately 56.1% of the variance. About 70.9% of the variance is explained by the first two components. With three components, this increases to approximately 80.9%. To capture more than 88% of the variance, we need to consider the first four components.\nvisualize the data in the reduced dimension using PCA Reducing the data to the first two and three principal components\n\n# Reducing the data to the first three principal components\npca_3 = PCA(n_components=3)\npca_3_data = pca_3.fit_transform(scaled_data_df)\n\n# Creating a DataFrame for the PCA results\npca_df = pd.DataFrame(data=pca_3_data, columns=['PC1', 'PC2', 'PC3'])\n\n\n# Plotting the first two principal components\nplt.figure(figsize=(10, 8))\nplt.scatter(pca_df['PC1'], pca_df['PC2'])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA: First Two Principal Components')\nplt.grid(True)\nplt.show()\n    \n# Plotting the first three principal components\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'])\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')\nax.set_title('PCA: First Three Principal Components')\nplt.show()\n\nexplained_variance.cumsum()\n\n\n\n\n\n\n\narray([0.56077972, 0.70942545, 0.8086487 , 0.88737038, 0.93182343,\n       0.96491149, 0.98987229, 1.        ])\n\n\nFirst Two Principal Components: The scatter plot of the first two principal components shows how the data points are spread out. This plot can be useful to identify clusters or patterns, although it’s not immediately clear if distinct clusters are present in this case.\nFirst Three Principal Components: The 3D plot including the first three principal components gives a more detailed view. This additional dimension can sometimes reveal structures that are not visible in two dimensions."
  },
  {
    "objectID": "reduction.html#t-sne-implementation-to-be-done-after-pca",
    "href": "reduction.html#t-sne-implementation-to-be-done-after-pca",
    "title": "Dimensionality Reduction",
    "section": "3.t-SNE Implementation (to be done after PCA):",
    "text": "3.t-SNE Implementation (to be done after PCA):\nPerplexit is a critical parameter in the t-SNE algorithm that helps determine the scale at which the algorithm operates, influencing the balance between highlighting local versus global data structures. is about the expected density around a point rather than the number of clusters in the data. when Perplexity = 10, The algorithm will assume that each point has about 10 close neighbors, when Perplexity = 50, the algorithm will assume that each point has about 50 close neighbors.\n\nApply t-SNE.  Use scikit-learn’s TSNE class. Experiment with different perplexity values (common choices are between 5 and 50). Set the number of components to 2 for visualization purposes.\nExperiment with different perplexity values. Perplexity can be thought of as a measure of how to balance attention between local and global aspects of the data. Low perplexity values lead to more attention being paid to local aspects of the data while high perplexity values result in the global structure of the data being given more weight.\nVisualize the t-SNE results. plotting the two-dimensional t-SNE results.\nCompare with PCA results.\n\n\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n\n# scaled_data_df is preprocessed dataset\n# print(scaled_data_df.head())\n\n# Standardizing the features\nX_std = StandardScaler().fit_transform(scaled_data_df)\n\n# Applying t-SNE with different perplexity values\nperplexities = [5, 10, 20, 30, 40, 50]\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n    X_tsne = tsne.fit_transform(X_std)\n\n    # Visualization\n    plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n    plt.title(f't-SNE with Perplexity = {perplexity}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerplexity = 5: This plot shows several well-separated clusters. The low perplexity value is focusing on local structures, which is why we see distinct groupings. However, some clusters may be artificially split due to the perplexity being too low to capture broader relationships.\nPerplexity = 10 maintains clear separation while starting to show some cohesion within clusters. This suggests that a slight increase in perplexity helps in capturing more of the global structure without losing local details.\nPerplexity = 20 continues to maintain this balance but with a risk of some clusters starting to merge, which may or may not be desirable depending on the true data structure.\nPerplexity = 40: Here, the clusters are less distinct, and some groups start to merge. This is an indication that the perplexity might be too high, leading to an oversimplification of the local structures. It’s capturing more of the global data distribution but at the expense of local detail.\nso, in summary, the t-SNE with a perplexity of 10 or 20 seems to be a good middle ground for my dataset, balancing local and global structures; offering a reasonable compromise between showing local groupings and fitting into the broader context of the entire dataset."
  },
  {
    "objectID": "reduction.html#evaluation-and-comparison",
    "href": "reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison:",
    "text": "Evaluation and Comparison:\n\nEvaluate the effectiveness of both techniques.\nGlobal vs. Local Structures: PCA tends to show the overall structure, while t-SNE can uncover local groupings and complex structures. Clarity of Clusters: Determine which method shows clearer clustering. t-SNE might reveal clusters that PCA cannot, especially in complex datasets.\nPCA is effective for understanding the overall variance and structure of the data. It’s computationally efficient and provides a quick overview. t-SNE excels in revealing complex patterns and clusters, particularly useful for datasets where non-linear relationships are significant. However, it’s more computationally intensive and its results can vary with different perplexity values.\nCompare their visualization capabilities.\nPCA Global Structure Visualization: PCA is excellent at visualizing the global structure of the data. It reduces dimensions while preserving as much variance as possible, which is helpful for understanding the overall distribution and direction of the dataset. Linearity: The linear nature of PCA makes it suitable for datasets where relationships among variables are linear. The principal components are linear combinations of the original features, providing a straightforward interpretation. Limitation in Complexity: PCA might not be as effective in revealing complex, non-linear relationships. Clusters or patterns that are not linearly separable might be missed in PCA visualizations. Discuss trade-offs and scenarios for their use.\nt-SNE: Local Structure and Clusters: t-SNE excels in visualizing local structures and clusters, even when these clusters are formed through complex, non-linear relationships. It’s particularly good at separating distinct groups in the data. Non-Linearity: t-SNE can uncover patterns and relationships that are not apparent with linear methods like PCA. This makes t-SNE visualizations more intricate and revealing for certain types of datasets. Sensitivity to Parameters: The output of t-SNE can be highly sensitive to the choice of perplexity and other parameters. This can lead to varying results, making interpretation sometimes subjective.\nTrade-offs and Scenarios for Use \nPCA: Best Use Cases: When you need a quick overview of the data. In cases where linear relationships dominate. For preprocessing in machine learning, especially when model simplicity and computational efficiency are crucial. Trade-offs: PCA might oversimplify complex datasets by missing non-linear patterns. It’s not the best tool for datasets where clusters are formed through non-linear boundaries.\nt-SNE:  Best Use Cases: For detailed exploratory data analysis, especially when you suspect non-linear relationships in your data. In scenarios where distinguishing between different clusters or groups is more important than understanding the variance across the entire dataset. Trade-offs: t-SNE is computationally intensive and not suitable for very large datasets. The results can vary with different runs, requiring careful selection of parameters and interpretation of results. It’s not typically used as a preprocessing step for machine learning models due to its computational complexity and the difficulty in interpreting the transformed features.\n\n4.Conclusion The choice between PCA and t-SNE for visualization depends on the dataset characteristics and the specific goals of the analysis. PCA offers a fast, linear approach suitable for an overview and linearly separable data, while t-SNE provides a detailed, non-linear approach ideal for datasets with complex structures and relationships. The decision should align with the analysis objectives, considering the trade-offs in terms of computational efficiency, ease of interpretation, and the ability to uncover underlying patterns in the data."
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction Dimentional reduction happens before clusering My feature data includes 2017(which is the gdp-per-capita in the year of 2017 of all economies),Schizophrenia (%), Bipolar disorder (%),Eating disorders (%),Anxiety disorders (%),Depression (%), average_learning_Adjusted_of_school,and the goal of the project is to cluster the economies into different groups(like Income group) based on the feature data.\nTheory KMEANS\nIt is a popular unsupervised machine learning algorithm, which is used to divide a given unlabeled dataset into a certain number of clusters, denoted as k. One example of this is imagine you are at a farmer’s market and you have a variety of fruits and you want to organize these fruits. And k-means clustering is you sorting these fruits into different bags according to their charactersitics, like color, size, shape, etc. So the first step is you need to determine the number of bags you want to sort these fruits(the number means K in K-means,which stands for the number of clusters). Then you randomly place these bags among the fruits and you put every friut to its cloest bag. After all the fruits are put into bags, you find the average characteristics of each bag and move the bag to the average position of the fruits in the bag. Then you repeat the process until the bags stop moving(The fruits no longer switch bags) And the result is you have a certain number of bags and each bag contains a certain number of fruits with similar characteristics. The goal is to have bags(clusters) where the fruits inside are more like each other than they are like fruits in other bags. And we use Elbow method, Silhouette method, Gap statistic to determine the number of bags we want to sort the fruits into.\nElbow method we increase the number of bags and get relavent scores of how close fruits are within bags, and in the plot we will look for the point where adding another bags doesn’t significantly improve the closeness - this point is the ‘elbow’. 2. Silhouette method It is like each fruit can whisper tou you and tell you that how happy it is in its bag by give score. The Silhouette method is like listening to all those whispers to figure out if you’ve sorted the fruits into bags well by taking all these scores and averaging them to give an overall score. The score close to +1 means that the fruit is very happy because it’s surrounded by similar fruits, and the socre about 0 means the fruit is cool and don’t care about whcih bag it is in, and the score close to -1 means the fruit is very sad because it’s surrounded by very different fruits. if many fruits are sad or cool, the he overall score will be low, you may need to reorganize the fruits into different bags by adjusting the number of bags or considering different fruit characteristics.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nIt is like organizing a group of children playing in a park into clusters based on how close they are to each other. we don’t need to decide how many groups you want beforehand, you set rules for what makes a group. Like a group needs at least five children, and all members of the group need to be within arm’s reach of someone else in the group. Children who are close enough to a group join it, and groups might merge if they get close enough to each other. Some children who prefer playing alone and don’t reach out to others are not part of any group(It is like the noise in the DBSCAN). DBSCAN works similarly. It groups points that are closely packed together, and points in low-density regions are marked as ‘noise’. This method is gvery ood when you don’t know how many clusters to expect, or when your data might have outliers. The silhouette score helps to evaluate how well the points are clustered, with higher scores indicating better-defined clusters.\nHierarchical Clustering It is like building a family tree for a set of ancient artifacts found at an archaeological site. Instead of sorting them into separate bages, you try to understand how these artifacts are related to each other, from the oldest to the newest. Hierarchical clustering starts by assuming every artifact is its own family (cluster). Then, it gradually links artifacts into families by their similarities, like materials or designs, until all artifacts are united into one big family (cluster). This method creates a tree diagram called a dendrogram, which shows the ‘family’ relationships. It allows us to choose the level of similarity at which we want to stop combining artifacts into families, which in turn determines the number of clusters. We can use the dendrogram to visually identify where to ‘cut’ the tree to get a sensible number of families,with the elbow method or silhouette score serving as tools to help decide on the best ‘cut’, ensuring that artifacts in the same family are as similar as possible.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndf_reduction=pd.read_csv('./Data/Reduction.csv')\n# print(df_reduction.isnull().sum())\n\n# Replace non-numeric entries with NaN\ndf_reduction.iloc[:, 1:8] = df_reduction.iloc[:, 1:8].apply(pd.to_numeric, errors='coerce')\n# [1:8] means from column 1 to column 7 because column 0 and 8 are not numeric\n# The apply(pd.to_numeric, errors='coerce') method attempts to convert all entries in the specified columns to numeric values.\n# If it encounters any entry that cannot be converted (like a string 'no data'), it replaces it with NaN (Not a Number), a standard missing dat\n\n# Impute missing values with the mean of the column\nimputer = SimpleImputer(strategy='mean')\ndf_reduction.iloc[:, 1:8] = imputer.fit_transform(df_reduction.iloc[:, 1:8])\n\n#impute missing values with the mode\ndf_reduction['Income group'] = df_reduction['Income group'].fillna(df_reduction['Income group'].mode()[0])\ndf_reduction.isnull().sum()\n\n#lable encoding: encode the categorical data into numeric \nlabel_encoder = LabelEncoder()\ndf_reduction['Income group'] = label_encoder.fit_transform(df_reduction['Income group'])\n# df_reduction['Economy']=label_encoder.fit_transform(df_reduction['Economy'])\ndf_reduction.isnull().sum()\n# Feature Scaling\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_reduction.iloc[:, 1:])\n\n# Creating a new DataFrame for the scaled features\nscaled_df = pd.DataFrame(scaled_features, columns=df_reduction.columns[1:])\nscaled_df.drop(['Income group'], axis=1, inplace=True)\n#after dimentional reduction, we drop the columns that are not important 'average_learning_Adjusted_of_school'and 'scaled_df.drop(['Depression (%)'], axis=1, inplace=True)'\nscaled_df.drop(['average_learning_Adjusted_of_school'], axis=1, inplace=True)\nscaled_df.drop(['Depression (%)'], axis=1, inplace=True)\nscaled_df.head()  \n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_96756/428490376.py:11: DeprecationWarning:\n\nIn a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n\n\n\n\n\n\n\n\n\n\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\n\n\n\n\n0\n-0.702995\n-1.079808\n-0.179703\n-0.920185\n0.731688\n\n\n1\n-0.501116\n-0.221633\n-0.202660\n-0.519143\n-0.544877\n\n\n2\n-0.527644\n-0.298235\n0.523775\n-0.281972\n0.888053\n\n\n3\n1.340754\n1.316321\n1.443806\n2.301250\n1.092253\n\n\n4\n-0.526368\n-0.916482\n-0.715177\n-0.521562\n-0.620196"
  },
  {
    "objectID": "Clustering.html#results",
    "href": "Clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\nALl clustering analysis methods. They all sugested that the data can be clustered into 2 groups is the best after the parameter tuning,except for the elbow method in K-means clustering, which suggested that the data can be clustered into 5 groups is the best.\nUsing unsupervised learning methods(clustering), we can see the underlying information is discovered from the mental health data, whch is the unexpected groupings (cluster is 2)\nWhen clustering algorithms with K-Means, DBSCAN, or Hierarchical Clustering consistently yield a certain number of clusters that doesn’t match the expected number based on known labels (like my four income groups), it suggests a few possibilities: my data naturally falls into two distinct group, maybe there is a inherent data distribution, These groups might not align with predefined income categories, suggesting that the selected features do not differentiate the income groups as clearly as expected."
  },
  {
    "objectID": "Clustering.html#conclusions",
    "href": "Clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions:",
    "text": "Conclusions:\nIn my investigation into the patterns and structures within our dataset, we applied various clustering techniques to uncover hidden groupings that may reflect real-world distinctions among the subjects. The essence of our findings reveals that the complexities of mental health indicators and educational outcomes do not necessarily align neatly with economic categories such as income levels.\nUnexpectedly, rather than observing four distinct groupings corresponding to high, upper-middle, lower-middle, and low income levels, our analysis consistently highlighted two predominant clusters. This suggests that the features we examined—ranging from schizophrenia to depression rates, and the average learning adjustments in schools—might interplay in ways that transcend traditional economic divisions.\nThis has profound implications for policymakers, educators, and healthcare providers. It suggests that strategies aimed at addressing mental health and education should not be overly tailored to economic status alone, as these issues are not confined to any one financial demographic. Instead, more universal approaches that cross these boundaries may be necessary to effectively address the needs highlighted by our findings.\nMoreover, my analysis serves as a reminder of the complex human stories behind the data. Numbers and categories can only tell us so much; the lived experiences of individuals often defy simple categorization. This study underlines the importance of considering the full tapestry of human experience when crafting policies and interventions aimed at improving mental health and educational outcomes.\nIn conclusion, my exploration into the dataset has illuminated the intricate relationship between mental health, education, and income, underscoring the need for broad-spectrum solutions that are sensitive to the nuanced ways these domains interact. These insights can guide us toward more inclusive and effective approaches to supporting well-being and educational achievement across all economic strata."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction Dimentional reduction happens before clusering My feature data includes 2017(which is the gdp-per-capita in the year of 2017 of all economies),Schizophrenia (%), Bipolar disorder (%),Eating disorders (%),Anxiety disorders (%),Depression (%), average_learning_Adjusted_of_school,and the goal of the project is to cluster the economies into different groups(like Income group) based on the feature data.\nTheory KMEANS\nIt is a popular unsupervised machine learning algorithm, which is used to divide a given unlabeled dataset into a certain number of clusters, denoted as k. One example of this is imagine you are at a farmer’s market and you have a variety of fruits and you want to organize these fruits. And k-means clustering is you sorting these fruits into different bags according to their charactersitics, like color, size, shape, etc. So the first step is you need to determine the number of bags you want to sort these fruits(the number means K in K-means,which stands for the number of clusters). Then you randomly place these bags among the fruits and you put every friut to its cloest bag. After all the fruits are put into bags, you find the average characteristics of each bag and move the bag to the average position of the fruits in the bag. Then you repeat the process until the bags stop moving(The fruits no longer switch bags) And the result is you have a certain number of bags and each bag contains a certain number of fruits with similar characteristics. The goal is to have bags(clusters) where the fruits inside are more like each other than they are like fruits in other bags. And we use Elbow method, Silhouette method, Gap statistic to determine the number of bags we want to sort the fruits into.\nElbow method we increase the number of bags and get relavent scores of how close fruits are within bags, and in the plot we will look for the point where adding another bags doesn’t significantly improve the closeness - this point is the ‘elbow’. 2. Silhouette method It is like each fruit can whisper tou you and tell you that how happy it is in its bag by give score. The Silhouette method is like listening to all those whispers to figure out if you’ve sorted the fruits into bags well by taking all these scores and averaging them to give an overall score. The score close to +1 means that the fruit is very happy because it’s surrounded by similar fruits, and the socre about 0 means the fruit is cool and don’t care about whcih bag it is in, and the score close to -1 means the fruit is very sad because it’s surrounded by very different fruits. if many fruits are sad or cool, the he overall score will be low, you may need to reorganize the fruits into different bags by adjusting the number of bags or considering different fruit characteristics.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nIt is like organizing a group of children playing in a park into clusters based on how close they are to each other. we don’t need to decide how many groups you want beforehand, you set rules for what makes a group. Like a group needs at least five children, and all members of the group need to be within arm’s reach of someone else in the group. Children who are close enough to a group join it, and groups might merge if they get close enough to each other. Some children who prefer playing alone and don’t reach out to others are not part of any group(It is like the noise in the DBSCAN). DBSCAN works similarly. It groups points that are closely packed together, and points in low-density regions are marked as ‘noise’. This method is gvery ood when you don’t know how many clusters to expect, or when your data might have outliers. The silhouette score helps to evaluate how well the points are clustered, with higher scores indicating better-defined clusters.\nHierarchical Clustering It is like building a family tree for a set of ancient artifacts found at an archaeological site. Instead of sorting them into separate bages, you try to understand how these artifacts are related to each other, from the oldest to the newest. Hierarchical clustering starts by assuming every artifact is its own family (cluster). Then, it gradually links artifacts into families by their similarities, like materials or designs, until all artifacts are united into one big family (cluster). This method creates a tree diagram called a dendrogram, which shows the ‘family’ relationships. It allows us to choose the level of similarity at which we want to stop combining artifacts into families, which in turn determines the number of clusters. We can use the dendrogram to visually identify where to ‘cut’ the tree to get a sensible number of families,with the elbow method or silhouette score serving as tools to help decide on the best ‘cut’, ensuring that artifacts in the same family are as similar as possible.\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndf_reduction=pd.read_csv('./Data/Reduction.csv')\n# print(df_reduction.isnull().sum())\n\n# Replace non-numeric entries with NaN\ndf_reduction.iloc[:, 1:8] = df_reduction.iloc[:, 1:8].apply(pd.to_numeric, errors='coerce')\n# [1:8] means from column 1 to column 7 because column 0 and 8 are not numeric\n# The apply(pd.to_numeric, errors='coerce') method attempts to convert all entries in the specified columns to numeric values.\n# If it encounters any entry that cannot be converted (like a string 'no data'), it replaces it with NaN (Not a Number), a standard missing dat\n\n# Impute missing values with the mean of the column\nimputer = SimpleImputer(strategy='mean')\ndf_reduction.iloc[:, 1:8] = imputer.fit_transform(df_reduction.iloc[:, 1:8])\n\n#impute missing values with the mode\ndf_reduction['Income group'] = df_reduction['Income group'].fillna(df_reduction['Income group'].mode()[0])\ndf_reduction.isnull().sum()\n\n#lable encoding: encode the categorical data into numeric \nlabel_encoder = LabelEncoder()\ndf_reduction['Income group'] = label_encoder.fit_transform(df_reduction['Income group'])\n# df_reduction['Economy']=label_encoder.fit_transform(df_reduction['Economy'])\ndf_reduction.isnull().sum()\n# Feature Scaling\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df_reduction.iloc[:, 1:])\n\n# Creating a new DataFrame for the scaled features\nscaled_df = pd.DataFrame(scaled_features, columns=df_reduction.columns[1:])\nscaled_df.drop(['Income group'], axis=1, inplace=True)\n#after dimentional reduction, we drop the columns that are not important 'average_learning_Adjusted_of_school'and 'scaled_df.drop(['Depression (%)'], axis=1, inplace=True)'\nscaled_df.drop(['average_learning_Adjusted_of_school'], axis=1, inplace=True)\nscaled_df.drop(['Depression (%)'], axis=1, inplace=True)\nscaled_df.head()  \n\n\n/var/folders/px/bhxss9d10zs_wzsv0ck6sb200000gn/T/ipykernel_32044/8429475.py:11: DeprecationWarning:\n\nIn a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n\n\n\n\n\n\n\n\n\n\n2017\nSchizophrenia (%)\nBipolar disorder (%)\nEating disorders (%)\nAnxiety disorders (%)\n\n\n\n\n0\n-0.702995\n-1.079808\n-0.179703\n-0.920185\n0.731688\n\n\n1\n-0.501116\n-0.221633\n-0.202660\n-0.519143\n-0.544877\n\n\n2\n-0.527644\n-0.298235\n0.523775\n-0.281972\n0.888053\n\n\n3\n1.340754\n1.316321\n1.443806\n2.301250\n1.092253\n\n\n4\n-0.526368\n-0.916482\n-0.715177\n-0.521562\n-0.620196"
  },
  {
    "objectID": "clustering.html#results",
    "href": "clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\nALl clustering analysis methods. They all sugested that the data can be clustered into 2 groups is the best after the parameter tuning,except for the elbow method in K-means clustering, which suggested that the data can be clustered into 5 groups is the best.\nUsing unsupervised learning methods(clustering), we can see the underlying information is discovered from the mental health data, whch is the unexpected groupings (cluster is 2)\nWhen clustering algorithms with K-Means, DBSCAN, or Hierarchical Clustering consistently yield a certain number of clusters that doesn’t match the expected number based on known labels (like my four income groups), it suggests a few possibilities: my data naturally falls into two distinct group, maybe there is a inherent data distribution, These groups might not align with predefined income categories, suggesting that the selected features do not differentiate the income groups as clearly as expected."
  },
  {
    "objectID": "clustering.html#conclusions",
    "href": "clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions:",
    "text": "Conclusions:\nIn my investigation into the patterns and structures within our dataset, we applied various clustering techniques to uncover hidden groupings that may reflect real-world distinctions among the subjects. The essence of our findings reveals that the complexities of mental health indicators and educational outcomes do not necessarily align neatly with economic categories such as income levels.\nUnexpectedly, rather than observing four distinct groupings corresponding to high, upper-middle, lower-middle, and low income levels, our analysis consistently highlighted two predominant clusters. This suggests that the features we examined—ranging from schizophrenia to depression rates, and the average learning adjustments in schools—might interplay in ways that transcend traditional economic divisions.\nThis has profound implications for policymakers, educators, and healthcare providers. It suggests that strategies aimed at addressing mental health and education should not be overly tailored to economic status alone, as these issues are not confined to any one financial demographic. Instead, more universal approaches that cross these boundaries may be necessary to effectively address the needs highlighted by our findings.\nMoreover, my analysis serves as a reminder of the complex human stories behind the data. Numbers and categories can only tell us so much; the lived experiences of individuals often defy simple categorization. This study underlines the importance of considering the full tapestry of human experience when crafting policies and interventions aimed at improving mental health and educational outcomes.\nIn conclusion, my exploration into the dataset has illuminated the intricate relationship between mental health, education, and income, underscoring the need for broad-spectrum solutions that are sensitive to the nuanced ways these domains interact. These insights can guide us toward more inclusive and effective approaches to supporting well-being and educational achievement across all economic strata."
  },
  {
    "objectID": "DT.html",
    "href": "DT.html",
    "title": "Decision Trees",
    "section": "",
    "text": "I used a decision tree algorithm to predict what income group a given economy is from based on record mental health data.\nIn general, a a decision Trees (DT) are a type of model used in machine learning that mimic human decision-making processes. Imagine you’re trying to decide what to wear. You might look at the weather (a feature) and choose accordingly. A decision tree does something similar with data: it makes a series of choices based on the features to reach a conclusion, like deciding if an email is spam. It’s like a flowchart – starting with a question about the data, it branches out until it reaches an answer.\nRandom Forests (RF) build on decision trees. If a decision tree is like a single advisor making a decision, a random forest is like a committee. It creates multiple decision trees, each with slightly different perspectives, and then combines their votes to make a final decision. This helps in getting a more balanced and less biased result, as it avoids over-relying on a single perspective.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Load the dataset\ndf = pd.read_csv('./Data/symptoms.csv')\ndf.columns\n\n# print(df.columns)\n# Compute the distribution of class labels\nclass_distribution = df['Label'].value_counts(normalize=True)\n\n# Output the distribution\nprint(class_distribution)\n\n# Visualize the distribution\nclass_distribution.plot(kind='bar')\nplt.title('Class Distribution')\nplt.show()\n\nPhysical health    0.537634\nMental health      0.462366\nName: Label, dtype: float64\n\n\n\n\n\n\n\n\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Prepare the data\nX = df.drop('Label', axis=1)\ny = df['Label']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit a random classifier\ndummy = DummyClassifier(strategy='stratified')\ndummy.fit(X_train, y_train)\n\n# Evaluate the classifier\ny_pred = dummy.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n                 precision    recall  f1-score   support\n\n  Mental health       0.65      0.50      0.56        22\nPhysical health       0.48      0.62      0.54        16\n\n       accuracy                           0.55        38\n      macro avg       0.56      0.56      0.55        38\n   weighted avg       0.58      0.55      0.55        38\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# Load and preprocess your data\nX = df['Symptoms']  # Replace with your text column name\ny = df['Label']  # Replace with your label column name\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest_results = []\ntrain_results = []\n#for num_layer in range(1, 30):\n#for num_layer in range(1, 7):\nfor num_layer in range(1, 4):\n    # Include TfidfVectorizer in the pipeline\n    model = Pipeline([\n        ('tfidf', TfidfVectorizer()),\n        ('dt', DecisionTreeClassifier(max_depth=num_layer))\n    ])\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    test_results.append([num_layer, accuracy_score(y_test, y_test_pred)])\n    train_results.append([num_layer, accuracy_score(y_train, y_train_pred)])\n\n# Create DataFrames for visualization\ntuning_train_df = pd.DataFrame(train_results, columns=[\"Tree_depth\", \"train_accuracy_score\"])\ntuning_test_df = pd.DataFrame(test_results, columns=[\"Tree_depth\", \"test_accuracy_score\"])\n# print(tuning_train_df)\n# print(tuning_test_df)\n#merge two dataframes\ntuning_df = pd.merge(tuning_train_df, tuning_test_df, on='Tree_depth')\nprint(tuning_df)\n# Plotting the results\nplt.figure(figsize=(12, 6))\nplt.plot(tuning_train_df['Tree_depth'], tuning_train_df['train_accuracy_score'], label='Train Accuracy')\nplt.plot(tuning_test_df['Tree_depth'], tuning_test_df['test_accuracy_score'], label='Test Accuracy')\nplt.xlabel('Tree Depth')\nplt.ylabel('Accuracy Score')\nplt.title('Model Tuning: Decision Tree Depth vs Accuracy')\nplt.legend()\nplt.show()\n\n   Tree_depth  train_accuracy_score  test_accuracy_score\n0           1              0.763514             0.763158\n1           2              0.844595             0.789474\n2           3              0.912162             0.736842\n\n\n\n\n\n\nprint(model.named_steps['dt'].classes_)\n# The first element is what '0' represents, and the second element is what '1' represents.\n\n['Mental health' 'Physical health']\n\n\n\n\n\nThe decesion tree has an accurcay of 0.91 on the traning dataset and an accurcy of 0.76 on the test dataset when the num_layer is set to 3.\nIn fact, when I set my num_layer to 35 at first, but I found that the training accuracy has reached 100% for tree depths greater than or equal to 7, while the test accuracy remains significantly lower and does not improve. This is a sign of overfitting, the model has perfectly learned the training dataset, including its noise and outliers. The model becomes increasingly tailored to the training data, losing its ability to generalize to unseen data, therefore, I decided to set the num_layer to 6.\nInitially, as the tree depth increases from 1 to 2, there’s an improvement in both training and testing accuracy. However, beyond a certain point (around a depth of 2 or 3), the increase in training accuracy does not translate to an improvement in testing accuracy. This suggests that the optimal tree depth for this model, where it balances learning from the training data without losing generalizability, might be around 2 or 3.\nIn summary, while a high training accuracy score may seem positive at first glance, it can be indicative of overfitting when it is not accompanied by a corresponding high test accuracy.\nBelow is the barplot and confusion matrix for the test dataset. For the Confusion matrix, Although the number of the testing set is limited, but we can still see the diagonal of the confusion matrix is dark blue whereas the other boxes are faintly colored, indicating that there is a high number of correct predictions.\n\n# TEST ACCURACY\n\n# Training dataset\nprint(\"Training dataset\")\nprint(\"Accuracy: \", accuracy_score(y_train, y_train_pred) * 100) \nprint(\"Number of mislabeled points: \", (y_train != y_train_pred).sum()) \n\n# Testing dataset\ny_test_pred = model.predict(X_test)\nprint(\"Testing dataset\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_test_pred)*100) \nprint(\"Number of mislabeled points: \", (y_test != y_test_pred).sum()) \n\n# Plot\nmodel_accuracies = pd.DataFrame({'Set':['Training set','Test set'], 'Accuracy (%)': [accuracy_score(y_train, y_train_pred) * 100, accuracy_score(y_test, y_test_pred)*100]})\nsns.barplot(data=model_accuracies, x=\"Set\", y=\"Accuracy (%)\").set(title = 'Accuracy of model for training vs test sets' )\n\nTraining dataset\nAccuracy:  91.21621621621621\nNumber of mislabeled points:  13\nTesting dataset\nAccuracy:  73.68421052631578\nNumber of mislabeled points:  10\n\n\n[Text(0.5, 1.0, 'Accuracy of model for training vs test sets')]\n\n\n\n\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, model.predict(X_test))\n\n# Plot using ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x28cba3d50&gt;\n\n\n\n\n\n\n#decesion tree visualization\nfrom sklearn.tree import plot_tree\n\n# Fit the model with desired tree depth(3)\ndesired_depth = 3\nmodel = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('dt', DecisionTreeClassifier(max_depth=desired_depth))\n])\nmodel.fit(X_train, y_train)\n\n# Visualize the tree\nplt.figure(figsize=(20,10))\nplot_tree(model.named_steps['dt'], filled=True, feature_names=model.named_steps['tfidf'].get_feature_names_out(), class_names=True)\nplt.show()"
  },
  {
    "objectID": "DT.html#methods",
    "href": "DT.html#methods",
    "title": "Decision Trees",
    "section": "",
    "text": "I used a decision tree algorithm to predict what income group a given economy is from based on record mental health data.\nIn general, a a decision Trees (DT) are a type of model used in machine learning that mimic human decision-making processes. Imagine you’re trying to decide what to wear. You might look at the weather (a feature) and choose accordingly. A decision tree does something similar with data: it makes a series of choices based on the features to reach a conclusion, like deciding if an email is spam. It’s like a flowchart – starting with a question about the data, it branches out until it reaches an answer.\nRandom Forests (RF) build on decision trees. If a decision tree is like a single advisor making a decision, a random forest is like a committee. It creates multiple decision trees, each with slightly different perspectives, and then combines their votes to make a final decision. This helps in getting a more balanced and less biased result, as it avoids over-relying on a single perspective."
  },
  {
    "objectID": "DT.html#class-distribution",
    "href": "DT.html#class-distribution",
    "title": "Decision Trees",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Load the dataset\ndf = pd.read_csv('./Data/symptoms.csv')\ndf.columns\n\n# print(df.columns)\n# Compute the distribution of class labels\nclass_distribution = df['Label'].value_counts(normalize=True)\n\n# Output the distribution\nprint(class_distribution)\n\n# Visualize the distribution\nclass_distribution.plot(kind='bar')\nplt.title('Class Distribution')\nplt.show()\n\nPhysical health    0.537634\nMental health      0.462366\nName: Label, dtype: float64"
  },
  {
    "objectID": "DT.html#baseline-model-for-comparison",
    "href": "DT.html#baseline-model-for-comparison",
    "title": "Decision Trees",
    "section": "",
    "text": "from sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Prepare the data\nX = df.drop('Label', axis=1)\ny = df['Label']\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit a random classifier\ndummy = DummyClassifier(strategy='stratified')\ndummy.fit(X_train, y_train)\n\n# Evaluate the classifier\ny_pred = dummy.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n                 precision    recall  f1-score   support\n\n  Mental health       0.65      0.50      0.56        22\nPhysical health       0.48      0.62      0.54        16\n\n       accuracy                           0.55        38\n      macro avg       0.56      0.56      0.55        38\n   weighted avg       0.58      0.55      0.55        38"
  },
  {
    "objectID": "DT.html#model-tuning",
    "href": "DT.html#model-tuning",
    "title": "Decision Trees",
    "section": "",
    "text": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# Load and preprocess your data\nX = df['Symptoms']  # Replace with your text column name\ny = df['Label']  # Replace with your label column name\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest_results = []\ntrain_results = []\n#for num_layer in range(1, 30):\n#for num_layer in range(1, 7):\nfor num_layer in range(1, 4):\n    # Include TfidfVectorizer in the pipeline\n    model = Pipeline([\n        ('tfidf', TfidfVectorizer()),\n        ('dt', DecisionTreeClassifier(max_depth=num_layer))\n    ])\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    test_results.append([num_layer, accuracy_score(y_test, y_test_pred)])\n    train_results.append([num_layer, accuracy_score(y_train, y_train_pred)])\n\n# Create DataFrames for visualization\ntuning_train_df = pd.DataFrame(train_results, columns=[\"Tree_depth\", \"train_accuracy_score\"])\ntuning_test_df = pd.DataFrame(test_results, columns=[\"Tree_depth\", \"test_accuracy_score\"])\n# print(tuning_train_df)\n# print(tuning_test_df)\n#merge two dataframes\ntuning_df = pd.merge(tuning_train_df, tuning_test_df, on='Tree_depth')\nprint(tuning_df)\n# Plotting the results\nplt.figure(figsize=(12, 6))\nplt.plot(tuning_train_df['Tree_depth'], tuning_train_df['train_accuracy_score'], label='Train Accuracy')\nplt.plot(tuning_test_df['Tree_depth'], tuning_test_df['test_accuracy_score'], label='Test Accuracy')\nplt.xlabel('Tree Depth')\nplt.ylabel('Accuracy Score')\nplt.title('Model Tuning: Decision Tree Depth vs Accuracy')\nplt.legend()\nplt.show()\n\n   Tree_depth  train_accuracy_score  test_accuracy_score\n0           1              0.763514             0.763158\n1           2              0.844595             0.789474\n2           3              0.912162             0.736842\n\n\n\n\n\n\nprint(model.named_steps['dt'].classes_)\n# The first element is what '0' represents, and the second element is what '1' represents.\n\n['Mental health' 'Physical health']"
  },
  {
    "objectID": "DT.html#final-results",
    "href": "DT.html#final-results",
    "title": "Decision Trees",
    "section": "",
    "text": "The decesion tree has an accurcay of 0.91 on the traning dataset and an accurcy of 0.76 on the test dataset when the num_layer is set to 3.\nIn fact, when I set my num_layer to 35 at first, but I found that the training accuracy has reached 100% for tree depths greater than or equal to 7, while the test accuracy remains significantly lower and does not improve. This is a sign of overfitting, the model has perfectly learned the training dataset, including its noise and outliers. The model becomes increasingly tailored to the training data, losing its ability to generalize to unseen data, therefore, I decided to set the num_layer to 6.\nInitially, as the tree depth increases from 1 to 2, there’s an improvement in both training and testing accuracy. However, beyond a certain point (around a depth of 2 or 3), the increase in training accuracy does not translate to an improvement in testing accuracy. This suggests that the optimal tree depth for this model, where it balances learning from the training data without losing generalizability, might be around 2 or 3.\nIn summary, while a high training accuracy score may seem positive at first glance, it can be indicative of overfitting when it is not accompanied by a corresponding high test accuracy.\nBelow is the barplot and confusion matrix for the test dataset. For the Confusion matrix, Although the number of the testing set is limited, but we can still see the diagonal of the confusion matrix is dark blue whereas the other boxes are faintly colored, indicating that there is a high number of correct predictions.\n\n# TEST ACCURACY\n\n# Training dataset\nprint(\"Training dataset\")\nprint(\"Accuracy: \", accuracy_score(y_train, y_train_pred) * 100) \nprint(\"Number of mislabeled points: \", (y_train != y_train_pred).sum()) \n\n# Testing dataset\ny_test_pred = model.predict(X_test)\nprint(\"Testing dataset\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_test_pred)*100) \nprint(\"Number of mislabeled points: \", (y_test != y_test_pred).sum()) \n\n# Plot\nmodel_accuracies = pd.DataFrame({'Set':['Training set','Test set'], 'Accuracy (%)': [accuracy_score(y_train, y_train_pred) * 100, accuracy_score(y_test, y_test_pred)*100]})\nsns.barplot(data=model_accuracies, x=\"Set\", y=\"Accuracy (%)\").set(title = 'Accuracy of model for training vs test sets' )\n\nTraining dataset\nAccuracy:  91.21621621621621\nNumber of mislabeled points:  13\nTesting dataset\nAccuracy:  73.68421052631578\nNumber of mislabeled points:  10\n\n\n[Text(0.5, 1.0, 'Accuracy of model for training vs test sets')]\n\n\n\n\n\n\n# confusion matrix(test set)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, model.predict(X_test))\n\n# Plot using ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=\"Blues\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x28cba3d50&gt;\n\n\n\n\n\n\n#decesion tree visualization\nfrom sklearn.tree import plot_tree\n\n# Fit the model with desired tree depth(3)\ndesired_depth = 3\nmodel = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('dt', DecisionTreeClassifier(max_depth=desired_depth))\n])\nmodel.fit(X_train, y_train)\n\n# Visualize the tree\nplt.figure(figsize=(20,10))\nplot_tree(model.named_steps['dt'], filled=True, feature_names=model.named_steps['tfidf'].get_feature_names_out(), class_names=True)\nplt.show()"
  },
  {
    "objectID": "HW-02/Code/test.html",
    "href": "HW-02/Code/test.html",
    "title": "Bella Shi's Website",
    "section": "",
    "text": "import pandas as pd\nphysical_health_symptoms = [\n    \"I often experience shortness of breath.\",\n    \"I have been feeling persistent fatigue lately.\",\n    \"I am experiencing unexplained weight loss.\",\n    \"I have a consistent cough that won't go away.\",\n    \"I am dealing with frequent headaches.\",\n    \"I have noticed a change in my appetite.\",\n    \"I've been having trouble sleeping recently.\",\n    \"I am experiencing muscle weakness.\",\n    \"I have chronic back pain.\",\n    \"I feel sharp chest pains occasionally.\",\n    \"I have been suffering from prolonged dizziness.\",\n    \"I get unusually tired after physical activity.\",\n    \"I've noticed swelling in my ankles and feet.\",\n    \"I have episodes of rapid heartbeat.\",\n    \"I am experiencing joint pain and stiffness.\",\n    \"I have persistent abdominal pain.\",\n    \"I suffer from severe menstrual cramps.\",\n    \"I have been having digestive issues frequently.\",\n    \"I feel numbness in my hands and feet.\",\n    \"I am dealing with constant throat irritation.\",\n    \"I have unusually dry skin.\",\n    \"I've noticed some vision changes.\",\n    \"I am experiencing frequent urination.\",\n    \"I have a high fever that won't subside.\",\n    \"I am dealing with uncontrolled bleeding.\",\n    \"I have sudden severe headaches.\",\n    \"I get easily bruised without any apparent reason.\",\n    \"I have been having chest congestion.\",\n    \"I am experiencing a loss of balance.\",\n    \"I have difficulty swallowing.\",\n    \"I've been suffering from constant nausea.\",\n    \"I have a persistent rash.\",\n    \"I experience severe allergic reactions.\",\n    \"I have irregular bowel movements.\",\n    \"I am dealing with excessive sweating.\",\n    \"I have a chronic sore throat.\",\n    \"I get severe cramps in my legs.\",\n    \"I have been experiencing unusual hair loss.\",\n    \"I get muscle spasms frequently.\",\n    \"I suffer from chronic dehydration.\",\n    \"I experience tingling sensations in my limbs.\",\n    \"I have unexplained bruises appearing.\",\n    \"I suffer from frequent earaches.\",\n    \"I have difficulty concentrating due to physical discomfort.\",\n    \"I experience unexplained changes in my weight.\",\n    \"I have a persistent feeling of being unwell.\",\n    \"I suffer from extreme fatigue after eating.\",\n    \"I have trouble breathing when lying down.\",\n    \"I get sharp pains in my abdomen.\",\n    \"I have been experiencing increased thirst.\",\n    \"I suffer from regular night sweats.\",\n    \"I have bouts of rapid and unexplained mood swings.\",\n    \"I get sharp pains in my joints.\",\n    \"I have trouble maintaining my balance.\",\n    \"I experience blurred vision.\",\n    \"I have persistent itching without a rash.\",\n    \"I suffer from swelling in my lymph nodes.\",\n    \"I experience a loss of muscle coordination.\",\n    \"I have severe pain in my lower back.\",\n    \"I suffer from extreme cold sensitivity.\",\n    \"I have unexplained gastrointestinal discomfort.\",\n    \"I experience frequent heartburn.\",\n    \"I have a constant salty taste in my mouth.\",\n    \"I suffer from severe migraines.\",\n    \"I experience chest tightness.\",\n    \"I have difficulty breathing during exercise.\",\n    \"I get numbness in my face.\",\n    \"I have been experiencing a persistent cough.\",\n    \"I suffer from sudden vision loss in one eye.\",\n    \"I have a loss of smell and taste.\",\n    \"I experience ringing in my ears.\",\n    \"I have swollen, red, or painful joints.\",\n    \"I get frequent infections.\",\n    \"I experience severe abdominal cramping.\",\n    \"I have trouble with frequent urination at night.\",\n    \"I suffer from excessive thirst.\",\n    \"I get dizzy spells when I stand up.\",\n    \"I have chronic indigestion.\",\n    \"I experience heat intolerance.\",\n    \"I have a prolonged sore that doesn't heal.\",\n    \"I suffer from unexplained muscle aches.\",\n    \"I experience chronic constipation.\",\n    \"I have frequent bouts of diarrhea.\",\n    \"I get unexplained nosebleeds.\",\n    \"I suffer from palpitations.\",\n    \"I have bouts of severe sweating at night.\",\n    \"I experience sudden swelling of hands and feet.\",\n    \"I have persistent acne that doesn’t respond to treatment.\",\n    \"I suffer from extreme light sensitivity.\",\n    \"I have recurring urinary tract infections.\",\n    \"I experience short, sharp pains in my chest.\",\n    \"I have a tingling sensation in my fingertips.\",\n    \"I suffer from excessive gas and bloating.\",\n    \"I experience frequent flare-ups of eczema.\",\n    \"I have chronic sinus pressure.\",\n    \"I suffer from recurrent bronchitis.\",\n    \"I have severe pains in my neck.\",\n    \"I experience swelling and pain in my knee.\",\n    \"I have a chronic metallic taste in my mouth.\",\n    \"I suffer from restless leg syndrome at night.\"\n]\nmental_health_symptoms = [\n    \"I frequently feel anxious without any specific reason.\",\n    \"I have been experiencing overwhelming sadness.\",\n    \"I feel a persistent sense of hopelessness.\",\n    \"I have lost interest in activities I used to enjoy.\",\n    \"I am dealing with significant mood swings.\",\n    \"I find it hard to concentrate on everyday tasks.\",\n    \"I am experiencing severe emotional highs and lows.\",\n    \"I feel a constant sense of worthlessness.\",\n    \"I have been having trouble sleeping or sleeping too much.\",\n    \"I feel persistently tired and have low energy.\",\n    \"I am experiencing unexplained irritability.\",\n    \"I am withdrawing from social interactions.\",\n    \"I have a significant change in my eating habits.\",\n    \"I am dealing with constant restlessness.\",\n    \"I feel intense and uncontrolled anger.\",\n    \"I have thoughts of self-harm or suicide.\",\n    \"I am experiencing panic attacks.\",\n    \"I have obsessive thoughts or behaviors.\",\n    \"I am struggling with compulsive behaviors.\",\n    \"I have been feeling detached from reality.\",\n    \"I am experiencing hallucinations.\",\n    \"I have a persistent sense of guilt.\",\n    \"I am having difficulty dealing with stress.\",\n    \"I feel disconnected from my own emotions.\",\n    \"I am experiencing paranoia or distrust of others.\",\n    \"I have a loss of motivation for most activities.\",\n    \"I am feeling unusually confused.\",\n    \"I am dealing with severe anxiety in social situations.\",\n    \"I am struggling with feelings of overwhelm.\",\n    \"I am experiencing persistent nightmares.\",\n    \"I feel an excessive fear or worry about specific things.\",\n    \"I am struggling with flashbacks of traumatic events.\",\n    \"I am having trouble forming or maintaining relationships.\",\n    \"I am experiencing severe physical reactions to stress.\",\n    \"I am having difficulty controlling my thoughts.\",\n    \"I have a strong feeling of dread about the future.\",\n    \"I am experiencing prolonged periods of sadness.\",\n    \"I am dealing with unexplained physical ailments.\",\n    \"I feel a lack of interest in personal hygiene.\",\n    \"I am experiencing significant weight gain or loss.\",\n    \"I feel a sense of detachment from loved ones.\",\n    \"I am dealing with an inability to cope with daily problems.\",\n    \"I have a decreased ability to feel pleasure.\",\n    \"I am experiencing frequent tearfulness.\",\n    \"I have a strong sense of guilt or worthlessness.\",\n    \"I am finding it hard to make decisions.\",\n    \"I am experiencing an inability to feel empathy.\",\n    \"I am dealing with persistent nervousness.\",\n    \"I have a pervasive fear of being judged by others.\",\n    \"I am experiencing a loss of sexual desire.\",\n    \"I am feeling trapped or in an unbearable situation.\",\n    \"I am dealing with excessive rumination on past events.\",\n    \"I feel a general sense of malaise or being unwell.\",\n    \"I am experiencing sudden changes in behavior.\",\n    \"I have a lack of desire to interact with others.\",\n    \"I am feeling a persistent emptiness.\",\n    \"I am experiencing unexplained aches and pains.\",\n    \"I have a strong sense of unreality or being detached from oneself.\",\n    \"I am dealing with a decreased ability to cope with minor problems.\",\n    \"I am experiencing intense fear or phobia.\",\n    \"I feel a chronic sense of unease or nervous anticipation.\",\n    \"I am dealing with an extreme sensitivity to rejection.\",\n    \"I am experiencing frequent outbursts of anger.\",\n    \"I have a pervasive pattern of unstable relationships.\",\n    \"I am feeling a chronic sense of alienation from others.\",\n    \"I am experiencing difficulty with memory or concentration.\",\n    \"I have a decreased sense of self-worth.\",\n    \"I am dealing with a persistent feeling of being overwhelmed.\",\n    \"I am experiencing strong feelings of envy or jealousy.\",\n    \"I feel an inability to be alone.\",\n    \"I am feeling an intense and irrational fear of everyday situations.\",\n    \"I am experiencing difficulty understanding and expressing emotions.\",\n    \"I am dealing with a constant preoccupation with certain ideas.\",\n    \"I have a lack of flexibility in thinking or behavior.\",\n    \"I am experiencing extreme restlessness or agitation.\",\n    \"I am feeling a deep fear of abandonment.\",\n    \"I have a tendency to self-isolate or withdraw from others.\",\n    \"I am experiencing a prolonged sense of grief or mourning.\",\n    \"I feel an excessive need for admiration or attention.\",\n    \"I am dealing with a severe reaction to criticism.\",\n    \"I am experiencing a persistent feeling of being out of control.\",\n    \"I have a distorted or negative body image.\",\n    \"I am feeling a chronic sense of boredom or emptiness.\",\n    \"I am experiencing an exaggerated sense of self-importance.\",\n    \"I feel an intense and inappropriate display of emotions.\",\n    \"I am dealing with a persistent distrust of others.\",\n]\n\n# Label for each symptom\nlabels = [\"Physical health\" for _ in range(100)]\n\nlabels2 = [\"Mental health\" for _ in range(86)]\n\n# Creating the DataFrame\nsymptoms_df = pd.DataFrame({\n    \"Symptoms\": physical_health_symptoms,\n    \"Label\": labels\n})\n\nsymptoms_df1 = pd.DataFrame({\n    \"Symptoms\": mental_health_symptoms,\n    \"Label\": labels2\n})\n\n# Check the lengths of the lists\nprint(len(mental_health_symptoms))\nprint(len(labels2))\n\n# If they are not the same, correct the lengths\n# Assuming 'labels2' is supposed to be the same length as 'mental_health_symptoms'\nlabels2 = [\"Mental Health\"] * len(mental_health_symptoms)\n\n# # Now create the DataFrame\n# symptoms_df1 = pd.DataFrame({\n#     \"Symptoms\": mental_health_symptoms,\n#     \"Label\": labels2\n# })\n\n# Saving to a CSV file\nsymptoms_df.to_csv(\"./physical_health_symptoms.csv\", index=False)\nsymptoms_df1.to_csv(\"./mental_health_symptoms.csv\", index=False)\n# combine two csv files\ndf1 = pd.read_csv(\"./physical_health_symptoms.csv\")\ndf2 = pd.read_csv(\"./mental_health_symptoms.csv\")\ndf = pd.concat([df1, df2])\ndf.to_csv(\"./symptoms.csv\", index=False)\n\n86\n86"
  }
]